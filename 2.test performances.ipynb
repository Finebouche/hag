{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T13:52:43.738827Z",
     "start_time": "2025-05-19T13:52:43.736629Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from scipy import sparse, stats\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "SEED = 923984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cda032b8a2214f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T13:52:44.515097Z",
     "start_time": "2025-05-19T13:52:44.512662Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "activation_function = lambda x : tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Regression : http://tseregression.org/ + https://arxiv.org/pdf/2012.02974"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6272-708d-4709-8e35-5a84268bed64",
   "metadata": {},
   "source": [
    "Prediction Datasets available :\n",
    "\n",
    "* MackeyGlass\n",
    "* Lorenz\n",
    "* Sunspot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b219b-eb64-4715-b983-7de3c392f088",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Classification Datasets available :\n",
    "\n",
    "* Custom :  FSDD, HAART, JapaneseVowels\n",
    "* Aeon : SpokenArabicDigits, CatsDogs, LSST\n",
    "* Torchaudio: SPEECHCOMMANDS\n",
    "\n",
    "More on https://www.timeseriesclassification.com/dataset.php or https://pytorch.org/audio/stable/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490eac1cc1341aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T13:52:45.980798Z",
     "start_time": "2025-05-19T13:52:45.971372Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Preprocessing\n",
    "from datasets.multivariate_generation import generate_multivariate_dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data\n",
    "from datasets.preprocessing import add_noise\n",
    "from datasets.load_data import load_data as load_dataset\n",
    "\n",
    "# Define noise parameter\n",
    "noise_std = 0.001\n",
    "\n",
    "data_type = \"normal\" # \"normal\" ou \"noisy\"\n",
    "\n",
    "def load_data(dataset_name, data_type, noise_std, step_ahead=5, visualize=False):\n",
    "    (is_instances_classification, is_multivariate, sampling_rate,\n",
    "     X_train_raw, X_test_raw, Y_train_raw, Y_test,\n",
    "     use_spectral_representation, spectral_representation,\n",
    "     groups) = load_dataset(dataset_name, step_ahead, visualize=False)\n",
    "\n",
    "    if is_multivariate:\n",
    "        X_train_band, X_test_band = X_train_raw, X_test_raw\n",
    "    else:\n",
    "        X_test, X_train = X_test_raw, X_train_raw\n",
    "    X_val_band, X_val = None, None\n",
    "    del X_train_raw, X_test_raw\n",
    "    Y_train = Y_train_raw\n",
    "    del Y_train_raw\n",
    "\n",
    "            \n",
    "    # PREPROCESSING\n",
    "    hop = 50 if is_instances_classification else 1\n",
    "    win_length = edge_cut = 100\n",
    "    if not is_multivariate:\n",
    "        X_train_band = generate_multivariate_dataset(\n",
    "            X_train, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "        )\n",
    "        \n",
    "        X_test_band = generate_multivariate_dataset(\n",
    "            X_test, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "        )\n",
    "    elif not use_spectral_representation:\n",
    "        X_train_band = generate_multivariate_dataset(\n",
    "            X_train_band, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "        )\n",
    "        X_test_band = generate_multivariate_dataset(\n",
    "            X_test_band, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "        )\n",
    "    else:\n",
    "        print(\"Data is already spectral and multivariate, nothing to do\")\n",
    "\n",
    "    # We cut the edges to remove the edges effects\n",
    "    if not is_instances_classification:\n",
    "        X_train_band = X_train_band[edge_cut:-edge_cut] \n",
    "        X_test_band = X_test_band[edge_cut:-edge_cut] \n",
    "        \n",
    "        Y_train = Y_train[edge_cut:-edge_cut] \n",
    "        Y_test  = Y_test[edge_cut:-edge_cut]\n",
    "\n",
    "    # NORMALIZATION\n",
    "    scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_band, X_val_band, X_test_band = scale_data(X_train_band, X_val_band, X_test_band, scaler_multi, is_instances_classification)\n",
    "    if not is_multivariate:\n",
    "        scaler_x_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train, X_val, X_test = scale_data(X_train, X_val, X_test, scaler_multi, is_instances_classification)       \n",
    "    \n",
    "    # OPTIONAL NOISE\n",
    "    if data_type == \"noisy\":\n",
    "        if is_instances_classification:\n",
    "            # uni\n",
    "            if not is_multivariate:\n",
    "                X_train_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train, desc=\"TRAIN\")]\n",
    "                X_test_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test, desc=\"TEST\")]\n",
    "                \n",
    "            # multi\n",
    "            X_train_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train_band, desc=\"TRAIN\")]\n",
    "            X_test_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test_band, desc=\"TEST\")]\n",
    "        \n",
    "        else:  #if prediction\n",
    "            # uni\n",
    "            if not is_multivariate:\n",
    "                X_train_noisy = add_noise(X_train, noise_std)\n",
    "                X_test_noisy = add_noise(X_test, noise_std)\n",
    "        \n",
    "            # multi\n",
    "            X_train_band_noisy = add_noise(X_train_band, noise_std)\n",
    "            X_test_band_noisy = add_noise(X_test_band, noise_std)\n",
    "\n",
    "    # PRETRAINING SET\n",
    "    # Define the number of instances you want to select\n",
    "    if is_instances_classification:\n",
    "        num_samples_for_pretrain = 500 if len(X_train_band) >= 500 else len(X_train_band)\n",
    "        indices = np.random.choice(len(X_train_band), num_samples_for_pretrain, replace=False)\n",
    "    else:\n",
    "        indices = range(len(X_train_band))\n",
    "    \n",
    "    if data_type == \"noisy\":\n",
    "        if not is_multivariate:\n",
    "            X_pretrain_noisy = np.array(X_train_noisy, dtype=object)[indices].flatten()\n",
    "        X_pretrain_band_noisy = np.array(X_train_band_noisy, dtype=object)[indices]\n",
    "    \n",
    "    if not is_multivariate:\n",
    "        X_pretrain = np.array(X_train, dtype=object)[indices].flatten()\n",
    "    X_pretrain_band = np.array(X_train_band, dtype=object)[indices]\n",
    "\n",
    "    return X_pretrain_band, X_train_band, X_test_band, Y_train, Y_test, is_multivariate, is_instances_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e97986c1a958fc",
   "metadata": {},
   "source": [
    "# Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606001921c4818e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T13:52:51.065846Z",
     "start_time": "2025-05-19T13:52:51.051503Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from numpy import random\n",
    "import tqdm \n",
    "\n",
    "# Evaluating\n",
    "from performances.esn_model_evaluation import train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from performances.esn_model_evaluation import (train_model_for_prediction, init_reservoir, init_ip_reservoir, init_local_rule_reservoir,\n",
    "                                                init_ip_local_rule_reservoir, init_readout)\n",
    "from analysis.richness import spectral_radius, pearson, squared_uncoupled_dynamics_alternative, distance_correlation\n",
    "from models.reservoir import init_matrices\n",
    "from hag.hag import run_algorithm\n",
    "\n",
    "\n",
    "nb_jobs = 10\n",
    "def evaluate_dataset_on_test(study, dataset_name, function_name, pretrain_data, train_data, test_data, Y_train, Y_test, is_instances_classification, nb_trials = 8, record_metrics=False, random_projection_experiment=False):\n",
    "    # Collect all hyperparameters in a dictionary\n",
    "    hyperparams = {param_name: param_value for param_name, param_value in study.best_trial.params.items()}\n",
    "    print(hyperparams)\n",
    "    leaky_rate = 1\n",
    "    input_connectivity = 1\n",
    "\n",
    "    # score for prediction\n",
    "    if dataset_name == \"Sunspot\":\n",
    "        start_step = 30\n",
    "        end_step = 500\n",
    "    else:\n",
    "        start_step = 500\n",
    "        end_step = 1500\n",
    "    SLICE_RANGE = slice(start_step, end_step)\n",
    "\n",
    "    if 'variance_target' not in hyperparams and 'min_variance' in hyperparams:\n",
    "        hyperparams['variance_target'] = hyperparams['min_variance']\n",
    "    if not is_instances_classification:\n",
    "        hyperparams['use_full_instance'] = False\n",
    "\n",
    "    RIDGE_COEF = 10**hyperparams['ridge']\n",
    "\n",
    "    if function_name in [\"hadsp\", \"desp\"]:\n",
    "        max_partners = np.inf\n",
    "\n",
    "    scores = []\n",
    "    if record_metrics:\n",
    "        spectral_radii = []\n",
    "        pearson_correlations = []\n",
    "        CEVs = []\n",
    "        dcors = []\n",
    "    for i in range(nb_trials):\n",
    "        common_index = 1\n",
    "        if is_instances_classification:\n",
    "            common_size = pretrain_data[0].shape[common_index]\n",
    "        else:\n",
    "            common_size = pretrain_data.shape[common_index]\n",
    "\n",
    "        # We want the size of the models to be at least network_size\n",
    "        K = math.ceil(hyperparams['network_size'] / common_size)\n",
    "        n = common_size * K\n",
    "\n",
    "        if function_name in [\"diag_ee\", \"diag_ei\"]:\n",
    "            use_block = True\n",
    "        else:\n",
    "            use_block = False\n",
    "\n",
    "        # UNSUPERVISED PRETRAINING\n",
    "        if function_name == \"random_ee\":\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'],  K, w_distribution=stats.uniform(loc=0, scale=1), use_block=use_block, seed=random.randint(0, 1000), random_projection_experiment=random_projection_experiment)\n",
    "        else:\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'],  K, w_distribution=stats.uniform(loc=-1, scale=2), use_block=use_block, seed=random.randint(0, 1000), random_projection_experiment=random_projection_experiment)\n",
    "        bias *= hyperparams['bias_scaling']\n",
    "        Win *= hyperparams['input_scaling']\n",
    "\n",
    "        if function_name == \"hadsp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data,\n",
    "                                     hyperparams['weight_increment'], hyperparams['target_rate'], hyperparams['rate_spread'], function_name,\n",
    "                                     multiple_instances=is_instances_classification,\n",
    "                                     min_increment = hyperparams['min_increment'], max_increment=hyperparams['max_increment'], use_full_instance=hyperparams['use_full_instance'],\n",
    "                                     max_partners=max_partners, method=\"pearson\", n_jobs=nb_jobs)\n",
    "        elif function_name == \"desp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data,\n",
    "                                         hyperparams['weight_increment'], hyperparams['variance_target'], hyperparams['variance_spread'], function_name,\n",
    "                                         multiple_instances=is_instances_classification,\n",
    "                                         min_increment = hyperparams['min_increment'], max_increment=hyperparams['max_increment'], use_full_instance = hyperparams['use_full_instance'],\n",
    "                                         max_partners=max_partners, method = \"pearson\",\n",
    "                                         intrinsic_saturation=hyperparams['intrinsic_saturation'], intrinsic_coef=hyperparams['intrinsic_coef'],\n",
    "                                         n_jobs = nb_jobs)\n",
    "        elif function_name in [\"random_ee\", \"random_ei\", \"diag_ee\", \"diag_ei\", \"ip_correct\", \"anti-oja_fast\", \"ip-anti-oja_fast\"]:\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= hyperparams['spectral_radius'] / max(abs(eigen))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid function: {function_name}\")\n",
    "\n",
    "        # unsupervised local rules\n",
    "        if is_instances_classification:\n",
    "            unsupervised_pretrain = np.concatenate(pretrain_data).astype(float)\n",
    "        else:\n",
    "            unsupervised_pretrain = pretrain_data.astype(float)\n",
    "        if function_name == \"ip_correct\":\n",
    "            reservoir = init_ip_reservoir(W, Win, bias, mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                          leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function\n",
    "                                          )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"anti-oja_fast\":\n",
    "            reservoir = init_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                  synapse_normalization=False, bcm_theta=None,\n",
    "                                                  leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                  )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"ip-anti-oja_fast\":\n",
    "            reservoir = init_ip_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                      synapse_normalization=False, bcm_theta=None,\n",
    "                                                      mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                                      leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                      )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        else:\n",
    "            reservoir = init_reservoir(W, Win, bias, leaky_rate, activation_function)\n",
    "        readout = init_readout(ridge_coef=RIDGE_COEF)\n",
    "\n",
    "\n",
    "        # TRAINING and EVALUATION\n",
    "        if record_metrics:\n",
    "            inputs = np.concatenate(test_data, axis=0) if is_instances_classification else test_data\n",
    "            states_history_multi = reservoir.run(inputs)\n",
    "\n",
    "            sr = spectral_radius(W)\n",
    "            pearson_correlation, _ = pearson(states_history_multi, num_windows=1, size_window=len(states_history_multi), step_size = 1, show_progress=False)\n",
    "            CEV = squared_uncoupled_dynamics_alternative(states_history_multi, num_windows=1, size_window=len(states_history_multi), step_size = 1, show_progress=True)\n",
    "            dcor = distance_correlation(states_history_multi, num_windows=1, size_window=len(states_history_multi), step_size = 1, show_progress=True, method=\"auto\", nb_jobs=nb_jobs)\n",
    "\n",
    "            spectral_radii.append(sr)\n",
    "            pearson_correlations.append(pearson_correlation[0])\n",
    "            CEVs.append(CEV[0])\n",
    "            dcors.append(dcor[0])\n",
    "        else:\n",
    "            if is_instances_classification:\n",
    "                mode = \"sequence-to-vector\"\n",
    "                train_model_for_classification(reservoir, readout, train_data, Y_train, n_jobs = nb_jobs, mode=mode)\n",
    "\n",
    "                Y_pred = predict_model_for_classification(reservoir, readout, test_data, n_jobs = nb_jobs, mode=mode)\n",
    "                score = compute_score(Y_pred, Y_test, is_instances_classification)\n",
    "            else:\n",
    "                esn = train_model_for_prediction(reservoir, readout, train_data, Y_train, warmup=start_step, n_jobs = nb_jobs)\n",
    "\n",
    "                Y_pred =  esn.run(test_data, reset=False)\n",
    "                score = compute_score(Y_pred[SLICE_RANGE], Y_test[SLICE_RANGE], is_instances_classification)\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "    if record_metrics:\n",
    "        return spectral_radii, pearson_correlations, CEVs, dcors\n",
    "\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f604b2e-f645-45f1-a7cb-3d8311d9c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from performances.esn_model_evaluation import compute_score\n",
    "from models.rnn import (\n",
    "    LSTMModel, RNNModel, GRUModel,\n",
    "    SequenceDataset, PrecomputedForecastDataset, make_sliding_windows,\n",
    "    pad_collate, BucketBatchSampler,\n",
    "    train as lstm_train,\n",
    "    evaluate as lstm_evaluate,\n",
    ")\n",
    "\n",
    "# new imports for HAG branch\n",
    "from performances.utility import retrieve_best_model\n",
    "from hag.hag import run_algorithm\n",
    "from models.reservoir import init_matrices\n",
    "import math\n",
    "from scipy import stats\n",
    "from numpy import random\n",
    "\n",
    "# device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "nb_jobs = 8\n",
    "def evaluate_dataset_on_test_rnn(\n",
    "    study,\n",
    "    dataset_name,\n",
    "    function_name,        # e.g. \"lstm_last\", \"rnn\", \"rnn-mean_hag\"\n",
    "    pretrain_data,        # list/array of pretraining bands for HAG\n",
    "    X_train,              # list of train sequences or array\n",
    "    X_test,               # list of test sequences or array\n",
    "    Y_train,              # labels or targets for train\n",
    "    Y_test,               # labels or targets for test\n",
    "    is_instances_classification,\n",
    "    nb_trials=8,\n",
    "    record_metrics=False\n",
    "):\n",
    "    # 1) best hyperparameters for LSTM/RNN\n",
    "    hp = study.best_trial.params.copy()\n",
    "    batch_size = hp.pop(\"batch_size\")\n",
    "    epochs     = hp.pop(\"epochs\")\n",
    "    lr         = hp.pop(\"learning_rate\")\n",
    "    nlayers    = hp.pop(\"num_layers\")\n",
    "    dropout    = hp.pop(\"dropout\")\n",
    "    \n",
    "    if function_name in [\"lstm\", \"rnn\", \"lstm_last\", \"gru\"]:\n",
    "            hidden     = hp.pop(\"hidden_size\")\n",
    "            bidir      = hp.pop(\"bidirectional\")\n",
    "\n",
    "    task_type = \"classification\" if is_instances_classification else \"regression\"\n",
    "    criterion = torch.nn.CrossEntropyLoss() if task_type==\"classification\" else torch.nn.MSELoss()\n",
    "\n",
    "    # for regression forecast slicing\n",
    "    if not is_instances_classification:\n",
    "        SLICE_RANGE = slice(500,1500) if dataset_name!=\"Sunspot\" else slice(30,500)\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    for seed in tqdm.tqdm(range(nb_trials), desc=\"Seeds\", unit=\"seed\"):\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # — build PyTorch Dataset & DataLoader —\n",
    "        if is_instances_classification:\n",
    "            train_ds = SequenceDataset(X_train, Y_train)\n",
    "            train_lens = [len(x) for x in X_train]\n",
    "            if len(set(train_lens))>1:\n",
    "                sampler = BucketBatchSampler(train_lens, batch_size=batch_size, bucket_size=batch_size*20, shuffle=True)\n",
    "                train_loader = DataLoader(train_ds, batch_sampler=sampler, collate_fn=pad_collate)\n",
    "            else:\n",
    "                train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "            test_ds = SequenceDataset(X_test, Y_test)\n",
    "            test_lens = [len(x) for x in X_test]\n",
    "            if len(set(test_lens))>1:\n",
    "                sampler = BucketBatchSampler(test_lens, batch_size=batch_size, bucket_size=batch_size*20, shuffle=False)\n",
    "                test_loader = DataLoader(test_ds, batch_sampler=sampler, collate_fn=pad_collate)\n",
    "            else:\n",
    "                test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "        else:\n",
    "            WINDOW = 100\n",
    "            X_tr_win, y_tr_tgt = make_sliding_windows(X_train, y=Y_train, window=WINDOW)\n",
    "            X_test_win, y_test_tgt = make_sliding_windows(X_test, y=Y_test, window=WINDOW)\n",
    "\n",
    "            train_ds = PrecomputedForecastDataset(X_tr_win, y_tr_tgt)\n",
    "            train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            test_ds = PrecomputedForecastDataset(X_test_win, y_test_tgt)\n",
    "            test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # infer dims\n",
    "        sample_x, sample_y = train_ds[0]\n",
    "        D_in  = sample_x.shape[-1]\n",
    "        if task_type==\"classification\":\n",
    "            D_out = sample_y.shape[-1]\n",
    "        else:\n",
    "            D_out = sample_y.shape[-1] if sample_y.ndim>0 else 1\n",
    "\n",
    "        # instantiate models\n",
    "        if function_name == \"lstm_last\":\n",
    "            model = LSTMModel(\n",
    "                input_size=D_in,\n",
    "                hidden_size=hidden,\n",
    "                num_layers=nlayers,\n",
    "                output_size=D_out,\n",
    "                dropout=dropout,\n",
    "                bidirectional=bidir\n",
    "            ).to(DEVICE)\n",
    "\n",
    "        elif function_name == \"gru\":\n",
    "            model = GRUModel(\n",
    "                input_size=D_in,\n",
    "                hidden_size=hidden,\n",
    "                num_layers=nlayers,\n",
    "                output_size=D_out,\n",
    "                dropout=dropout,\n",
    "                bidirectional=bidir\n",
    "            ).to(DEVICE)\n",
    "\n",
    "        elif function_name == \"rnn\":\n",
    "            model = RNNModel(\n",
    "                input_size=D_in,\n",
    "                hidden_size=hidden,\n",
    "                num_layers=nlayers,\n",
    "                output_size=D_out,\n",
    "                dropout=dropout,\n",
    "                bidirectional=bidir\n",
    "            ).to(DEVICE)\n",
    "\n",
    "        elif function_name == \"rnn-mean_hag\":\n",
    "            # HAG-based reservoir initialization\n",
    "            # 1) Retrieve best HAG hyperparameters\n",
    "            hag_study = retrieve_best_model(\"hadsp\", dataset_name, False, variate_type=\"multi\", data_type=\"normal\")\n",
    "            hyper = {k: v for k, v in hag_study.best_trial.params.items()}\n",
    "            if 'variance_target' not in hyper and 'min_variance' in hyper:\n",
    "                hyper['variance_target'] = hyper.pop('min_variance')\n",
    "            hyper['use_full_instance'] = not is_instances_classification\n",
    "\n",
    "            # 2) Build reservoir matrices\n",
    "            input_connectivity = 1\n",
    "            common_size = X_train[0].shape[1] if is_instances_classification else X_train.shape[1]\n",
    "            K = math.ceil(hyper['network_size'] / common_size)\n",
    "            n = common_size * K\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyper['connectivity'], K,\n",
    "                w_distribution=stats.uniform(loc=-1, scale=2), seed=random.randint(0, 1000))\n",
    "            bias *= hyper['bias_scaling']\n",
    "            Win *= hyper['input_scaling']\n",
    "\n",
    "            # 3) Adapt weights via HAG\n",
    "            activation_function = np.tanh\n",
    "            fold_idx = seed\n",
    "            X_pre = pretrain_data\n",
    "            W, _ = run_algorithm(\n",
    "                W, Win, bias,\n",
    "                hyper['leaky_rate'], activation_function,\n",
    "                X_pre, hyper['weight_increment'],\n",
    "                hyper['target_rate'], hyper['rate_spread'],\n",
    "                function_name, multiple_instances=is_instances_classification,\n",
    "                min_increment=hyper['min_increment'], max_increment=hyper['max_increment'],\n",
    "                use_full_instance=hyper['use_full_instance'], max_partners=np.inf,\n",
    "                method=\"pearson\", n_jobs=nb_jobs\n",
    "            )\n",
    "\n",
    "            # 4) Readout training\n",
    "            from performances.esn_model_evaluation import train_model_for_classification, train_model_for_prediction, init_readout, init_reservoir\n",
    "            reservoir = init_reservoir(W, Win, bias, hyper['leaky_rate'], activation_function)\n",
    "            RIDGE_COEF = 10 ** hyper['ridge']\n",
    "            readout = init_readout(ridge_coef=RIDGE_COEF)\n",
    "            start_step = SLICE_RANGE.start if not is_instances_classification else None\n",
    "            if is_instances_classification:\n",
    "                train_model_for_classification(reservoir, readout, X_train, Y_train, n_jobs=1, mode=\"sequence-to-vector\")\n",
    "            else:\n",
    "                _ = train_model_for_prediction(reservoir, readout, X_train, Y_train, warmup=start_step, n_jobs=nb_jobs)\n",
    "            Wout = readout.Wout\n",
    "            bias_out = readout.bias.reshape(-1)\n",
    "\n",
    "            # 5) Instantiate PyTorch RNNModel and overwrite weights\n",
    "            model = RNNModel(\n",
    "                input_size=D_in,\n",
    "                hidden_size=n,\n",
    "                num_layers=1,\n",
    "                output_size=D_out,\n",
    "                dropout=dropout,\n",
    "                bidirectional=False\n",
    "            ).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                model.rnn.weight_ih_l0.copy_(torch.tensor(Win, dtype=torch.float32, device=DEVICE))\n",
    "                model.rnn.weight_hh_l0.copy_(torch.tensor(W, dtype=torch.float32, device=DEVICE))\n",
    "                model.rnn.bias_ih_l0.zero_()\n",
    "                model.rnn.bias_hh_l0.copy_(torch.tensor(bias, dtype=torch.float32, device=DEVICE))\n",
    "                model.fc.weight.copy_(torch.tensor(Wout.T, dtype=torch.float32, device=DEVICE))\n",
    "                model.fc.bias.copy_(torch.tensor(bias_out, dtype=torch.float32, device=DEVICE))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown function_name: {function_name}\")\n",
    "\n",
    "\n",
    "        # Train\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        model = torch.compile(model)\n",
    "        # train for a few epochs\n",
    "        for _ in range(epochs):\n",
    "            _ = lstm_train(model, train_loader, criterion, optimizer, task_type=task_type)\n",
    "            \n",
    "        # evaluate\n",
    "        metric = lstm_evaluate(model, test_loader, task_type=task_type)\n",
    "        all_scores.append(metric)\n",
    "\n",
    "    if record_metrics:\n",
    "        raise NotImplementedError(\"Hidden-state metrics for LSTM not yet supported.\")\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049fbb3-7c63-4ed1-b969-cd5b4a00bef6",
   "metadata": {},
   "source": [
    "# Common visualisation definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ae41a-4c2c-4d17-adcf-a812bdd9d42b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T13:52:52.151781Z",
     "start_time": "2025-05-19T13:52:52.146748Z"
    }
   },
   "outputs": [],
   "source": [
    "from seaborn import color_palette\n",
    "\n",
    "# -- Define color palettes for each group --\n",
    "blues = color_palette(\"Blues\", 5)      # shades of blue\n",
    "oranges = color_palette(\"Oranges\", 2)  # shades of orange\n",
    "greens = color_palette(\"Greens\", 2)    # shades of green\n",
    "reds = color_palette(\"Reds\", 2)    # shades of red\n",
    "greys = color_palette(\"Greys\", 3)    # shades of greys\n",
    "\n",
    "# -- Map each function to its color --\n",
    "function_colors = {\n",
    "    'E-ESN':                greens[0],\n",
    "    'ESN':                  greens[1],\n",
    "    'IP':                   blues[0],\n",
    "    'Anti-Oja':             blues[1],\n",
    "    'Anti-Oja-Fast':        blues[2],\n",
    "    'IP +\\nAnti-Oja':       blues[3],\n",
    "    'IP +\\nAnti-Oja\\nFast': blues[4],\n",
    "    'mean HAG':             oranges[0],\n",
    "    'variance HAG':         oranges[1],\n",
    "    'LSTM':                 greys[0],\n",
    "#    'RNN':                  greys[1],\n",
    "    'GRU':                  greys[2],\n",
    "#    'RNN-HAG':              greys[2],\n",
    "#    'diag EE':             reds[0],\n",
    "#    'diag EI':             reds[1],\n",
    "}\n",
    "\n",
    "# If you want a specific order for the bars, you can enforce it:\n",
    "functions_order = [\n",
    "    'E-ESN',\n",
    "    'ESN',\n",
    "    'IP',\n",
    "    'Anti-Oja',\n",
    "    'IP +\\nAnti-Oja',\n",
    "    'mean HAG',\n",
    "    'variance HAG',\n",
    "#    'RNN',\n",
    "#    'RNN-HAG',\n",
    "    'LSTM',\n",
    "    'GRU',\n",
    "#    'diag EE',\n",
    "#    'diag EI',\n",
    "]\n",
    "\n",
    "function_mapping = {\n",
    "    'random_ee':        'E-ESN',\n",
    "    'random_ei':        'ESN',\n",
    "    'ip_correct':       'IP',\n",
    "    'anti-oja_fast':    'Anti-Oja',\n",
    "    'ip-anti-oja_fast': 'IP +\\nAnti-Oja',\n",
    "    'hadsp':            'mean HAG',\n",
    "    'desp':             'variance HAG',\n",
    "    'lstm_last':        'LSTM',\n",
    "#    'rnn':              'RNN',\n",
    "    'gru':              'GRU',\n",
    "#    'rnn-mean_hag':     'RNN-HAG',\n",
    "#    'diag_ei':        'diag EI',\n",
    "#    'diag_ee':        'diag EE',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7479a5-29a8-435e-a7f4-ba7a59e7b9a7",
   "metadata": {},
   "source": [
    "# Input matrix impact"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f1f3618-1046-41ed-a7a3-2704e799fd58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:30:06.084143Z",
     "iopub.status.busy": "2025-08-31T21:30:06.083832Z",
     "iopub.status.idle": "2025-08-31T22:25:37.530638Z",
     "shell.execute_reply": "2025-08-31T22:25:37.530302Z",
     "shell.execute_reply.started": "2025-08-31T21:30:06.084122Z"
    }
   },
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError  # <-- add this\n",
    "import optuna\n",
    "\n",
    "from performances.utility import camel_to_snake \n",
    "from performances.utility import retrieve_best_model\n",
    "\n",
    "noise_std = 0.001\n",
    "outfile = \"outputs/input_strategy.csv\"\n",
    "\n",
    "columns = ['Dataset', 'Function', 'Random Input Mapping', 'Average Score', 'Standard Deviation', 'Date']\n",
    "\n",
    "# Ensure folder exists\n",
    "os.makedirs(os.path.dirname(outfile), exist_ok=True)\n",
    "\n",
    "# Create or repair the CSV header if file is missing OR empty\n",
    "if (not os.path.exists(outfile)) or (os.path.getsize(outfile) == 0):\n",
    "    pd.DataFrame(columns=columns).to_csv(outfile, index=False)\n",
    "\n",
    "for dataset_name in [\"FSDD\"]:\n",
    "    print(f\"\\n===== DATASET: {dataset_name} =====\")\n",
    "\n",
    "    (pretrain_data,\n",
    "     train_data,\n",
    "     test_data,\n",
    "     Y_train,\n",
    "     Y_test,\n",
    "     is_multivariate,\n",
    "     is_instances_classification) = load_data(dataset_name, data_type, noise_std, visualize=False)\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    for random_projection_experiment in [True, False]:\n",
    "        print(f\"--- Random projection: {random_projection_experiment} ---\")\n",
    "         # \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\"\n",
    "        for function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\"]:\n",
    "            print(\"Function:\", function_name)\n",
    "            prefix = \"rdn-projection_tpe\" if random_projection_experiment else \"new_tpe\"\n",
    "            study = retrieve_best_model(function_name, dataset_name, is_multivariate,\n",
    "                                        variate_type=\"multi\", data_type=\"normal\", prefix=prefix)\n",
    "\n",
    "            scores = evaluate_dataset_on_test(\n",
    "                study, \n",
    "                dataset_name,\n",
    "                function_name, \n",
    "                pretrain_data, \n",
    "                train_data, \n",
    "                test_data,\n",
    "                Y_train, \n",
    "                Y_test,\n",
    "                is_instances_classification,\n",
    "                nb_trials = 8,\n",
    "                record_metrics=False,\n",
    "                random_projection_experiment=random_projection_experiment,\n",
    "            )\n",
    "            \n",
    "            avg = float(np.mean(scores))\n",
    "            std = float(np.std(scores))\n",
    "\n",
    "            if is_instances_classification:\n",
    "                avg_fmt = f\"{round(avg * 100, 5)} %\"\n",
    "                std_fmt = f\"± {round(std * 100, 5)} %\"\n",
    "            else:\n",
    "                avg_fmt = f\"{round(avg, 5)}\"\n",
    "                std_fmt = f\"± {round(std, 5)}\"\n",
    "\n",
    "            mapping_label = \"random\" if random_projection_experiment else \"modular\"\n",
    "\n",
    "            new_rows.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Function': function_name,\n",
    "                'Random Input Mapping': mapping_label,\n",
    "                'Average Score': avg_fmt,\n",
    "                'Standard Deviation': std_fmt,\n",
    "                'Date': datetime.now().strftime('%Y-%m-%d')\n",
    "            })\n",
    "\n",
    "    if new_rows:\n",
    "        new_results = pd.DataFrame(new_rows, columns=columns)\n",
    "        print(\"\\n== New results for\", dataset_name, \"==\")\n",
    "        print(new_results)\n",
    "\n",
    "        # Read previous safely; treat empty file as no rows\n",
    "        try:\n",
    "            prev = pd.read_csv(outfile)\n",
    "        except (EmptyDataError, FileNotFoundError):\n",
    "            prev = pd.DataFrame(columns=columns)\n",
    "\n",
    "        combined = pd.concat([prev, new_results], ignore_index=True)\n",
    "\n",
    "        # Atomic-ish write\n",
    "        tmp = outfile + \".tmp\"\n",
    "        combined.to_csv(tmp, index=False)\n",
    "        os.replace(tmp, outfile)\n",
    "        print(f\"Results saved to {outfile}.\")\n",
    "    else:\n",
    "        print(\"No new studies found; nothing to append.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44860d7-3673-49d9-bee9-da43f065e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# === Config you can tweak ===\n",
    "file_name = \"outputs/input_strategy.csv\"   # <- uses the CSV you wrote earlier\n",
    "\n",
    "dataset_label_map = {\n",
    "    \"JapaneseVowels\": \"Japanese\\nVowels\",\n",
    "    \"CatsDogs\": \"Cats vs\\nDogs\",\n",
    "    \"FSDD\": \"FSDD\",\n",
    "}\n",
    "\n",
    "mapping_hatching = {\n",
    "    \"modular\": \"\",       # no hatch\n",
    "    \"random\": \"///\",     # hatched bars\n",
    "}\n",
    "\n",
    "# ---- load & clean ----\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Normalize column names just in case\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Clean numeric strings like \"97.123 %\", \"± 1.23 %\"\n",
    "def numify(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s).replace(\"%\", \"\").replace(\"±\", \"\").strip()\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "df[\"Average Score\"] = df[\"Average Score\"].apply(numify)\n",
    "df[\"Standard Deviation\"] = df[\"Standard Deviation\"].apply(numify)\n",
    "\n",
    "# If your Average Score is in percent (classification), keep it as-is (already % stripped).\n",
    "# If it’s prediction (NRMSE), also fine.\n",
    "\n",
    "# Apply pretty labels\n",
    "df[\"Function\"] = df[\"Function\"].replace(function_mapping)\n",
    "df[\"Dataset\"] = df[\"Dataset\"].replace(dataset_label_map)\n",
    "\n",
    "# Make sure mapping labels are normalized\n",
    "if \"Random Input Mapping\" not in df.columns:\n",
    "    raise ValueError(\"CSV must include 'Random Input Mapping' with values like 'modular'/'random'\")\n",
    "df[\"Random Input Mapping\"] = df[\"Random Input Mapping\"].str.lower().map({\"modular\":\"modular\",\"random\":\"random\"})\n",
    "\n",
    "# Keep only functions we know (in case CSV holds others)\n",
    "present_functions = [function_mapping.get(f, f) for f in functions_order if function_mapping.get(f, f) in df[\"Function\"].unique()]\n",
    "if not present_functions:\n",
    "    present_functions = sorted(df[\"Function\"].unique())\n",
    "\n",
    "mappings = [m for m in [\"modular\",\"random\"] if m in df[\"Random Input Mapping\"].unique()]\n",
    "\n",
    "# Consistent dataset order\n",
    "datasets = list(df[\"Dataset\"].unique())\n",
    "# Optional: enforce a specific order\n",
    "preferred_order = [dataset_label_map.get(k, k) for k in [\"JapaneseVowels\",\"CatsDogs\",\"FSDD\"]]\n",
    "datasets = [d for d in preferred_order if d in datasets] + [d for d in df[\"Dataset\"].unique() if d not in preferred_order]\n",
    "\n",
    "# ---- plot ----\n",
    "x = np.arange(len(datasets))                     # dataset positions\n",
    "width = 0.08                                     # narrow bars to fit many\n",
    "fontsize = 12\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Outer loop over functions, inner loop over mapping styles\n",
    "for i, func in enumerate(present_functions):\n",
    "    for j, mapping in enumerate(mappings):\n",
    "        offset = i * (len(mappings) * width) + j * width\n",
    "\n",
    "        sub = df[(df[\"Function\"] == func) & (df[\"Random Input Mapping\"] == mapping)]\n",
    "        # align to dataset positions\n",
    "        merged = pd.DataFrame({\"Dataset\": datasets}).merge(sub, on=\"Dataset\", how=\"left\")\n",
    "\n",
    "        # pick color for function\n",
    "        color = function_colors.get(func, \"gray\")\n",
    "        hatch = mapping_hatching.get(mapping, \"\")\n",
    "\n",
    "        ax.bar(\n",
    "            x + offset,\n",
    "            merged[\"Average Score\"],\n",
    "            width,\n",
    "            yerr=merged[\"Standard Deviation\"],\n",
    "            capsize=4,\n",
    "            color=color,\n",
    "            hatch=hatch,\n",
    "            edgecolor=\"black\",\n",
    "            label=None\n",
    "        )\n",
    "\n",
    "# cosmetics\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.tick_params(axis=\"both\", labelsize=fontsize)\n",
    "ax.set_xlabel(\"Dataset\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Average Score\", fontsize=fontsize)  # If classification, this is in %\n",
    "\n",
    "# center ticks under grouped bars\n",
    "group_width = len(present_functions) * len(mappings) * width\n",
    "ax.set_xticks(x + group_width/2 - (width/2))\n",
    "ax.set_xticklabels(datasets, rotation=0)\n",
    "\n",
    "# legends\n",
    "func_legend = [Patch(facecolor=function_colors.get(f,\"gray\"), edgecolor=\"black\", label=f) for f in present_functions]\n",
    "map_legend = [Patch(facecolor=\"white\", edgecolor=\"black\", hatch=mapping_hatching[m], label=m.title()) for m in mappings]\n",
    "\n",
    "first_legend = ax.legend(handles=func_legend, title=\"Function\", loc=\"upper left\", fontsize=fontsize)\n",
    "ax.add_artist(first_legend)\n",
    "ax.legend(handles=map_legend, title=\"Input Mapping\", loc=\"lower left\", fontsize=fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c679b1-3869-4733-b77c-e4047044fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Inputs ===\n",
    "csv_path = \"outputs/input_strategy.csv\"\n",
    "\n",
    "# Reuse your objects if already defined; otherwise provide safe fallbacks\n",
    "dataset_label_map = globals().get(\"dataset_label_map\", {\n",
    "    \"JapaneseVowels\": \"Japanese\\nVowels\",\n",
    "    \"CatsDogs\": \"Cats vs\\nDogs\",\n",
    "    \"FSDD\": \"FSDD\",\n",
    "})\n",
    "function_colors   = globals().get(\"function_colors\", {})\n",
    "functions_order   = globals().get(\"functions_order\", [])\n",
    "function_mapping  = globals().get(\"function_mapping\", {})\n",
    "\n",
    "# ---------- Matplotlib styling (clean, print-ready) ----------\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\": 150,\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 15,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"pdf.fonttype\": 42,       # embed text as text in PDFs\n",
    "    \"ps.fonttype\": 42,\n",
    "})\n",
    "\n",
    "# ---------- Load & clean ----------\n",
    "df = pd.read_csv(csv_path)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "def numify(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s).replace(\"%\", \"\").replace(\"±\", \"\").strip()\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "df[\"avg\"] = df[\"Average Score\"].apply(numify)\n",
    "df[\"std\"] = df[\"Standard Deviation\"].apply(numify)\n",
    "df[\"Random Input Mapping\"] = (\n",
    "    df[\"Random Input Mapping\"].astype(str).str.strip().str.lower()\n",
    "      .map({\"random\": \"random\", \"modular\": \"modular\"})\n",
    ")\n",
    "\n",
    "# Pretty labels\n",
    "df[\"Function_disp\"] = df[\"Function\"].map(function_mapping).fillna(df[\"Function\"])\n",
    "df[\"Dataset_disp\"]  = df[\"Dataset\"].map(dataset_label_map).fillna(df[\"Dataset\"])\n",
    "\n",
    "# Pivot to get modular/random side by side, compute Δ (pp)\n",
    "wide = (\n",
    "    df.pivot_table(index=[\"Dataset\",\"Dataset_disp\",\"Function_disp\"],\n",
    "                   columns=\"Random Input Mapping\",\n",
    "                   values=\"avg\")\n",
    "      .reset_index()\n",
    ")\n",
    "wide[\"delta_pp\"] = wide[\"modular\"] - wide[\"random\"]\n",
    "\n",
    "# Ordering\n",
    "dataset_order = [k for k in [\"JapaneseVowels\",\"CatsDogs\",\"FSDD\"] if k in wide[\"Dataset\"].unique()]\n",
    "fn_order = [f for f in functions_order if f in wide[\"Function_disp\"].unique()]\n",
    "if not fn_order:\n",
    "    fn_order = sorted(wide[\"Function_disp\"].unique())\n",
    "\n",
    "# Build heatmap table (functions × datasets) using display labels for columns\n",
    "heat = (\n",
    "    wide.pivot_table(index=\"Function_disp\", columns=\"Dataset\", values=\"delta_pp\")\n",
    "        .reindex(index=fn_order, columns=dataset_order)\n",
    ")\n",
    "col_labels = [dataset_label_map.get(d, d) for d in heat.columns]\n",
    "row_labels = list(heat.index)\n",
    "\n",
    "# ---------- Heatmap (Δ = Modular − Random, in percentage points) ----------\n",
    "# Centered diverging scale with symmetric bounds\n",
    "vmax = np.nanmax(np.abs(heat.values))\n",
    "if not np.isfinite(vmax):  # guard against all-NaN\n",
    "    vmax = 1.0\n",
    "norm = mpl.colors.TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "cmap = plt.get_cmap(\"RdBu_r\").copy()\n",
    "cmap.set_bad(\"#eeeeee\")  # missing cells in light grey\n",
    "\n",
    "data = np.ma.masked_invalid(heat.values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(1.9*len(col_labels) + 2.8, 0.55*len(row_labels) + 2.2), layout=\"constrained\")\n",
    "im = ax.imshow(data, cmap=cmap, norm=norm, aspect=\"auto\")\n",
    "\n",
    "# Major ticks\n",
    "ax.set_xticks(np.arange(len(col_labels)))\n",
    "ax.set_xticklabels(col_labels)\n",
    "ax.set_yticks(np.arange(len(row_labels)))\n",
    "ax.set_yticklabels(row_labels)\n",
    "\n",
    "# Fine white gridlines between cells (journal-friendly)\n",
    "ax.set_xticks(np.arange(-0.5, data.shape[1], 1), minor=True)\n",
    "ax.set_yticks(np.arange(-0.5, data.shape[0], 1), minor=True)\n",
    "ax.grid(which=\"minor\", color=\"white\", linestyle=\"-\", linewidth=1.2)\n",
    "ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "# Frame & title\n",
    "for spine in (\"top\",\"right\"):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "# Annotate each cell with value; choose text color for contrast\n",
    "def luminance(rgb):\n",
    "    r, g, b = rgb[:3]\n",
    "    return 0.2126*r + 0.7152*g + 0.0722*b\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        val = heat.values[i, j]\n",
    "        if np.isnan(val): \n",
    "            continue\n",
    "        # pick text color based on colormap lightness at this value\n",
    "        rgba = cmap(norm(val))\n",
    "        txt_color = \"white\" if luminance(rgba) < 0.45 else \"black\"\n",
    "        ax.text(j, i, f\"{val:+.2f}\", ha=\"center\", va=\"center\", fontsize=10, color=txt_color)\n",
    "\n",
    "# Colorbar with symmetric ticks\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "ticks = np.linspace(-vmax, vmax, 5)\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.set_ticklabels([f\"{t:.1f}\" for t in ticks])\n",
    "cbar.set_label(\"Δ (pp)\")\n",
    "\n",
    "\n",
    "# Exports: PNG (raster) and PDF (vector)\n",
    "pdf_out = \"outputs/images/heatmap_deltas.pdf\"\n",
    "fig.savefig(pdf_out, bbox_inches=\"tight\")\n",
    "print(f\"Saved {png_out} and {pdf_out}\")\n",
    "\n",
    "# ---------- Concise textual summary ----------\n",
    "by_ds = wide.groupby(\"Dataset\")[\"delta_pp\"].mean().reindex(dataset_order)\n",
    "overall = wide[\"delta_pp\"].mean()\n",
    "\n",
    "print(\"\\nAverage gain of Modular over Random (pp):\")\n",
    "for ds, v in by_ds.items():\n",
    "    ds_disp = dataset_label_map.get(ds, ds)\n",
    "    print(f\"  {ds_disp}: {v:+.2f} pp\")\n",
    "print(f\"  Overall: {overall:+.2f} pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048caacd-b461-4095-99bb-ebfbbc49cd3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac2247-a66c-425d-a5e1-561398f350b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"CatsDogs\", \"FSDD\", \"JapaneseVowels\", \"SPEECHCOMMANDS\", \"SpokenArabicDigits\"\n",
    "]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    (is_instances_classification, is_multivariate, sampling_rate,\n",
    "     X_train_raw, X_test_raw, Y_train_raw, Y_test,\n",
    "     use_spectral_representation, spectral_representation,\n",
    "     groups) = load_dataset(dataset_name, 5, visualize=False)\n",
    "    \n",
    "    # Compute the length of each instance (assuming each instance is a 1D sequence)\n",
    "    lengths = [len(x) for x in X_train_raw]\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}  -->  Average length of X_train_raw: {avg_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792b2bd-e48b-4acd-9a99-05e08ac699c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset list\n",
    "datasets = [\n",
    "    \"MackeyGlass\", \"Lorenz\", \"Sunspot_daily\", \"Henon\", \"NARMA\", \"CatsDogs\", \n",
    "    \"FSDD\", \"JapaneseVowels\", \"SPEECHCOMMANDS\", \"SpokenArabicDigits\"\n",
    "]\n",
    "\n",
    "# Function to test dataset characteristics\n",
    "def dataset_characteristics(dataset, data_type, noise_std):\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset, data_type, noise_std, visualize=False)\n",
    "\n",
    "    # Score for prediction\n",
    "    if dataset == \"Sunspot\":\n",
    "        start_step = 30\n",
    "        end_step = 500\n",
    "    else:\n",
    "        start_step = 500\n",
    "        end_step = 1500\n",
    "    SLICE_RANGE = slice(start_step, end_step)\n",
    "\n",
    "    common_index = 1\n",
    "    if is_instances_classification:\n",
    "        common_size = pretrain_data[0].shape[common_index]\n",
    "    else:\n",
    "        common_size = pretrain_data.shape[common_index]\n",
    "\n",
    "    # We want the size of the models to be at least network_size\n",
    "    network_size = 500\n",
    "    K = math.ceil(network_size / common_size)\n",
    "    n = common_size * K\n",
    "\n",
    "    # Pretraining dataset processing\n",
    "    if is_instances_classification:\n",
    "        unsupervised_pretrain = np.concatenate(pretrain_data).astype(float)\n",
    "    else:\n",
    "        unsupervised_pretrain = pretrain_data.astype(float)\n",
    "\n",
    "    # **Calculate total step count**\n",
    "    if is_instances_classification:\n",
    "        total_pretrain_length = sum(instance.shape[0] for instance in pretrain_data)\n",
    "        total_train_length = sum(instance.shape[0] for instance in train_data)\n",
    "        total_test_length = sum(instance.shape[0] for instance in test_data)\n",
    "        original_dimension = train_data[0].shape[1]\n",
    "        number_class = Y_test.shape[1]\n",
    "        \n",
    "        # Calculate min, max and average sample size over test+train datasets\n",
    "        sample_sizes = [instance.shape[0] for instance in train_data] + [instance.shape[0] for instance in test_data]\n",
    "        min_sample_size = min(sample_sizes)\n",
    "        max_sample_size = max(sample_sizes)\n",
    "        avg_sample_size = sum(sample_sizes) / len(sample_sizes)\n",
    "    else:\n",
    "        total_pretrain_length = pretrain_data.shape[0]\n",
    "        total_train_length = train_data.shape[0]\n",
    "        total_test_length = test_data.shape[0]\n",
    "        original_dimension = train_data.shape[1]\n",
    "        number_class = 0\n",
    "        min_sample_size = None\n",
    "        max_sample_size = None\n",
    "        avg_sample_size = None\n",
    "\n",
    "    return {\n",
    "        \"dataset_name\": dataset,\n",
    "        \"tot_pretrain_length\": total_pretrain_length,\n",
    "        \"tot_train_length\": total_train_length,\n",
    "        \"tot_test_length\": total_test_length,\n",
    "        \"origin_dim\": original_dimension,\n",
    "        \"nb_dupication\": K,\n",
    "        \"reservoir_size\": n,\n",
    "        \"final_dim\": common_size,\n",
    "        \"nb_class\": number_class,\n",
    "        \"min_sample_size\": min_sample_size,\n",
    "        \"max_sample_size\": max_sample_size,\n",
    "        \"avg_sample_size\": avg_sample_size,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run tests on all datasets\n",
    "results = []\n",
    "for dataset in datasets:\n",
    "    characteristics = dataset_characteristics(dataset, data_type, noise_std)\n",
    "    results.append(characteristics)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"outputs/dataset_characteristics.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Dataset characteristics saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bb5ed-a01e-4874-8b28-a9d7abde179d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HPO Comparaison"
   ]
  },
  {
   "cell_type": "raw",
   "id": "977ffefd-1fc2-473c-a960-3f2f8c626f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T22:54:20.816445Z",
     "iopub.status.busy": "2025-02-26T22:54:20.816130Z",
     "iopub.status.idle": "2025-02-26T23:05:00.885803Z",
     "shell.execute_reply": "2025-02-26T23:05:00.884567Z",
     "shell.execute_reply.started": "2025-02-26T22:54:20.816422Z"
    },
    "scrolled": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from performances.utility import camel_to_snake, retrieve_best_model\n",
    "\n",
    "# Your existing imports\n",
    "# from performances.utility import retrieve_best_model  # Now replaced with new retrieve_best_model above\n",
    "\n",
    "# Add \"Sampler\" to the columns\n",
    "columns = ['Dataset', 'Function', 'Sampler', 'Average Score', 'Standard Deviation', 'Date']\n",
    "variate_type = \"multi\"  # \"multi\" or \"uni\"\n",
    "file_name = \"outputs/hpo_strategy.csv\"\n",
    "\n",
    "for dataset_name in [\"JapaneseVowels\"]: \n",
    "    new_results = pd.DataFrame(columns=columns)\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset_name, data_type, noise_std, visualize=True)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"===== DATASET: {dataset_name} =====\")\n",
    "    \n",
    "    for sampler_name in [\"cmaes\", \"tpe\"]:\n",
    "        print(f\"--- Sampler: {sampler_name} ---\")\n",
    "        for function_name in [\"desp\", \"hadsp\", \"ip-anti-oja\", \"anti-oja\", \"ip_correct\", \"random_ee\", \"random_ei\"]:\n",
    "            print(\"Function:\", function_name)\n",
    "\n",
    "            # Retrieve the best study for that (sampler, function, dataset)\n",
    "            study = retrieve_best_model(\n",
    "                function_name=function_name,\n",
    "                dataset_name=dataset_name,\n",
    "                is_multivariate=is_multivariate,\n",
    "                variate_type=variate_type,\n",
    "                data_type=\"normal\",           # or \"noisy\" if needed\n",
    "                sampler_name=sampler_name\n",
    "            )\n",
    "\n",
    "            # Evaluate on test set\n",
    "            scores = evaluate_dataset_on_test(\n",
    "                study,\n",
    "                dataset_name,\n",
    "                function_name,\n",
    "                pretrain_data,\n",
    "                train_data,\n",
    "                test_data,\n",
    "                Y_train,\n",
    "                Y_test,\n",
    "                is_instances_classification,\n",
    "                record_metrics=False\n",
    "            )\n",
    "\n",
    "            average_score = np.mean(scores)\n",
    "            std_deviation = np.std(scores)\n",
    "\n",
    "            if is_instances_classification:\n",
    "                # Classification => format as percent\n",
    "                formatted_average = f\"{round(average_score * 100, 5)} %\"\n",
    "                formatted_std = f\"± {round(std_deviation * 100, 5)} %\"\n",
    "            else:\n",
    "                # Prediction => raw numeric\n",
    "                formatted_average = f\"{round(average_score, 5)}\"\n",
    "                formatted_std = f\"± {round(std_deviation, 5)}\"\n",
    "            \n",
    "            current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "            # Add to this dataset's results\n",
    "            new_row = pd.DataFrame({\n",
    "                'Dataset': [dataset_name],\n",
    "                'Function': [function_name],\n",
    "                'Sampler': [sampler_name],\n",
    "                'Average Score': [formatted_average],\n",
    "                'Standard Deviation': [formatted_std],\n",
    "                'Date': [current_date]\n",
    "            })\n",
    "            new_results = pd.concat([new_results, new_row], ignore_index=True)\n",
    "\n",
    "    # Show results for this dataset\n",
    "    print(\"\\n== New results for\", dataset_name, \"==\")\n",
    "    print(new_results)\n",
    "    \n",
    "    # Load or create local CSV file\n",
    "    if os.path.exists(file_name):\n",
    "        previous_results = pd.read_csv(file_name)\n",
    "    else:\n",
    "        columns = ['Dataset', 'Function', 'Sampler', 'Average Score', 'Standard Deviation', 'Date']\n",
    "        previous_results = pd.DataFrame(columns=columns)\n",
    "        previous_results.to_csv(file_name, index=False)\n",
    "        print(f\"{file_name} created successfully.\")\n",
    "        \n",
    "    # Combine new + old\n",
    "    tots_results = pd.concat([new_results, previous_results], axis=0)\n",
    "\n",
    "    # Save\n",
    "    tots_results.to_csv(file_name, index=False)\n",
    "    print(f\"Results saved to {file_name}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa6e8c-b933-4f0c-8ea5-9c553217abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read your CSV file (already containing TPE vs. CMA-ES results)\n",
    "file_name = \"outputs/hpo_strategy.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# --- 1) Basic Cleanup ---\n",
    "# Remove '%' from \"Average Score\" so we can convert to float\n",
    "df['Average Score'] = pd.to_numeric(\n",
    "    df['Average Score'].str.replace('%', ''), \n",
    "    errors='coerce'\n",
    ")\n",
    "# Remove '±' and '%' from \"Standard Deviation\"\n",
    "df['Standard Deviation'] = pd.to_numeric(\n",
    "    df['Standard Deviation'].str.replace('±', '').str.replace('%', ''), \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "\n",
    "# Optionally remove an unwanted function\n",
    "# df = df[df['Function'] != 'ip']\n",
    "\n",
    "# Rename functions for your final labels\n",
    "df['Function'] = df['Function'].replace(function_mapping)\n",
    "\n",
    "# Optional replacements for dataset names\n",
    "df['Dataset'] = df['Dataset'].replace({\n",
    "    'JapaneseVowels':     'Japanese\\nVowels',\n",
    "})\n",
    "\n",
    "\n",
    "# 2b) distinguish TPE vs CMA-ES by hatch pattern (or alpha, edgecolor, etc.)\n",
    "sampler_hatching = {\n",
    "    'tpe':   '',     # no hatch\n",
    "    'cmaes': '///',  # diagonal hatch\n",
    "}\n",
    "\n",
    "# Filter to only those in the DataFrame\n",
    "functions = [f for f in functions_order if f in df['Function'].unique()]\n",
    "\n",
    "# If you also want an explicit order for Sampler\n",
    "samplers = ['tpe', 'cmaes']\n",
    "\n",
    "# --- 3) Build the grouped bar chart ---\n",
    "\n",
    "datasets = df['Dataset'].unique()\n",
    "datasets.sort()  # optional: sort datasets alphabetically\n",
    "x = np.arange(len(datasets))  # label locations\n",
    "width = 0.08                  # narrower bar for more sub-bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fontsize = 12\n",
    "\n",
    "# Outer loop over functions\n",
    "for i, func in enumerate(functions):\n",
    "    # Inner loop over the two samplers\n",
    "    for j, sampler in enumerate(samplers):\n",
    "        # Horizontal offset for each bar\n",
    "        # We have 2 samplers per function, so total sub-width = 2*width\n",
    "        offset = i * (2 * width) + j * width\n",
    "\n",
    "        # Extract rows for this function + sampler\n",
    "        sub_df = df[(df['Function'] == func) & (df['Sampler'] == sampler)]\n",
    "\n",
    "        # Merge with the dataset ordering to align bar positions\n",
    "        merged = pd.DataFrame({'Dataset': datasets}).merge(sub_df, on='Dataset', how='left')\n",
    "\n",
    "        # Plot the bars\n",
    "        ax.bar(\n",
    "            x + offset,\n",
    "            merged['Average Score'],\n",
    "            width,\n",
    "            label=None,  # We'll manually handle legend\n",
    "            yerr=merged['Standard Deviation'],\n",
    "            capsize=4,\n",
    "            color=function_colors.get(func, 'gray'),  # fallback color if missing\n",
    "            hatch=sampler_hatching.get(sampler, ''),  # or '' if missing\n",
    "            edgecolor='black'                         # optional to see the hatch better\n",
    "        )\n",
    "\n",
    "# --- 4) Cosmetic adjustments ---\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "\n",
    "# X-axis label\n",
    "ax.set_xlabel('Dataset', fontsize=fontsize)\n",
    "\n",
    "# Y-axis label (change if classification vs. NRMSE)\n",
    "# e.g., if your CSV is for classification, you might put \"Classification Rate (%)\"\n",
    "ax.set_ylabel('Average Score', fontsize=fontsize)\n",
    "\n",
    "# Position x-ticks in the center of each dataset group\n",
    "total_functions = len(functions)\n",
    "total_samplers = len(samplers)  # 2\n",
    "group_width = total_functions * total_samplers * width\n",
    "ax.set_xticks(x + group_width/2 - (width/2))\n",
    "ax.set_xticklabels(datasets, rotation=0)\n",
    "\n",
    "# --- 5) Build a custom legend ---\n",
    "# 5a) Legend for the Functions (colors)\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "function_legend = [\n",
    "    Patch(facecolor=function_colors[f], edgecolor='black', label=f) for f in functions\n",
    "]\n",
    "\n",
    "# 5b) Legend for the Samplers (hatching)\n",
    "sampler_legend = [\n",
    "    Patch(facecolor='white', edgecolor='black', hatch=sampler_hatching[s], \n",
    "          label=s.upper())  # or s.title()\n",
    "    for s in samplers\n",
    "]\n",
    "\n",
    "# Combine them in one line, or do them separately\n",
    "first_legend = ax.legend(handles=function_legend, title='Function', loc='upper left', fontsize=fontsize)\n",
    "ax.add_artist(first_legend)  # explicitly add the first legend, then a second one\n",
    "ax.legend(handles=sampler_legend, title='Sampler', loc='lower left', fontsize=fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a2af988df0f5e",
   "metadata": {},
   "source": [
    "# Test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e899e0651f36b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-12T09:25:33.718433Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from performances.utility import retrieve_best_model\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "variate_type = \"multi\"  # \"multi\" or \"uni\"\n",
    "\n",
    "\n",
    "for dataset_name in [\"FSDD\"]:\n",
    "    new_results = pd.DataFrame(columns=columns)\n",
    "    # Can be \"MackeyGlass\", \"Lorenz\", \"Sunspot_daily\", \"CatsDogs\", \"JapaneseVowels\", \"FSDD\", \"SpokenArabicDigits\", \"SPEECHCOMMANDS\"\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset_name, data_type, noise_std, visualize=True)\n",
    "    if is_instances_classification:\n",
    "        file_name = \"outputs/test_results/test_results_classification.csv\"\n",
    "    else: \n",
    "        file_name = \"outputs/test_results/test_results_prediction.csv\"\n",
    "    print(dataset_name)\n",
    "    # Simulate your data and loop for evaluation\n",
    "    \n",
    "    # \"random_ee\", \"random_ei\", \"diag_ee\", \"diag_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\", \"lstm_last\", \"rnn\"\n",
    "    for function_name in [\"ip-anti-oja_fast\"]:\n",
    "        print(function_name)\n",
    "\n",
    "        # dispatch to the correct evaluator\n",
    "        if function_name in [\"lstm_last\", \"rnn\", \"rnn-mean_hag\", \"gru\"]:\n",
    "            prefix = \"lstm_tpe\" if dataset_name == \"SPEECHCOMMANDS\" else \"new_tpe\"\n",
    "            study = retrieve_best_model(function_name, dataset_name, is_multivariate, variate_type = \"multi\", data_type = \"normal\", prefix=prefix)\n",
    "            scores = evaluate_dataset_on_test_rnn(\n",
    "                study,\n",
    "                dataset_name,\n",
    "                function_name,\n",
    "                pretrain_data,   # this arg is ignored by the LSTM evaluator\n",
    "                train_data,\n",
    "                test_data,\n",
    "                Y_train,\n",
    "                Y_test,\n",
    "                is_instances_classification,\n",
    "                nb_trials=8,\n",
    "                record_metrics=False\n",
    "            )\n",
    "        else:\n",
    "            study = retrieve_best_model(function_name, dataset_name, is_multivariate, variate_type = \"multi\", data_type = \"normal\", prefix=\"new_tpe\")\n",
    "\n",
    "            scores = evaluate_dataset_on_test(\n",
    "                study, \n",
    "                dataset_name,\n",
    "                function_name, \n",
    "                pretrain_data, \n",
    "                train_data, \n",
    "                test_data,\n",
    "                Y_train, \n",
    "                Y_test,\n",
    "                is_instances_classification,\n",
    "                nb_trials = 8,\n",
    "                record_metrics=False\n",
    "            )\n",
    "            \n",
    "        # Compute the average and standard deviation of the scores\n",
    "        average_score = np.mean(scores)\n",
    "        std_deviation = np.std(scores)\n",
    "    \n",
    "        if is_instances_classification:\n",
    "            formatted_average = f\"{round(average_score * 100, 5)} %\"\n",
    "            formatted_std = f\"± {round(std_deviation * 100, 5)} %\"\n",
    "        else:\n",
    "            formatted_average = f\"{round(average_score, 5)}\"\n",
    "            formatted_std = f\"± {round(std_deviation, 5)}\"\n",
    "        \n",
    "        # Capture the current date\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Create a new DataFrame row with the Date column\n",
    "        new_row = pd.DataFrame({\n",
    "            'Dataset': [dataset_name],\n",
    "            'Function': [function_name],\n",
    "            'Average Score': [formatted_average],\n",
    "            'Standard Deviation': [formatted_std],\n",
    "            'Date': [current_date]\n",
    "        })\n",
    "        \n",
    "        # Concatenate the new row to the results DataFrame\n",
    "        new_results = pd.concat([new_results, new_row], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(new_results)\n",
    "    \n",
    "    # Load the existing CSV\n",
    "    if os.path.exists(file_name):\n",
    "        previous_results = pd.read_csv(file_name)\n",
    "    else:\n",
    "        columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "        previous_results = pd.DataFrame(columns=columns)\n",
    "        previous_results.to_csv(file_name, index=False)\n",
    "        print(f\"{file_name} created successfully.\")\n",
    "        \n",
    "    tots_results = pd.concat([new_results, previous_results], axis=0)\n",
    "    \n",
    "    tots_results.to_csv(file_name, index=False)\n",
    "    print(f\"Results saved to {file_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d750b-febf-4fd8-8d6d-f58b2530fcf4",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc86944f1a29e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "file_name = \"outputs/test_results/test_results_classification.csv\"\n",
    "\n",
    "if 'file_name' not in locals() and 'file_name' not in globals():\n",
    "    file_name = \"outputs/test_results/test_results_prediction.csv\"  #  test_results_classification.csv or test_results_prediction.csv\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    previous_results = pd.read_csv(file_name)\n",
    "else:\n",
    "    # File does not exist, create it with the necessary columns\n",
    "    columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "    previous_results = pd.DataFrame(columns=columns)\n",
    "    # Save the empty DataFrame as a CSV\n",
    "    previous_results.to_csv(file_name, index=False)\n",
    "    print(f\"{file_name} created successfully.\")\n",
    "\n",
    "print(f\"Results saved to {file_name}.\")\n",
    "previous_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f301fa1838a4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "all_results = pd.read_csv(file_name)\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Clean data as before\n",
    "df['Average Score'] = df['Average Score'].astype(str).str.replace('%', '').astype(float)\n",
    "df['Standard Deviation'] = df['Standard Deviation'].str.replace('±', '').str.replace('%', '').astype(float)\n",
    "\n",
    "df = df[df['Function'] != 'ip']\n",
    "\n",
    "df['Function'] = df['Function'].map(function_mapping)\n",
    "\n",
    "# Optional replacements for dataset names\n",
    "#df = df[df['Dataset'].isin([\"Lorenz\", \"MackeyGlass\", \"Sunspot\"])]\n",
    "\n",
    "if file_name == \"test_results/test_results_classification.csv\":\n",
    "    df['Dataset'] = df['Dataset'].str.replace('SpokenArabicDigits', 'Spoken\\nArabic\\nDigits')\n",
    "    df['Dataset'] = df['Dataset'].str.replace('SPEECHCOMMANDS', 'SPEECH\\nCOMMANDS')\n",
    "    df['Dataset'] = df['Dataset'].str.replace('JapaneseVowels', 'Japanese\\nVowels')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "datasets = df['Dataset'].unique()\n",
    "x = np.arange(len(datasets))  # The label locations\n",
    "width = 0.1                   # Width of each bar\n",
    "\n",
    "for i, func in enumerate(functions_order):\n",
    "    # Grab only rows for this function\n",
    "    values = df[df['Function'] == func]\n",
    "    \n",
    "    # We create a Series in the same order as 'datasets'\n",
    "    merged = pd.DataFrame({'Dataset': datasets}).merge(values, on='Dataset', how='left')\n",
    "    \n",
    "    ax.bar(\n",
    "        x + i * width,\n",
    "        merged['Average Score'],\n",
    "        width,\n",
    "        label=func,\n",
    "        yerr=merged['Standard Deviation'],\n",
    "        capsize=5,\n",
    "        color=function_colors[func],\n",
    "#        log=True\n",
    "    )\n",
    "\n",
    "fontsize = 14\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "\n",
    "if file_name == \"test_results/test_results_prediction.csv\":\n",
    "    plt.ylabel('NRMSE', size=fontsize)\n",
    "else:\n",
    "    plt.ylabel('Classification Rate', size=fontsize)\n",
    "plt.legend(title='Algorithm', fontsize=10, title_fontsize=fontsize, bbox_to_anchor=(0.5, 1.15), loc='upper center', ncol=len(functions_order))\n",
    "\n",
    "# Position x-ticks in the center of all the bars for each dataset\n",
    "ax.set_xticks(x + width * (len(functions_order)-1)/2)\n",
    "ax.set_xticklabels(datasets)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae919b11422d98a",
   "metadata": {},
   "source": [
    "# Export best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8311b29a31c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from performances.utility import camel_to_snake, retrieve_best_model\n",
    "# Can be \"MackeyGlass\", \"Lorenz\", \"Sunspot_daily\", \"CatsDogs\", \"JapaneseVowels\", \"FSDD\", \"SpokenArabicDigits\", \"SPEECHCOMMANDS\"\n",
    "datasets = [\n",
    "    \"MackeyGlass\", \"Lorenz\", \"Sunspot_daily\", \"CatsDogs\", \"JapaneseVowels\", \"FSDD\", \"SpokenArabicDigits\", \"SPEECHCOMMANDS\",\n",
    "]\n",
    "\n",
    "# helper ────────────────────────────────────────────────────────────\n",
    "def smart_format(x, ndigits=5):\n",
    "    \"\"\"\n",
    "    • If |x| is smaller than 1e-3  → scientific notation with `ndigits` decimals.\n",
    "    • Otherwise                  → round to `ndigits` decimals (keeps 12345.6, 0.012345 …).\n",
    "    \"\"\"\n",
    "    if not isinstance(x, float):\n",
    "        return x                      # leave non-floats untouched\n",
    "    if x == 0.0:\n",
    "        return 0.0                    # keep plain zero\n",
    "    if abs(x) < 1e-3:                 # 0.000 … region\n",
    "        return f\"{x:.{ndigits}e}\"     # e.g. 1.23456e-04\n",
    "    return round(x, ndigits)          # ordinary decimal\n",
    "\n",
    "# main loop ─────────────────────────────────────────────────────────\n",
    " # \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\", \"lstm_last\", \"gru\"\n",
    "for fn_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\", \"lstm_last\", \"gru\"]:\n",
    "    rows = []\n",
    "    for ds in datasets:\n",
    "        prefix = \"lstm_tpe\" if (ds == \"SPEECHCOMMANDS\" and fn_name in [\"lstm_last\", \"gru\"]) else \"new_tpe\"\n",
    "        study = retrieve_best_model(\n",
    "            fn_name, ds,\n",
    "            is_multivariate=True, variate_type=\"multi\", data_type=\"normal\", prefix=prefix\n",
    "        )\n",
    "        params = {k: smart_format(v) for k, v in study.best_trial.params.items()}\n",
    "\n",
    "        rows.append({\n",
    "            \"dataset\": ds,\n",
    "            \"function_name\": fn_name,\n",
    "            **params,\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(\n",
    "        f\"outputs/best_hyperparameters_{fn_name}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    print(f\"Results saved to best_hyperparameters_{fn_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b411812-aa82-41da-8588-aa9999cc2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from performances.utility import retrieve_best_model\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------\n",
    "DATASETS = [\n",
    "    \"JapaneseVowels\", \"CatsDogs\", \"SpokenArabicDigits\", \"FSDD\",\n",
    "    \"SPEECHCOMMANDS\", \"MackeyGlass\", \"Lorenz\", \"Sunspot_daily\",\n",
    "]\n",
    "\n",
    "#     \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\", \"ip-anti-oja_fast\", \"hadsp\", \"desp\", \"lstm_last\", \"gru\",\n",
    "FUNCTIONS = [\n",
    "    \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\", \"ip-anti-oja_fast\", \"hadsp\", \"desp\", \"lstm_last\", \"gru\",\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"outputs/best_hyperparameters\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"best_cv_scores.csv\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Core collection loop\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "for fn_name in FUNCTIONS:\n",
    "    for ds_name in DATASETS:\n",
    "        # Grab the Optuna study that already contains the finished HPO\n",
    "        prefix = \"lstm_tpe\" if (ds_name == \"SPEECHCOMMANDS\" and fn_name in [\"lstm_last\", \"gru\"]) else \"new_tpe\"\n",
    "        study = retrieve_best_model(\n",
    "            fn_name, ds_name,\n",
    "            is_multivariate=True, variate_type=\"multi\", data_type=\"normal\", prefix=prefix\n",
    "        )\n",
    "\n",
    "        # Optuna exposes the optimum either as study.best_value or study.best_trial.value\n",
    "        best_score = getattr(study, \"best_value\", study.best_trial.value)\n",
    "\n",
    "        rows.append({\n",
    "                \"dataset\": ds_name,\n",
    "                \"function\": fn_name,\n",
    "                \"best_score\": best_score,\n",
    "        })\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Save to disk\n",
    "# ------------------------------------------------------------------\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Results saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cec6b61c1fecd",
   "metadata": {},
   "source": [
    "# Richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661626e2cfc509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from performances.utility import camel_to_snake, retrieve_best_model\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "columns = [\n",
    "    \"dataset\", \n",
    "    \"function_name\", \n",
    "    \"spectral_radius_mean\", \n",
    "    \"spectral_radius_std\", \n",
    "    \"pearson_mean\", \n",
    "    \"pearson_std\",\n",
    "    \"CEVD_mean\",\n",
    "    \"CEVD_std\",\n",
    "    \"dcor_mean\",\n",
    "    \"dcor_std\",\n",
    "    \"final_correlations_mean\",\n",
    "    \"final_correlations_std\",\n",
    "]\n",
    "\n",
    "\n",
    "# List of datasets (extract from filenames)\n",
    "datasets = [\n",
    "#    \"JapaneseVowels\",\n",
    "#    \"CatsDogs\",\n",
    "    \"FSDD\",\n",
    " #   \"SpokenArabicDigits\",\n",
    " #   \"SPEECHCOMMANDS\",\n",
    "#    \"MackeyGlass\",\n",
    "#    \"Lorenz\",\n",
    "#    \"Sunspot_daily\",\n",
    "]\n",
    "\n",
    "\n",
    "new_results = []\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset, data_type, noise_std, visualize=False)\n",
    "    for function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\", \"hadsp\", \"desp\"]: # \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\"\n",
    "        # Get the best trial from the study\n",
    "        print(function_name)\n",
    "        study = retrieve_best_model(function_name, dataset, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "        \n",
    "        SRs, pearsons, CEVs, dcors = evaluate_dataset_on_test(\n",
    "            study, \n",
    "            dataset,\n",
    "            function_name, \n",
    "            pretrain_data, \n",
    "            train_data, \n",
    "            test_data,\n",
    "            Y_train, \n",
    "            Y_test,\n",
    "            is_instances_classification,\n",
    "            nb_trials = 4,\n",
    "            record_metrics=True\n",
    "        )\n",
    "        # Create a new DataFrame row\n",
    "        new_row = pd.DataFrame({\n",
    "            \"dataset\": [dataset],\n",
    "            \"function_name\": [function_name],\n",
    "            \"spectral_radius_mean\": [np.mean(SRs)],\n",
    "            \"spectral_radius_std\": [np.std(SRs)],\n",
    "            \"pearson_mean\": [np.mean(pearsons)],\n",
    "            \"pearson_std\": [np.std(pearsons)],\n",
    "            \"CEVD_mean\": [np.mean(CEVs)],\n",
    "            \"CEVD_std\": [np.std(CEVs)],\n",
    "            \"dcor_mean\": [np.mean(dcors)],\n",
    "            \"dcor_std\": [np.std(dcors)],\n",
    "        })\n",
    "    \n",
    "        # Concatenate the new row to the results DataFrame\n",
    "        new_results.append(new_row)\n",
    "        \n",
    "\n",
    "# Display the DataFrame\n",
    "print(new_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5b109-081b-45b6-8af1-eb18793dab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"outputs/metrics.csv\"\n",
    "\n",
    "orig    = pd.read_csv(file_name).set_index([\"dataset\", \"function_name\"])\n",
    "corr_df = (\n",
    "    pd.concat(new_results, ignore_index=True)\n",
    "      .set_index([\"dataset\", \"function_name\"])\n",
    ")\n",
    "\n",
    "# Make sure both frames have the same columns\n",
    "for col in corr_df.columns:\n",
    "    if col not in orig.columns:\n",
    "        orig[col] = np.nan\n",
    "for col in orig.columns:\n",
    "    if col not in corr_df.columns:\n",
    "        corr_df[col] = np.nan\n",
    "\n",
    "# Union: keep all rows, prefer corr_df where available\n",
    "merged = corr_df.combine_first(orig)\n",
    "\n",
    "# If you want corr_df to overwrite even when not NaN:\n",
    "merged = orig.reindex(orig.index.union(corr_df.index))\n",
    "merged.loc[corr_df.index, corr_df.columns] = corr_df\n",
    "\n",
    "# Save back\n",
    "merged.reset_index().to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"✔ Added {len(set(corr_df.index) - set(orig.index))} new rows \"\n",
    "      f\"and updated {len(set(corr_df.index) & set(orig.index))} rows in {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104f22f-a28c-4f09-90f4-93d373963973",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e4e55-cdcf-4dab-ab18-f9cf8f66d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# ---------- vector-friendly text (Nature likes real text in PDFs) ----------\n",
    "mpl.rcParams['pdf.fonttype'] = 42      # embed TrueType\n",
    "mpl.rcParams['ps.fonttype']  = 42\n",
    "mpl.rcParams['svg.fonttype'] = 'none'  # keep text as text\n",
    "mpl.rcParams['font.family']  = ['Arial', 'DejaVu Sans']  # Arial if available\n",
    "\n",
    "# ------------------------------ load data -----------------------------------\n",
    "file_name = 'outputs/metrics.csv'\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "datasets_keep = [\n",
    "    \"JapaneseVowels\",\n",
    "    \"CatsDogs\",\n",
    "    \"FSDD\",\n",
    "    \"SpokenArabicDigits\",\n",
    "    \"SPEECHCOMMANDS\",\n",
    "]\n",
    "\n",
    "label_map = {\n",
    "    \"JapaneseVowels\":     \"Japanese\\nVowels\",\n",
    "    \"CatsDogs\":           \"Cats vs\\nDogs\",\n",
    "    \"FSDD\":               \"FSDD\",\n",
    "    \"SpokenArabicDigits\": \"Spoken\\nArabic\\nDigits\",\n",
    "    \"SPEECHCOMMANDS\":     \"SPEECH\\nCOMMANDS\",\n",
    "}\n",
    "\n",
    "# Stable order present in file\n",
    "datasets_order = [d for d in datasets_keep if d in data['dataset'].unique()]\n",
    "dataset_labels = [label_map[d] for d in datasets_order]\n",
    "\n",
    "data = data[data[\"dataset\"].isin(datasets_order)].copy()\n",
    "data[\"dataset\"] = data[\"dataset\"].map(label_map)\n",
    "\n",
    "# Map algorithm names; don't drop rows if mapping missing\n",
    "data[\"Algorithm\"] = data[\"function_name\"].map(function_mapping).fillna(data[\"function_name\"])\n",
    "\n",
    "# Keep only algorithms that actually appear, honoring your preferred order\n",
    "algos_present = list(dict.fromkeys(data[\"Algorithm\"].dropna()))\n",
    "algos = [a for a in functions_order if a in algos_present] or algos_present\n",
    "\n",
    "# ------------------------------- metrics ------------------------------------\n",
    "metrics     = [\"spectral_radius_mean\", \"pearson_mean\", \"CEVD_mean\", \"dcor_mean\"]\n",
    "err_metrics = [\"spectral_radius_std\",  \"pearson_std\",  \"CEVD_std\",  \"dcor_std\"]\n",
    "\n",
    "# ------------------------------ layout knobs --------------------------------\n",
    "fs_axis    = 8.5\n",
    "fs_tick    = 7.5\n",
    "width_bar  = min(0.72 / max(1, len(algos)), 0.10)   # slightly slimmer bars\n",
    "x          = np.arange(len(datasets_order))\n",
    "\n",
    "# 2×2 grid; reserve a slimmer band at the very top for legend; tighter hspace\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2,\n",
    "    figsize=(9.0, 5.8),\n",
    "    gridspec_kw=dict(left=0.085, right=0.995, top=0.84, bottom=0.12,\n",
    "                     wspace=0.40, hspace=0.30),\n",
    "    constrained_layout=False\n",
    ")\n",
    "axes = axes.ravel()\n",
    "\n",
    "handles = labels = None\n",
    "panel_titles = [\"(a)\", \"(b)\", \"(c)\", \"(d)\"]   # panel labels only\n",
    "\n",
    "for m, (metric, emetric) in enumerate(zip(metrics, err_metrics)):\n",
    "    ax = axes[m]\n",
    "\n",
    "    # Pivot to (dataset × algorithm) and align for equal-length series\n",
    "    mean_p = (data.pivot_table(index=\"dataset\", columns=\"Algorithm\", values=metric,  aggfunc=\"mean\")\n",
    "                    .reindex(index=dataset_labels, columns=algos))\n",
    "    std_p  = (data.pivot_table(index=\"dataset\", columns=\"Algorithm\", values=emetric, aggfunc=\"mean\")\n",
    "                    .reindex(index=dataset_labels, columns=algos))\n",
    "\n",
    "    # Bars --------------------------------------------------------------------\n",
    "    for i, alg in enumerate(algos):\n",
    "        means  = mean_p[alg].to_numpy()\n",
    "        errors = std_p[alg].to_numpy()\n",
    "        ax.bar(x + i*width_bar, means, width_bar,\n",
    "               label=alg, yerr=errors, capsize=2.5,\n",
    "               color=function_colors.get(alg, None),\n",
    "               error_kw={\"elinewidth\": 0.8, \"capthick\": 0.8})\n",
    "\n",
    "    # Panel label only (no caption text)\n",
    "    ax.set_title(panel_titles[m], loc='left', fontsize=fs_axis, fontweight='bold', pad=2)\n",
    "\n",
    "    # Axis cosmetics\n",
    "    ax.set_ylabel(metric.replace(\"_\", \" \").title(), fontsize=fs_axis)\n",
    "    ax.set_xticks(x + width_bar*(len(algos)-1)/2)\n",
    "    ax.set_xticklabels(dataset_labels, fontsize=fs_tick)\n",
    "    ax.tick_params(axis=\"y\", labelsize=fs_tick)\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.grid(axis='y', linestyle=':', linewidth=0.5, alpha=0.55)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    if m == 0:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# --------------------------- legend on TOP -----------------------------------\n",
    "if handles and labels:\n",
    "    fig.legend(handles, labels, title=\"Algorithm\",\n",
    "               ncol=min(7, len(labels)), frameon=False,\n",
    "               fontsize=fs_tick, title_fontsize=fs_tick,\n",
    "               loc=\"upper center\", bbox_to_anchor=(0.5, 0.995), borderaxespad=0.0)\n",
    "\n",
    "# -------------------------- save vector outputs ------------------------------\n",
    "out_dir = \"outputs/figures\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pdf_path = os.path.join(out_dir, \"Fig_dim_metrics_panel.pdf\")\n",
    "\n",
    "fig.savefig(pdf_path, bbox_inches=\"tight\")  # PRIMARY: use this PDF for submission\n",
    "\n",
    "# ------------------------------ quick preview --------------------------------\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da4ddc-eb48-49da-97bf-e9ada522fa95",
   "metadata": {},
   "source": [
    "# Separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54929f79-bfc1-46ad-a092-7609b9964a5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:34:40.607303Z",
     "start_time": "2025-04-25T14:34:40.590357Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from numpy import random\n",
    "\n",
    "# Evaluating\n",
    "from performances.esn_model_evaluation import train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from performances.esn_model_evaluation import (train_model_for_prediction, init_reservoir, init_ip_reservoir, init_local_rule_reservoir, init_ip_local_rule_reservoir, init_readout)\n",
    "from analysis.richness import spectral_radius, pearson, squared_uncoupled_dynamics_alternative, distance_correlation\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import analysis.separability\n",
    "from analysis.separability import (\n",
    "    inter_intra_class_distance,\n",
    "    fisher_discriminant_ratio,\n",
    "    silhouette,\n",
    "    davies_bouldin,\n",
    "    calinski_harabasz\n",
    ")\n",
    "reload(analysis.separability)\n",
    "\n",
    "nb_jobs = 10\n",
    "def evaluate_dataset_on_test_alternative(study, dataset_name, function_name, pretrain_data, train_data, test_data, Y_train, Y_test, is_instances_classification, nb_trials = 8, record_metrics=False):\n",
    "    # Collect all hyperparameters in a dictionary\n",
    "    hyperparams = {param_name: param_value for param_name, param_value in study.best_trial.params.items()}\n",
    "    print(hyperparams)\n",
    "    leaky_rate = 1\n",
    "    input_connectivity = 1\n",
    "\n",
    "    # score for prediction\n",
    "    if dataset_name == \"Sunspot\":\n",
    "        start_step = 30\n",
    "        end_step = 500\n",
    "    else:\n",
    "        start_step = 500\n",
    "        end_step = 1500\n",
    "    SLICE_RANGE = slice(start_step, end_step)\n",
    "\n",
    "    if 'variance_target' not in hyperparams and 'min_variance' in hyperparams:\n",
    "        hyperparams['variance_target'] = hyperparams['min_variance']\n",
    "    if not is_instances_classification:\n",
    "        hyperparams['use_full_instance'] = False\n",
    "\n",
    "    RIDGE_COEF = 10**hyperparams['ridge']\n",
    "    \n",
    "    if function_name in [\"hadsp\", \"desp\"]:\n",
    "        max_partners = np.inf\n",
    "    \n",
    "    inter_dists = []\n",
    "    intra_dists = []\n",
    "    separability_ratios = []\n",
    "    sil_scores = []\n",
    "    dbi_scores = []\n",
    "    ch_scores  = []\n",
    "\n",
    "    for i in range(nb_trials):\n",
    "        print(\"Trial\", i + 1, \"of\", nb_trials)\n",
    "        common_index = 1\n",
    "        if is_instances_classification:\n",
    "            common_size = pretrain_data[0].shape[common_index]\n",
    "        else:\n",
    "            common_size = pretrain_data.shape[common_index]\n",
    "\n",
    "        # We want the size of the models to be at least network_size\n",
    "        K = math.ceil(hyperparams['network_size'] / common_size)\n",
    "        n = common_size * K\n",
    "        \n",
    "        if function_name in [\"diag_ee\", \"diag_ei\"]:\n",
    "            use_block = True\n",
    "        else:\n",
    "            use_block = False\n",
    "            \n",
    "        # UNSUPERVISED PRETRAINING \n",
    "        if function_name == \"random_ee\":\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'],  K, w_distribution=stats.uniform(loc=0, scale=1), use_block=use_block, seed=random.randint(0, 1000))\n",
    "        else:\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'],  K, w_distribution=stats.uniform(loc=-1, scale=2), use_block=use_block, seed=random.randint(0, 1000))\n",
    "        bias *= hyperparams['bias_scaling']\n",
    "        Win *= hyperparams['input_scaling']\n",
    "\n",
    "        if function_name == \"hadsp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, \n",
    "                                     hyperparams['weight_increment'], hyperparams['target_rate'], hyperparams['rate_spread'], function_name, \n",
    "                                     multiple_instances=is_instances_classification, \n",
    "                                     min_increment = hyperparams['min_increment'], max_increment=hyperparams['max_increment'], use_full_instance=hyperparams['use_full_instance'],\n",
    "                                     max_partners=max_partners, method=\"pearson\", n_jobs=nb_jobs)\n",
    "        elif function_name == \"desp\":\n",
    "            print(\"DESP\")\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, \n",
    "                                         hyperparams['weight_increment'], hyperparams['variance_target'], hyperparams['variance_spread'], function_name, \n",
    "                                         multiple_instances=is_instances_classification, \n",
    "                                         min_increment = hyperparams['min_increment'], max_increment=hyperparams['max_increment'], use_full_instance = hyperparams['use_full_instance'], \n",
    "                                         max_partners=max_partners, method = \"pearson\", \n",
    "                                         intrinsic_saturation=hyperparams['intrinsic_saturation'], intrinsic_coef=hyperparams['intrinsic_coef'], \n",
    "                                         n_jobs = nb_jobs)\n",
    "        elif function_name in [\"random_ee\", \"random_ei\", \"diag_ee\", \"diag_ei\", \"ip_correct\", \"anti-oja_fast\", \"ip-anti-oja_fast\"]:\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= hyperparams['spectral_radius'] / max(abs(eigen))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid function: {function_name}\")\n",
    "        \n",
    "        # unsupervised local rules\n",
    "        if is_instances_classification:\n",
    "            unsupervised_pretrain = np.concatenate(pretrain_data).astype(float)\n",
    "        else:\n",
    "            unsupervised_pretrain = pretrain_data.astype(float)\n",
    "        if function_name == \"ip_correct\":\n",
    "            reservoir = init_ip_reservoir(W, Win, bias, mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                          leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function\n",
    "                                          )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"anti-oja_fast\":\n",
    "            reservoir = init_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                  synapse_normalization=False, bcm_theta=None,\n",
    "                                                  leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                  )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)        \n",
    "        elif function_name == \"ip-anti-oja_fast\":\n",
    "            reservoir = init_ip_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                      synapse_normalization=False, bcm_theta=None,\n",
    "                                                      mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                                      leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                      )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        else:\n",
    "            reservoir = init_reservoir(W, Win, bias, leaky_rate, activation_function)\n",
    "        readout = init_readout(ridge_coef=RIDGE_COEF)\n",
    "\n",
    "\n",
    "        # TRAINING and EVALUATION\n",
    "        # Step 1: Collect final hidden states\n",
    "        final_states = []\n",
    "        for seq in test_data:\n",
    "            states = reservoir.run(seq)\n",
    "            final_states.append(states[-1])\n",
    "\n",
    "        final_states = np.array(final_states)  # (n_test, reservoir_dim)\n",
    "\n",
    "        # Convert one-hot labels to flat labels\n",
    "        if Y_test.ndim == 2:\n",
    "            y_test = np.argmax(Y_test, axis=1)\n",
    "        else:\n",
    "            y_test = np.array(Y_test)\n",
    "\n",
    "        # Step 2: Compute inter/intra class distances\n",
    "        if is_instances_classification:\n",
    "            inter_dist, intra_dist, sep_ratio = inter_intra_class_distance(final_states, y_test)\n",
    "            sil = silhouette(final_states, y_test)\n",
    "            dbi = davies_bouldin(final_states, y_test)\n",
    "            ch  = calinski_harabasz(final_states, y_test)\n",
    "\n",
    "            inter_dists.append(inter_dist)\n",
    "            intra_dists.append(intra_dist)\n",
    "            separability_ratios.append(sep_ratio)\n",
    "\n",
    "            sil_scores.append(sil)\n",
    "            dbi_scores.append(dbi)\n",
    "            ch_scores.append(ch)\n",
    "\n",
    "    return {\n",
    "        'inter': inter_dists,\n",
    "        'intra': intra_dists,\n",
    "        'ratio': separability_ratios,\n",
    "        'silhouette': sil_scores,\n",
    "        'davies_bouldin': dbi_scores,\n",
    "        'calinski_harabasz': ch_scores,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a119ad50576960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T16:25:42.906788Z",
     "start_time": "2025-04-25T14:34:43.339261Z"
    }
   },
   "outputs": [],
   "source": [
    "from performances.utility import retrieve_best_model\n",
    "\n",
    "datasets = [\n",
    "#    \"JapaneseVowels\",\n",
    "#    \"CatsDogs\",\n",
    "    \"FSDD\",\n",
    "#    \"SpokenArabicDigits\",\n",
    "#    \"SPEECHCOMMANDS\",\n",
    "]\n",
    "\n",
    "\n",
    "corr_columns = []\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, \\\n",
    "        is_instances_classification = load_data(dataset, data_type, noise_std, visualize=False)\n",
    "\n",
    "    for function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\"]: # \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\", \"ip-anti-oja_fast\", \"hadsp\", \"desp\"\n",
    "        print(function_name)\n",
    "\n",
    "        study = retrieve_best_model(\n",
    "            function_name, dataset, is_multivariate,\n",
    "            variate_type=\"multi\", data_type=\"normal\"\n",
    "        )\n",
    "\n",
    "        results = evaluate_dataset_on_test_alternative(\n",
    "            study, dataset, function_name,\n",
    "            pretrain_data, train_data, test_data,\n",
    "            Y_train, Y_test,\n",
    "            is_instances_classification,\n",
    "            nb_trials=4,\n",
    "            record_metrics=True\n",
    "        )\n",
    "\n",
    "        # add *one* dict to corr_rows\n",
    "        corr_columns.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"function_name\": function_name,\n",
    "            \"inter_dists_mean\": np.mean(results['inter']),\n",
    "            \"inter_dists_std\":  np.std(results['inter']),\n",
    "            \"intra_dists_mean\": np.mean(results['intra']),\n",
    "            \"intra_dists_std\":  np.std(results['intra']),\n",
    "            \"separability_ratios_mean\": np.mean(results['ratio']),\n",
    "            \"separability_ratios_std\":  np.std(results['ratio']),\n",
    "            \"silhouette_mean\": np.mean(results['silhouette']),\n",
    "            \"silhouette_std\":  np.std(results['silhouette']),\n",
    "            \"davies_bouldin_mean\": np.mean(results['davies_bouldin']),\n",
    "            \"davies_bouldin_std\":  np.std(results['davies_bouldin']),\n",
    "            \"calinski_harabasz_mean\": np.mean(results['calinski_harabasz']),\n",
    "            \"calinski_harabasz_std\":  np.std(results['calinski_harabasz']),\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d452cab13b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"outputs/metrics.csv\"\n",
    "\n",
    "orig    = pd.read_csv(file_name).set_index([\"dataset\", \"function_name\"])\n",
    "corr_df = pd.DataFrame(corr_columns).set_index([\"dataset\", \"function_name\"])\n",
    "\n",
    "# Ensure all columns are present in both frames\n",
    "for col in corr_df.columns:\n",
    "    if col not in orig.columns:\n",
    "        orig[col] = np.nan\n",
    "for col in orig.columns:\n",
    "    if col not in corr_df.columns:\n",
    "        corr_df[col] = np.nan\n",
    "\n",
    "# Prefer new results; keep existing where new is NaN.\n",
    "merged = corr_df.combine_first(orig)\n",
    "\n",
    "# (Optional) if you want new to OVERWRITE existing non-NaNs too:\n",
    "merged = orig.copy()\n",
    "merged = merged.reindex(merged.index.union(corr_df.index))\n",
    "merged.loc[corr_df.index, corr_df.columns] = corr_df.values\n",
    "\n",
    "# Write out\n",
    "merged.reset_index().to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"✔ Wrote {len(corr_df)} rows (added {len(set(corr_df.index) - set(orig.index))}, \"\n",
    "      f\"updated {len(set(corr_df.index) & set(orig.index))}) to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ff25c-19a7-401c-83c9-1106c8de7b13",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb41d84baab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- figure: 2×2 top + centered narrow bottom (e) --------------------\n",
    "fig = plt.figure(figsize=(9.6, 6.6))\n",
    "gs = fig.add_gridspec(\n",
    "    3, 2,\n",
    "    left=0.08, right=0.995, top=0.90, bottom=0.10,\n",
    "    wspace=0.38, hspace=0.5\n",
    ")\n",
    "\n",
    "axes_top = [\n",
    "    fig.add_subplot(gs[0,0]),\n",
    "    fig.add_subplot(gs[0,1]),\n",
    "    fig.add_subplot(gs[1,0]),\n",
    "    fig.add_subplot(gs[1,1]),\n",
    "]\n",
    "\n",
    "# --- create a 1×3 sub-grid only for the bottom row; center col is panel (e)\n",
    "gs_bottom = gs[2, :].subgridspec(1, 3, width_ratios=[1.0, RANK_WIDTH_FRAC, 1.0], wspace=0.0)\n",
    "ax_rank   = fig.add_subplot(gs_bottom[0, 1])   # center = panel (e)\n",
    "# side “spacer” axes (hidden) so (e) is visually narrower\n",
    "for side in (fig.add_subplot(gs_bottom[0, 0]), fig.add_subplot(gs_bottom[0, 2])):\n",
    "    side.axis('off')\n",
    "\n",
    "handles = labels = None\n",
    "panel_labels = [\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\"]\n",
    "\n",
    "# --------------------------- panels (a)-(d) ----------------------------------\n",
    "for m, (metric, emetric) in enumerate(zip(metrics, error_metrics)):\n",
    "    ax = axes_top[m]\n",
    "\n",
    "    mean_p = (df.pivot_table(index=\"dataset_label\", columns=\"Algorithm\", values=metric,  aggfunc=\"mean\")\n",
    "                .reindex(index=datasets_labels, columns=algos))\n",
    "    std_p  = (df.pivot_table(index=\"dataset_label\", columns=\"Algorithm\", values=emetric, aggfunc=\"mean\")\n",
    "                .reindex(index=datasets_labels, columns=algos))\n",
    "\n",
    "    for i, alg in enumerate(algos):\n",
    "        means  = mean_p[alg].to_numpy()\n",
    "        errors = std_p[alg].to_numpy()\n",
    "        ax.bar(x + i*width_bar, means, width_bar,\n",
    "               label=alg, yerr=errors, capsize=2.5,\n",
    "               color=function_colors.get(alg, None),\n",
    "               error_kw={\"elinewidth\": 0.8, \"capthick\": 0.8})\n",
    "\n",
    "    ax.set_title(panel_labels[m], loc='left', fontsize=fs_axis, fontweight='bold', pad=2)\n",
    "\n",
    "    nice_name = metric.replace(\"_\", \" \").title()\n",
    "    if metric == \"davies_bouldin_mean\":\n",
    "        nice_name = \"Davies–Bouldin (↓ better)\"\n",
    "    ax.set_ylabel(nice_name, fontsize=fs_axis)\n",
    "\n",
    "    ax.set_xticks(x + width_bar*(len(algos)-1)/2)\n",
    "    ax.set_xticklabels(datasets_labels, fontsize=fs_tick)\n",
    "    ax.tick_params(axis=\"y\", labelsize=fs_tick)\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.grid(axis='y', linestyle=':', linewidth=0.5, alpha=0.6)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    if m == 0:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# --------------------------- panel (e): cumulative ranking -------------------\n",
    "RANK_WIDTH_FRAC = 1.15   # smaller => narrower panel (e)\n",
    "\n",
    "colors = [function_colors.get(alg, None) for alg in rank_sums.index]\n",
    "bars = ax_rank.bar(rank_sums.index, rank_sums.values, edgecolor='black', color=colors, linewidth=0.8)\n",
    "\n",
    "ax_rank.set_title(\"(e)\", loc='left', fontsize=fs_axis, fontweight='bold', pad=2)\n",
    "ax_rank.set_ylabel(\"Cumulative Rank\", fontsize=fs_axis)\n",
    "\n",
    "ax_rank.spines['top'].set_visible(False)\n",
    "ax_rank.spines['right'].set_visible(False)\n",
    "ax_rank.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax_rank.set_axisbelow(True)\n",
    "ax_rank.tick_params(axis=\"y\", labelsize=fs_tick)\n",
    "ax_rank.set_xticklabels(rank_sums.index, rotation=45, ha=\"right\", fontsize=fs_tick)\n",
    "\n",
    "ymax = float(rank_sums.max() if len(rank_sums) else 0)\n",
    "for bar, val in zip(bars, rank_sums.values):\n",
    "    ax_rank.text(\n",
    "        bar.get_x() + bar.get_width() * 0.5,\n",
    "        bar.get_height() + (0.02 * ymax if ymax > 0 else 0.2),\n",
    "        f\"{int(round(val))}\",\n",
    "        ha='center', va='bottom', fontsize=fs_tick\n",
    "    )\n",
    "    \n",
    "# --------------------------- bigger legend on TOP ----------------------------\n",
    "LEGEND_FS        = 9.5    # larger legend text\n",
    "LEGEND_TITLE_FS  = 10.5\n",
    "if handles and labels:\n",
    "    fig.legend(\n",
    "        handles, labels, title=\"Algorithm\",\n",
    "        ncol=min(7, len(labels)),\n",
    "        frameon=False,\n",
    "        fontsize=LEGEND_FS, title_fontsize=LEGEND_TITLE_FS,\n",
    "        handlelength=1.6, handletextpad=0.6, columnspacing=1.2, labelspacing=0.6,\n",
    "        loc=\"upper center\", bbox_to_anchor=(0.5, 1.025), borderaxespad=0.2\n",
    "    )\n",
    "\n",
    "# -------------------------- save vector outputs ------------------------------\n",
    "os.makedirs(\"outputs/figures\", exist_ok=True)\n",
    "fig.savefig(\"outputs/figures/Fig_cluster_metrics_panel.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd111ee-a499-4ae6-a11a-1dff3541399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lire le CSV\n",
    "df = pd.read_csv(\"outputs/metrics.csv\")\n",
    "\n",
    "# Arrondir toutes les colonnes numériques à 5 décimales\n",
    "df = df.round(5)\n",
    "\n",
    "# Sauvegarder le fichier modifié (optionnel)\n",
    "df.to_csv(\"outputs/metrics_rounded.csv\", index=False)\n",
    "\n",
    "print(\"Fichier arrondi sauvegardé dans outputs/Metrics_rounded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ce870-d31e-494d-bef4-e4408b1c119a",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070edf6-ecaf-4801-a0d0-3ed8b4e20dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and filter data\n",
    "data = pd.read_csv(\"outputs/metrics.csv\")\n",
    "datasets = [\"JapaneseVowels\", \"CatsDogs\", \"SpokenArabicDigits\", \"FSDD\", \"SPEECHCOMMANDS\"]\n",
    "df = data[data[\"dataset\"].isin(datasets)].copy()\n",
    "\n",
    "# Map function names to algorithm names\n",
    "df[\"Algorithm\"] = df[\"function_name\"].map(function_mapping)\n",
    "\n",
    "# Metrics used\n",
    "metrics = [\n",
    "    \"separability_ratios_mean\",\n",
    "    \"silhouette_mean\",\n",
    "    \"davies_bouldin_mean\",\n",
    "    \"calinski_harabasz_mean\"\n",
    "]\n",
    "\n",
    "# Average metrics per dataset-algorithm combination\n",
    "agg = df.groupby([\"dataset\", \"Algorithm\"])[metrics].mean().reset_index()\n",
    "\n",
    "algorithms = [\"E-ESN\", \"ESN\", \"IP\", \"Anti-Oja\", \"IP +\\nAnti-Oja\", \"mean HAG\", \"variance HAG\"]\n",
    "rank_sums = pd.Series(0, index=algorithms)\n",
    "\n",
    "# Compute ranks and sum across all dataset × metric combinations\n",
    "for ds in datasets:\n",
    "    sub = agg[agg[\"dataset\"] == ds].set_index(\"Algorithm\")\n",
    "    for metric in metrics:\n",
    "        ascending = metric == \"davies_bouldin_mean\"  # lower is better for DB, higher for others\n",
    "        ranks = sub[metric].rank(ascending=ascending, method=\"min\")\n",
    "        rank_sums += ranks.reindex(algorithms)\n",
    "\n",
    "# Sort by cumulative rank sums\n",
    "rank_sums = rank_sums.sort_values()\n",
    "\n",
    "# --------------\n",
    "# Plot\n",
    "\n",
    "# Plotting cumulative rank sums (polished for publication)\n",
    "plt.figure(figsize=(8, 5))\n",
    "colors = [function_colors[alg] for alg in rank_sums.index]\n",
    "bars = plt.bar(rank_sums.index, rank_sums.values, edgecolor='black', color=colors, linewidth=0.8)\n",
    "\n",
    "# Clean up spines \n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add horizontal grid lines for readability\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Labels and title in bold\n",
    "plt.ylabel(\"Cumulative Rank Sum\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# Ticks styling\n",
    "plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Annotate bars with integer labels\n",
    "ymax = rank_sums.max()\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() * 0.5,\n",
    "        height + ymax * 0.02,\n",
    "        f\"{int(height)}\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a468b-42a9-4c8e-ae77-cc0ebfaba1ae",
   "metadata": {},
   "source": [
    "# Final matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ce7dd-4826-4a68-ab44-66ef90f031de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from performances.utility import camel_to_snake, retrieve_best_model\n",
    "from models.reservoir import init_matrices\n",
    "from connexion_generation.hag import run_algorithm\n",
    "from scipy import sparse\n",
    "\n",
    "functions_order = [\n",
    "    'E-ESN',\n",
    "    'ESN',\n",
    "    'IP',\n",
    "    'Anti-Oja',\n",
    "    'IP +\\nAnti-Oja',\n",
    "    'mean HAG',\n",
    "    'variance HAG',\n",
    "#    'diag EE',\n",
    "#    'diag EI',\n",
    "]\n",
    "\n",
    "function_mapping = {\n",
    "    'ip-anti-oja_fast': 'IP +\\nAnti-Oja',\n",
    "    'ip_correct':       'IP',\n",
    "    'anti-oja_fast':    'Anti-Oja',\n",
    "    'desp':             'variance HAG',\n",
    "    'hadsp':            'mean HAG',\n",
    "    'random_ei':        'ESN',\n",
    "    'random_ee':        'E-ESN',\n",
    "    'diag_ei':        'diag EI',\n",
    "    'diag_ee':        'diag EE',\n",
    "}\n",
    "\n",
    "# List of datasets\n",
    "classification = [\n",
    "    \"JapaneseVowels\",\n",
    "    \"CatsDogs\",\n",
    "    \"FSDD\",\n",
    "    \"SpokenArabicDigits\",\n",
    "    \"SPEECHCOMMANDS\",\n",
    "]\n",
    "\n",
    "prediction = [\n",
    "    \"MackeyGlass\",\n",
    "    \"Lorenz\",\n",
    "    \"Sunspot_daily\",\n",
    "]\n",
    "datasets=classification\n",
    "\n",
    "\n",
    "# Initialize lists to store results and max values\n",
    "Ws = []\n",
    "titles = []\n",
    "max_values = []\n",
    "\n",
    "leaky_rate = 1\n",
    "input_connectivity = 1\n",
    "\n",
    "# Loop through datasets and function names to compute W matrices and find global vmax\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset, data_type, noise_std)\n",
    "\n",
    "    # \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\", \"ip-anti-oja_fast\",  \"hadsp\", \"desp\", \"diag_ee\", \"diag_ei\"\n",
    "    for function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\", \"ip-anti-oja_fast\",  \"hadsp\", \"desp\"]: #\"diag_ee\", \"diag_ei\",\n",
    "        print(function_name)\n",
    "        # Get the best trial from the study\n",
    "        study = retrieve_best_model(function_name, dataset, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "        hyperparams = {param_name: param_value for param_name, param_value in study.best_trial.params.items()}\n",
    "        print(hyperparams)\n",
    "\n",
    "        if 'variance_target' not in hyperparams and 'min_variance' in hyperparams:\n",
    "            hyperparams['variance_target'] = hyperparams['min_variance']\n",
    "        if not is_instances_classification:\n",
    "            hyperparams['use_full_instance'] = False\n",
    "    \n",
    "        if function_name in [\"hadsp\", \"desp\"]:\n",
    "            max_partners = np.inf\n",
    "        \n",
    "        common_index = 1\n",
    "        if is_instances_classification:\n",
    "            common_size = pretrain_data[0].shape[common_index]\n",
    "        else:\n",
    "            common_size = pretrain_data.shape[common_index]\n",
    "\n",
    "        # We want the size of the models to be at least network_size\n",
    "        K = math.ceil(hyperparams[\"network_size\"] / common_size)\n",
    "        n = common_size * K\n",
    "        \n",
    "        if function_name in [\"diag_ee\", \"diag_ei\"]:\n",
    "            use_block = True\n",
    "        else:\n",
    "            use_block = False\n",
    "            \n",
    "        # UNSUPERVISED PRETRAINING \n",
    "        if function_name == \"random_ee\":\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'],  K, w_distribution=stats.uniform(loc=0, scale=1), use_block=use_block, seed=random.randint(0, 1000))\n",
    "        else:\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'],  K, w_distribution=stats.uniform(loc=-1, scale=2), use_block=use_block, seed=random.randint(0, 1000))\n",
    "        bias *= hyperparams['bias_scaling']\n",
    "        Win *= hyperparams['input_scaling']\n",
    "\n",
    "        if function_name == \"hadsp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, \n",
    "                                     hyperparams['weight_increment'], hyperparams['target_rate'], hyperparams['rate_spread'], function_name, \n",
    "                                     multiple_instances=is_instances_classification, \n",
    "                                     min_increment = hyperparams['min_increment'], max_increment=hyperparams['max_increment'], use_full_instance=hyperparams['use_full_instance'],\n",
    "                                     max_partners=max_partners, method=\"pearson\", n_jobs=nb_jobs)\n",
    "        elif function_name == \"desp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, \n",
    "                                         hyperparams['weight_increment'], hyperparams['variance_target'], hyperparams['variance_spread'], function_name, \n",
    "                                         multiple_instances=is_instances_classification, \n",
    "                                         min_increment = hyperparams['min_increment'], max_increment=hyperparams['max_increment'], use_full_instance = hyperparams['use_full_instance'], \n",
    "                                         max_partners=max_partners, method = \"pearson\", \n",
    "                                         intrinsic_saturation=hyperparams['intrinsic_saturation'], intrinsic_coef=hyperparams['intrinsic_coef'], \n",
    "                                         n_jobs = nb_jobs)\n",
    "        elif function_name in [\"random_ee\", \"random_ei\"]:\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= hyperparams['spectral_radius'] / max(abs(eigen))\n",
    "            \n",
    "        elif function_name in [\"ip_correct\", \"anti-oja_fast\", \"ip-anti-oja_fast\"]:\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= hyperparams['spectral_radius'] / max(abs(eigen))\n",
    "\n",
    "            # unsupervised local rules\n",
    "            if is_instances_classification:\n",
    "                unsupervised_pretrain = np.concatenate(pretrain_data).astype(float)\n",
    "            else:\n",
    "                unsupervised_pretrain = pretrain_data.astype(float)\n",
    "            if function_name == \"ip_correct\":\n",
    "                reservoir = init_ip_reservoir(W, Win, bias, mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                              leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function\n",
    "                                              )\n",
    "                _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "            elif function_name == \"anti-oja_fast\":\n",
    "                reservoir = init_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                       synapse_normalization=True, bcm_theta=None,\n",
    "                                                       leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                       )\n",
    "                _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "            elif function_name == \"ip-anti-oja_fast\":\n",
    "                reservoir = init_ip_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                          synapse_normalization=True, bcm_theta=None,\n",
    "                                                          mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                                          leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                          )\n",
    "                _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "            else:\n",
    "                reservoir = init_reservoir(W, Win, bias, leaky_rate, activation_function)\n",
    "\n",
    "            W = reservoir.W\n",
    "            \n",
    "\n",
    "        # Store W matrix and corresponding title\n",
    "        Ws.append(W)\n",
    "        titles.append(f\"{dataset} - {[function_name]}\")\n",
    "        max_values.append(np.max(W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ccacef-d0d2-4633-9c30-a226789aa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "\n",
    "n_datasets = len(datasets)\n",
    "n_functions = len(functions_order)\n",
    "\n",
    "# 1) Determine global min/max to center color scale around 0\n",
    "# Flatten all Ws into one array to find overall min & max\n",
    "all_values = []\n",
    "for W in Ws:\n",
    "    if hasattr(W, \"toarray\"):\n",
    "        W = W.toarray()\n",
    "    all_values.append(W.ravel())\n",
    "all_values = np.concatenate(all_values)\n",
    "\n",
    "global_abs_max = np.max(np.abs(all_values))  # largest absolute value\n",
    "g_vmin, g_vmax = -global_abs_max, global_abs_max\n",
    "\n",
    "# 2) Create a figure with one row per dataset and one column per function\n",
    "fig, axes = plt.subplots(\n",
    "    n_datasets, \n",
    "    n_functions, \n",
    "    figsize=(n_functions * 3, n_datasets * 3),\n",
    "    sharex=True, \n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "# Ensure axes is 2D even if n_datasets=1 or n_functions=1\n",
    "if n_datasets == 1:\n",
    "    axes = np.array([axes])\n",
    "if n_functions == 1:\n",
    "    axes = np.array([axes]).T\n",
    "\n",
    "idx = 0\n",
    "im = None  # to store the last image for the colorbar\n",
    "for i in range(n_datasets):\n",
    "    for j in range(n_functions):\n",
    "        W = Ws[idx]\n",
    "        idx += 1\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(W, \"toarray\"):\n",
    "            W = W.toarray()\n",
    "        \n",
    "        # 3) Plot base heatmap, using a diverging colormap like 'bwr'\n",
    "        #    and a symmetric vmin/vmax around 0\n",
    "        local_abs_max = np.max(np.abs(W))\n",
    "        vmin, vmax = -local_abs_max, local_abs_max\n",
    "        \n",
    "        im = axes[i, j].imshow(\n",
    "            W, \n",
    "            cmap='seismic', \n",
    "            interpolation='nearest', \n",
    "            vmin=vmin, \n",
    "            vmax=vmax\n",
    "        )\n",
    "        \n",
    "        # 4) Overlay zeros in white\n",
    "        zero_mask = (W == 0)\n",
    "        axes[i, j].imshow(\n",
    "            np.ma.masked_where(~zero_mask, W),\n",
    "            cmap=mcolors.ListedColormap(['white']),\n",
    "            interpolation='nearest'\n",
    "        )\n",
    "        \n",
    "        # Remove per-subplot x/y labels\n",
    "        axes[i, j].set_xlabel('')\n",
    "        axes[i, j].set_ylabel('')\n",
    "        \n",
    "        # Row label: dataset name on the left edge\n",
    "        if j == 0:\n",
    "            axes[i, j].text(\n",
    "                -0.3, 0.5, datasets[i],\n",
    "                rotation=90,\n",
    "                transform=axes[i, j].transAxes,\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=15\n",
    "            )\n",
    "        \n",
    "        # Column label: function name on top row\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(functions_order[j], fontsize=15)\n",
    "            \n",
    "        # 5) Add a colorbar for each heatmap\n",
    "        #cbar = fig.colorbar(im, ax=axes[i, j], fraction=0.046, pad=0.04)\n",
    "        #cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "# 5) Add shared axis labels\n",
    "fig.text(0.5, 0.005, 'Neurons', ha='center', va='center', fontsize=14)\n",
    "fig.text(0.015, 0.5, 'Neurons', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "# Tight layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Single colorbar on the right\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), fraction=0.03, pad=0.01)\n",
    "cbar.set_label('Recurrent weight strength', fontsize=12)\n",
    "\n",
    "\n",
    "plt.savefig(f'connectivity_matrices.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19ecf7-5a8f-40b0-b6d2-b117373c7326",
   "metadata": {},
   "source": [
    "# Cumulated visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b06ffd97265926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T13:36:21.501439Z",
     "start_time": "2025-05-20T13:36:21.316283Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from svgpath2mpl import parse_path\n",
    "from matplotlib.markers import MarkerStyle\n",
    "\n",
    "# ---------- vector-friendly text (keep text as text in PDF/SVG) ----------\n",
    "mpl.rcParams['pdf.fonttype'] = 42      # embed TrueType\n",
    "mpl.rcParams['ps.fonttype']  = 42\n",
    "mpl.rcParams['svg.fonttype'] = 'none'  # keep text as text\n",
    "mpl.rcParams['font.family']  = ['Arial', 'DejaVu Sans']  # Arial if available\n",
    "\n",
    "def make_colorable_marker(svg_path, scale=1.0):\n",
    "    tree = ET.parse(svg_path)\n",
    "    root = tree.getroot()\n",
    "    ns = {\"svg\": \"http://www.w3.org/2000/svg\"}\n",
    "    d_list = [p.attrib[\"d\"] for p in root.findall(\".//svg:path\", ns)]\n",
    "    full_d = \" \".join(d_list)\n",
    "    p = parse_path(full_d)\n",
    "    p.vertices[:, 1] *= -1\n",
    "    p.vertices -= p.vertices.mean(axis=0)\n",
    "    p.vertices *= scale\n",
    "    return MarkerStyle(p)\n",
    "\n",
    "file_name = \"outputs/metrics.csv\"\n",
    "performance_file = \"outputs/test_results/test_results_classification.csv\"\n",
    "data = pd.read_csv(file_name)\n",
    "performance = pd.read_csv(performance_file)\n",
    "\n",
    "# ---------- keep the following datasets -------------------------\n",
    "datasets_keep = [ \"JapaneseVowels\", \"CatsDogs\", \"SpokenArabicDigits\", \"FSDD\", \"SPEECHCOMMANDS\"]\n",
    "data = data[data[\"dataset\"].isin(datasets_keep)]\n",
    "\n",
    "# ---------- map functions and datasets → human-readable ------\n",
    "data[\"dataset\"] = (data[\"dataset\"]\n",
    "                   .str.replace(\"SpokenArabicDigits\", \"Spoken\\nArabic\\nDigits\")\n",
    "                   .str.replace(\"SPEECHCOMMANDS\",     \"SPEECH\\nCOMMANDS\")\n",
    "                   .str.replace(\"JapaneseVowels\",     \"Japanese\\nVowels\"))\n",
    "data[\"Algorithm\"] = data[\"function_name\"].map(function_mapping)\n",
    "\n",
    "# ---------- marker for every dataset ---------------------\n",
    "dataset_marker = {\n",
    "    \"CatsDogs\":               r\"$🐱$\",  # ⚠️ If this glyph is missing, swap to an SVG via make_colorable_marker(...)\n",
    "    \"Japanese\\nVowels\":       r\"$¥$\",\n",
    "    \"Spoken\\nArabic\\nDigits\":  make_colorable_marker(\"analysis/icons/arabic.svg\", scale=0.015),\n",
    "    \"FSDD\":                   r\"$5$\",\n",
    "    \"SPEECH\\nCOMMANDS\":       make_colorable_marker(\"analysis/icons/microphone.svg\", scale=0.015),\n",
    "}\n",
    "\n",
    "# ---------- aggregate to one point per (Algorithm, Dataset) ----\n",
    "group = (\n",
    "    data.groupby([\"Algorithm\", \"dataset\"])\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "# ---------- merge in performance -----------\n",
    "performance[\"Algorithm\"] = performance[\"Function\"].map(function_mapping)\n",
    "performance[\"dataset\"] = (\n",
    "    performance[\"Dataset\"]\n",
    "    .str.replace(\"SpokenArabicDigits\", \"Spoken\\nArabic\\nDigits\")\n",
    "    .str.replace(\"SPEECHCOMMANDS\",     \"SPEECH\\nCOMMANDS\")\n",
    "    .str.replace(\"JapaneseVowels\",     \"Japanese\\nVowels\")\n",
    ")\n",
    "performance[\"score\"] = (\n",
    "    performance[\"Average Score\"]\n",
    "    .str.replace(\"%\", \"\", regex=False)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "group = pd.merge(\n",
    "    group,\n",
    "    performance[[\"Algorithm\", \"dataset\", \"score\"]],\n",
    "    on=[\"Algorithm\", \"dataset\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Normalize performance per dataset and size the markers\n",
    "group[\"score\"] = group.groupby(\"dataset\")[\"score\"].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.mean() if (x.max() - x.min()) == 0 else (x.max() - x.min()))\n",
    ")\n",
    "group[\"size\"] = group[\"score\"] * 1000  # Scale for visibility\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3)  Scatter plot :   decorrelation  vs  separability_score\n",
    "# ---------------------------------------------------------------\n",
    "metric_x = \"CEVD_mean\"\n",
    "metric_y = \"intra_dists_mean\"\n",
    "\n",
    "group['x_norm'] = group.groupby(\"dataset\")[metric_x].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "group['y_norm'] = group.groupby(\"dataset\")[metric_y].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "ax.set_facecolor(\"white\")\n",
    "\n",
    "for _, row in group.iterrows():\n",
    "    alg = row[\"Algorithm\"]\n",
    "    ds  = row[\"dataset\"]\n",
    "    ax.scatter(\n",
    "        row[metric_x], row[metric_y],\n",
    "        marker=dataset_marker.get(ds, \"o\"),\n",
    "        s=row[\"size\"],\n",
    "        edgecolor=\"none\",\n",
    "        facecolor=function_colors[alg],\n",
    "        linewidths=1.2,\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "# ---------- legends ---------------------------------------------\n",
    "alg_handles = [\n",
    "    Line2D([0], [0], marker=\"o\", linestyle=\"\",\n",
    "           markersize=12, markeredgecolor=\"none\",\n",
    "           markerfacecolor=clr, label=alg)\n",
    "    for alg, clr in function_colors.items()\n",
    "]\n",
    "\n",
    "ds_handles = [\n",
    "    Line2D([0], [0], marker=mk, linestyle=\"\",\n",
    "           markersize=14, markeredgecolor=\"none\",\n",
    "           markerfacecolor=\"black\",\n",
    "           label=ds.replace(\"\\n\", \" \"))\n",
    "    for ds, mk in dataset_marker.items()\n",
    "]\n",
    "\n",
    "legend1 = ax.legend(alg_handles, function_colors.keys(), title=\"Algorithm\", loc=\"upper left\", frameon=False)\n",
    "ax.add_artist(legend1)\n",
    "ax.legend(ds_handles, [d.replace(\"\\n\", \" \") for d in dataset_marker], title=\"Dataset\", loc=\"upper right\", frameon=False)\n",
    "\n",
    "# ---------- cosmetics -------------------------------------------\n",
    "for side in (\"left\", \"bottom\"):\n",
    "    ax.spines[side].set_visible(True)\n",
    "    ax.spines[side].set_linewidth(1.3)\n",
    "    ax.spines[side].set_color(\"black\")\n",
    "ax.xaxis.set_ticks_position(\"bottom\")\n",
    "ax.yaxis.set_ticks_position(\"left\")\n",
    "\n",
    "ax.set_xlabel(metric_x.replace(\"_\", \" \").title(), fontsize=16)\n",
    "ax.set_ylabel(metric_y.replace(\"_\", \" \").title(), fontsize=16)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ---------- SAVE VECTOR OUTPUTS (PDF primary, SVG optional) -----\n",
    "out_dir = \"outputs/figures\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "pdf_path = os.path.join(out_dir, \"Fig_scatter_CEVD_vs_Intra.pdf\")   # <-- use this PDF in submission\n",
    "\n",
    "fig.savefig(pdf_path, bbox_inches=\"tight\")      # vector PDF with embedded fonts\n",
    "fig.savefig(svg_path, bbox_inches=\"tight\")      # optional SVG for editing\n",
    "\n",
    "plt.show()\n",
    "print(f\"Saved:\\n - {pdf_path}\\n - {svg_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8fb2045fa0288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T13:16:32.600132Z",
     "start_time": "2025-05-15T13:16:32.598765Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d76b8-70f0-4152-80d7-c95dd62e1ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hag_env",
   "language": "python",
   "name": "hag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
