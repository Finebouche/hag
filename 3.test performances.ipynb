{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "SEED = 923984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cda032b8a2214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))tanh(x)\n",
    "activation_function = lambda x : tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Regression : http://tseregression.org/ + https://arxiv.org/pdf/2012.02974"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6272-708d-4709-8e35-5a84268bed64",
   "metadata": {},
   "source": [
    "Prediction Datasets available :\n",
    "\n",
    "* MackeyGlass\n",
    "* Lorenz\n",
    "* Sunspot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b219b-eb64-4715-b983-7de3c392f088",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Classification Datasets available :\n",
    "\n",
    "* Custom :  FSDD, HAART, JapaneseVowels\n",
    "* Aeon : SpokenArabicDigits, CatsDogs, LSST\n",
    "* Torchaudio: SPEECHCOMMANDS\n",
    "\n",
    "More on https://www.timeseriesclassification.com/dataset.php or https://pytorch.org/audio/stable/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ed105fb711998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import sparse, stats\n",
    "from datasets.load_forecasting import load_dataset_forecasting\n",
    "from datasets.load_classification import load_dataset_classification\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, StratifiedGroupKFold\n",
    "from datasets.preprocessing import flexible_indexing\n",
    "\n",
    "#Preprocessing\n",
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data\n",
    "from datasets.preprocessing import add_noise, duplicate_data\n",
    "from datasets.load_data import load_data as load_dataset\n",
    "\n",
    "# Define noise parameter\n",
    "noise_std = 0.001\n",
    "\n",
    "data_type = \"normal\" # \"normal\" ou \"noisy\"\n",
    "\n",
    "def load_data(dataset_name, data_type, noise_std, step_ahead=5, visualize=False):\n",
    "    (is_instances_classification, is_multivariate, sampling_rate,\n",
    "     X_train_raw, X_test_raw, Y_train_raw, Y_test,\n",
    "     use_spectral_representation, spectral_representation,\n",
    "     groups) = load_dataset(dataset_name, step_ahead, visualize=False)\n",
    "    \n",
    "    WINDOW_LENGTH = 10\n",
    "    freq_train_data = X_train_raw\n",
    "    flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "    extract_peak_frequencies(flat_train_data, sampling_rate, smooth=True, window_length=WINDOW_LENGTH, threshold=1e-5, nperseg=1024, visualize=False)\n",
    "    \n",
    "    if is_multivariate:\n",
    "        X_train_band, X_test_band = X_train_raw, X_test_raw\n",
    "        del X_train_raw, X_test_raw\n",
    "        X_val_band = None\n",
    "    else:\n",
    "        X_test, X_train = X_test_raw, X_train_raw\n",
    "        X_val, X_val_band = None, None\n",
    "        del X_train_raw, X_test_raw\n",
    "    Y_train = Y_train_raw\n",
    "    del Y_train_raw\n",
    "            \n",
    "    # PREPROCESSING    \n",
    "    freq_train_data = X_train_band if is_multivariate else X_train\n",
    "    flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "    peak_freqs = extract_peak_frequencies(flat_train_data, sampling_rate, smooth=True, window_length=WINDOW_LENGTH, threshold=1e-5, nperseg=1024, visualize=False)\n",
    "    \n",
    "    \n",
    "    if not is_multivariate:\n",
    "        X_train_band = generate_multivariate_dataset(\n",
    "            X_train, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "        \n",
    "        X_test_band = generate_multivariate_dataset(\n",
    "            X_test, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "    elif not use_spectral_representation:\n",
    "        X_train_band = generate_multivariate_dataset(\n",
    "            X_train_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "        X_test_band = generate_multivariate_dataset(\n",
    "            X_test_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "    else:\n",
    "        print(\"Data is already spectral and multivariate, nothing to do\")\n",
    "        \n",
    "    scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_band, X_val_band, X_test_band = scale_data(X_train_band, X_val_band, X_test_band, scaler_multi, is_instances_classification)\n",
    "                \n",
    "    if not is_multivariate:\n",
    "        scaler_x_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train, X_val, X_test = scale_data(X_train, X_val, X_test, scaler_multi, is_instances_classification)       \n",
    "    \n",
    "    # NOISE\n",
    "    if data_type == \"noisy\":\n",
    "        if is_instances_classification:\n",
    "            # UNI\n",
    "            if not is_multivariate:\n",
    "                X_train_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train, desc=\"TRAIN\")]\n",
    "                X_test_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test, desc=\"TEST\")]\n",
    "                \n",
    "            # MULTI\n",
    "            X_train_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train_band, desc=\"TRAIN\")]\n",
    "            X_test_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test_band, desc=\"TEST\")]\n",
    "        \n",
    "        else:  #if prediction\n",
    "            # UNI\n",
    "            if not is_multivariate:\n",
    "                X_train_noisy = add_noise(X_train, noise_std)\n",
    "                X_test_noisy = add_noise(X_test, noise_std)\n",
    "        \n",
    "            # MULTI\n",
    "            X_train_band_noisy = add_noise(X_train_band, noise_std)\n",
    "            X_test_band_noisy = add_noise(X_test_band, noise_std)\n",
    "    \n",
    "    # Define the number of instances you want to select\n",
    "    # Define the number of instances you want to select\n",
    "    x_size = len(X_train_band) if is_multivariate else len(X_train)\n",
    "    num_samples_for_pretrain = 500 if x_size >= 500 else x_size\n",
    "    if is_instances_classification:\n",
    "        indices = np.random.choice(x_size, num_samples_for_pretrain, replace=False)\n",
    "    else:\n",
    "        indices = range(x_size)\n",
    "    \n",
    "    \n",
    "    if data_type == \"noisy\":\n",
    "        # Defining pretrain   \n",
    "        if not is_multivariate:\n",
    "            X_pretrain_noisy = np.array(X_train_noisy, dtype=object)[indices].flatten()\n",
    "        X_pretrain_band_noisy = np.array(X_train_band_noisy, dtype=object)[indices]\n",
    "    \n",
    "    if not is_multivariate:\n",
    "        X_pretrain = np.array(X_train, dtype=object)[indices].flatten()\n",
    "    X_pretrain_band = np.array(X_train_band, dtype=object)[indices]\n",
    "\n",
    "    return X_pretrain_band, X_train_band, X_test_band, Y_train, Y_test, is_multivariate, is_instances_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e97986c1a958fc",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606001921c4818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from numpy import random\n",
    "import os\n",
    "\n",
    "\n",
    "# Evaluating\n",
    "from performances.esn_model_evaluation import train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from performances.esn_model_evaluation import (train_model_for_prediction, init_reservoir, init_ip_reservoir, init_local_rule_reservoir, \n",
    "                                                init_ip_local_rule_reservoir, init_readout)\n",
    "from analysis.richness import spectral_radius, pearson, squared_uncoupled_dynamics_alternative, condition_number\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.hag import run_algorithm\n",
    "\n",
    "nb_jobs = 10\n",
    "def evaluate_dataset_on_test(study, dataset_name, function_name, pretrain_data, train_data, test_data, Y_train, Y_test, is_instances_classification, nb_trials = 8, record_metrics=False):\n",
    "    # Collect all hyperparameters in a dictionary\n",
    "    hyperparams = {param_name: param_value for param_name, param_value in study.best_trial.params.items()}\n",
    "    print(hyperparams)\n",
    "    leaky_rate = 1\n",
    "    input_connectivity = 1\n",
    "\n",
    "    # score for prediction\n",
    "    if dataset_name == \"Sunspot\":\n",
    "        start_step = 30\n",
    "        end_step = 500\n",
    "    else:\n",
    "        start_step = 500\n",
    "        end_step = 1500\n",
    "    SLICE_RANGE = slice(start_step, end_step)\n",
    "\n",
    "    if 'variance_target' not in hyperparams and 'min_variance' in hyperparams:\n",
    "        hyperparams['variance_target'] = hyperparams['min_variance']\n",
    "    if not is_instances_classification:\n",
    "        hyperparams['use_full_instance'] = False\n",
    "\n",
    "    RIDGE_COEF = 10**hyperparams['ridge']\n",
    "    \n",
    "    if function_name in [\"hadsp\", \"desp\"]:\n",
    "        MAX_TIME_INCREMENT = hyperparams['time_increment'] + hyperparams['time_increment_span'] #int(max_window_size) or None or TIME_INCREMENT\n",
    "    \n",
    "    scores = [] \n",
    "    if record_metrics:\n",
    "        spectral_radii = []\n",
    "        pearson_correlations = []\n",
    "        CEVs = []\n",
    "        CNs = []\n",
    "    for i in range(nb_trials):\n",
    "        common_index = 1\n",
    "        if is_instances_classification:\n",
    "            common_size = pretrain_data[0].shape[common_index]\n",
    "        else:\n",
    "            common_size = pretrain_data.shape[common_index]\n",
    "\n",
    "        # We want the size of the reservoir to be at least network_size\n",
    "        K = math.ceil(hyperparams['network_size'] / common_size)\n",
    "        n = common_size * K\n",
    "    \n",
    "        # UNSUPERVISED PRETRAINING \n",
    "        if function_name == \"random_ei\":\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'],  K, w_distribution=stats.uniform(-1, 1), seed=random.randint(0, 1000))\n",
    "        else:\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'] ,  K, seed=random.randint(0, 1000))\n",
    "        bias *= hyperparams['bias_scaling']\n",
    "        Win *= hyperparams['input_scaling']\n",
    "\n",
    "        if function_name == \"hadsp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, hyperparams['time_increment'], hyperparams['weight_increment'],\n",
    "                                     hyperparams['target_rate'], hyperparams['rate_spread'], function_name, is_instance=is_instances_classification, use_full_instance=hyperparams['use_full_instance'],\n",
    "                                     max_increment=MAX_TIME_INCREMENT, max_partners=hyperparams['max_partners'], method=\"pearson\", n_jobs=nb_jobs)\n",
    "        elif function_name == \"desp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, hyperparams['time_increment'], hyperparams['weight_increment'],\n",
    "                                    hyperparams['variance_target'], hyperparams['variance_spread'], function_name, is_instance=is_instances_classification, \n",
    "                                    use_full_instance = hyperparams['use_full_instance'], max_increment=MAX_TIME_INCREMENT, max_partners=hyperparams['max_partners'], method = \"pearson\", \n",
    "                                    intrinsic_saturation=hyperparams['intrinsic_saturation'], intrinsic_coef=hyperparams['intrinsic_coef'], n_jobs = nb_jobs)\n",
    "        elif function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja\", \"ip-anti-oja\"]:\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= hyperparams['spectral_radius'] / max(abs(eigen))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid function: {function_name}\")\n",
    "        \n",
    "        # unsupervised local rules\n",
    "        if is_instances_classification:\n",
    "            unsupervised_pretrain = np.concatenate(pretrain_data).astype(float)\n",
    "        else:\n",
    "            unsupervised_pretrain = pretrain_data.astype(float)\n",
    "        if function_name == \"ip_correct\":\n",
    "            reservoir = init_ip_reservoir(W, Win, bias, mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                          leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function\n",
    "                                          )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"anti_oja\":\n",
    "            reservoir = init_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                   synapse_normalization=True, bcm_theta=None,\n",
    "                                                   leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                   )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"ip-anti-oja\":\n",
    "            reservoir = init_ip_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                      synapse_normalization=True, bcm_theta=None,\n",
    "                                                      mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                                      leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                      )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        else:\n",
    "            reservoir = init_reservoir(W, Win, bias, leaky_rate, activation_function)\n",
    "        readout = init_readout(ridge_coef=RIDGE_COEF)\n",
    "\n",
    "        \n",
    "        # TRAINING and EVALUATION\n",
    "        if is_instances_classification:\n",
    "            mode = \"sequence-to-vector\"\n",
    "            train_model_for_classification(reservoir, readout, train_data, Y_train, n_jobs = nb_jobs, mode=mode)\n",
    "\n",
    "            Y_pred = predict_model_for_classification(reservoir, readout, test_data, n_jobs = nb_jobs, mode=mode)\n",
    "            score = compute_score(Y_pred, Y_test, is_instances_classification)\n",
    "        else:\n",
    "            esn = train_model_for_prediction(reservoir, readout, train_data, Y_train, warmup=start_step)\n",
    "            \n",
    "            Y_pred =  esn.run(test_data, reset=False)\n",
    "            score = compute_score(Y_pred[SLICE_RANGE], Y_test[SLICE_RANGE], is_instances_classification)\n",
    "\n",
    "        scores.append(score)\n",
    "        if record_metrics:\n",
    "            states_history_multi = []\n",
    "            neurons_state = np.random.uniform(0, 1, bias.size)\n",
    "            inputs = np.concatenate(test_data, axis=0) if is_instances_classification else test_data\n",
    "            states_history_multi = reservoir.run(inputs)\n",
    "            \n",
    "            sr = spectral_radius(W)\n",
    "            pearson_correlation, _ = pearson(states_history_multi, num_windows=1, size_window=len(states_history_multi), step_size = 1, show_progress=True)\n",
    "            CEV = squared_uncoupled_dynamics_alternative(states_history_multi, num_windows=1, size_window=len(states_history_multi), step_size = 1, show_progress=True)\n",
    "\n",
    "            spectral_radii.append(sr)\n",
    "            pearson_correlations.append(pearson_correlation[0])\n",
    "            CEVs.append(CEV[0])\n",
    "            #CNs.append(CN[0])\n",
    "\n",
    "    if record_metrics:\n",
    "        return scores, spectral_radii, pearson_correlations, CEVs\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049fbb3-7c63-4ed1-b969-cd5b4a00bef6",
   "metadata": {},
   "source": [
    "# Common visualisation definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ae41a-4c2c-4d17-adcf-a812bdd9d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import color_palette\n",
    "\n",
    "# -- Define color palettes for each group --\n",
    "blues = color_palette(\"Blues\", 3)      # 3 shades of blue\n",
    "oranges = color_palette(\"Oranges\", 2)  # 2 shades of orange\n",
    "greens = color_palette(\"Greens\", 2)    # 2 shades of green\n",
    "\n",
    "# -- Map each function to its color --\n",
    "function_colors = {\n",
    "    'E-ESN':              greens[0],\n",
    "    'ESN':                greens[1],\n",
    "    'IP':                 blues[0],\n",
    "    'Anti-Oja':           blues[1],\n",
    "    'IP +\\nAnti-Oja':     blues[2],\n",
    "    'mean HAG':           oranges[0],\n",
    "    'variance HAG':       oranges[1],\n",
    "}\n",
    "\n",
    "# If you want a specific order for the bars, you can enforce it:\n",
    "functions_order = [\n",
    "    'E-ESN',\n",
    "    'ESN', \n",
    "    'IP', \n",
    "    'Anti-Oja', \n",
    "    'IP +\\nAnti-Oja', \n",
    "    'mean HAG', \n",
    "    'variance HAG', \n",
    "]\n",
    "\n",
    "function_mapping = {\n",
    "    'ip-anti-oja': 'IP +\\nAnti-Oja',\n",
    "    'ip_correct':  'IP',\n",
    "    'anti-oja':    'Anti-Oja',\n",
    "    'desp':        'variance HAG',\n",
    "    'hadsp':       'mean HAG',\n",
    "    'random_ei':   'ESN',\n",
    "    'random_ee':      'E-ESN',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bb5ed-a01e-4874-8b28-a9d7abde179d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HPO Comparaison"
   ]
  },
  {
   "cell_type": "raw",
   "id": "977ffefd-1fc2-473c-a960-3f2f8c626f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T22:54:20.816445Z",
     "iopub.status.busy": "2025-02-26T22:54:20.816130Z",
     "iopub.status.idle": "2025-02-26T23:05:00.885803Z",
     "shell.execute_reply": "2025-02-26T23:05:00.884567Z",
     "shell.execute_reply.started": "2025-02-26T22:54:20.816422Z"
    },
    "scrolled": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from performances.utility import camel_to_snake, retrieve_best_model\n",
    "\n",
    "# Your existing imports\n",
    "# from performances.utility import retrieve_best_model  # Now replaced with new retrieve_best_model above\n",
    "\n",
    "# Add \"Sampler\" to the columns\n",
    "columns = ['Dataset', 'Function', 'Sampler', 'Average Score', 'Standard Deviation', 'Date']\n",
    "variate_type = \"multi\"  # \"multi\" or \"uni\"\n",
    "file_name = \"outputs/hpo_strategy.csv\"\n",
    "\n",
    "for dataset_name in [\"JapaneseVowels\"]: \n",
    "    new_results = pd.DataFrame(columns=columns)\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset_name, data_type, noise_std, visualize=True)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"===== DATASET: {dataset_name} =====\")\n",
    "    \n",
    "    for sampler_name in [\"cmaes\", \"tpe\"]:\n",
    "        print(f\"--- Sampler: {sampler_name} ---\")\n",
    "        for function_name in [\"desp\", \"hadsp\", \"ip-anti-oja\", \"anti-oja\", \"ip_correct\", \"random_ee\", \"random_ei\"]:\n",
    "            print(\"Function:\", function_name)\n",
    "\n",
    "            # Retrieve the best study for that (sampler, function, dataset)\n",
    "            study = retrieve_best_model(\n",
    "                function_name=function_name,\n",
    "                dataset_name=dataset_name,\n",
    "                is_multivariate=is_multivariate,\n",
    "                variate_type=variate_type,\n",
    "                data_type=\"normal\",           # or \"noisy\" if needed\n",
    "                sampler_name=sampler_name\n",
    "            )\n",
    "\n",
    "            # Evaluate on test set\n",
    "            scores = evaluate_dataset_on_test(\n",
    "                study,\n",
    "                dataset_name,\n",
    "                function_name,\n",
    "                pretrain_data,\n",
    "                train_data,\n",
    "                test_data,\n",
    "                Y_train,\n",
    "                Y_test,\n",
    "                is_instances_classification,\n",
    "                record_metrics=False\n",
    "            )\n",
    "\n",
    "            average_score = np.mean(scores)\n",
    "            std_deviation = np.std(scores)\n",
    "\n",
    "            if is_instances_classification:\n",
    "                # Classification => format as percent\n",
    "                formatted_average = f\"{round(average_score * 100, 5)} %\"\n",
    "                formatted_std = f\"± {round(std_deviation * 100, 5)} %\"\n",
    "            else:\n",
    "                # Prediction => raw numeric\n",
    "                formatted_average = f\"{round(average_score, 5)}\"\n",
    "                formatted_std = f\"± {round(std_deviation, 5)}\"\n",
    "            \n",
    "            current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "            # Add to this dataset's results\n",
    "            new_row = pd.DataFrame({\n",
    "                'Dataset': [dataset_name],\n",
    "                'Function': [function_name],\n",
    "                'Sampler': [sampler_name],\n",
    "                'Average Score': [formatted_average],\n",
    "                'Standard Deviation': [formatted_std],\n",
    "                'Date': [current_date]\n",
    "            })\n",
    "            new_results = pd.concat([new_results, new_row], ignore_index=True)\n",
    "\n",
    "    # Show results for this dataset\n",
    "    print(\"\\n== New results for\", dataset_name, \"==\")\n",
    "    print(new_results)\n",
    "    \n",
    "    # Load or create local CSV file\n",
    "    if os.path.exists(file_name):\n",
    "        previous_results = pd.read_csv(file_name)\n",
    "    else:\n",
    "        columns = ['Dataset', 'Function', 'Sampler', 'Average Score', 'Standard Deviation', 'Date']\n",
    "        previous_results = pd.DataFrame(columns=columns)\n",
    "        previous_results.to_csv(file_name, index=False)\n",
    "        print(f\"{file_name} created successfully.\")\n",
    "        \n",
    "    # Combine new + old\n",
    "    tots_results = pd.concat([new_results, previous_results], axis=0)\n",
    "\n",
    "    # Save\n",
    "    tots_results.to_csv(file_name, index=False)\n",
    "    print(f\"Results saved to {file_name}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa6e8c-b933-4f0c-8ea5-9c553217abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Read your CSV file (already containing TPE vs. CMA-ES results)\n",
    "file_name = \"outputs/hpo_strategy.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# --- 1) Basic Cleanup ---\n",
    "# Remove '%' from \"Average Score\" so we can convert to float\n",
    "df['Average Score'] = pd.to_numeric(\n",
    "    df['Average Score'].str.replace('%', ''), \n",
    "    errors='coerce'\n",
    ")\n",
    "# Remove '±' and '%' from \"Standard Deviation\"\n",
    "df['Standard Deviation'] = pd.to_numeric(\n",
    "    df['Standard Deviation'].str.replace('±', '').str.replace('%', ''), \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "\n",
    "# Optionally remove an unwanted function\n",
    "# df = df[df['Function'] != 'ip']\n",
    "\n",
    "# Rename functions for your final labels\n",
    "df['Function'] = df['Function'].replace(function_mapping)\n",
    "\n",
    "# Optional replacements for dataset names\n",
    "df['Dataset'] = df['Dataset'].replace({\n",
    "    'JapaneseVowels':     'Japanese\\nVowels',\n",
    "})\n",
    "\n",
    "\n",
    "# 2b) distinguish TPE vs CMA-ES by hatch pattern (or alpha, edgecolor, etc.)\n",
    "sampler_hatching = {\n",
    "    'tpe':   '',     # no hatch\n",
    "    'cmaes': '///',  # diagonal hatch\n",
    "}\n",
    "\n",
    "# Filter to only those in the DataFrame\n",
    "functions = [f for f in functions_order if f in df['Function'].unique()]\n",
    "\n",
    "# If you also want an explicit order for Sampler\n",
    "samplers = ['tpe', 'cmaes']\n",
    "\n",
    "# --- 3) Build the grouped bar chart ---\n",
    "\n",
    "datasets = df['Dataset'].unique()\n",
    "datasets.sort()  # optional: sort datasets alphabetically\n",
    "x = np.arange(len(datasets))  # label locations\n",
    "width = 0.08                  # narrower bar for more sub-bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fontsize = 12\n",
    "\n",
    "# Outer loop over functions\n",
    "for i, func in enumerate(functions):\n",
    "    # Inner loop over the two samplers\n",
    "    for j, sampler in enumerate(samplers):\n",
    "        # Horizontal offset for each bar\n",
    "        # We have 2 samplers per function, so total sub-width = 2*width\n",
    "        offset = i * (2 * width) + j * width\n",
    "\n",
    "        # Extract rows for this function + sampler\n",
    "        sub_df = df[(df['Function'] == func) & (df['Sampler'] == sampler)]\n",
    "\n",
    "        # Merge with the dataset ordering to align bar positions\n",
    "        merged = pd.DataFrame({'Dataset': datasets}).merge(sub_df, on='Dataset', how='left')\n",
    "\n",
    "        # Plot the bars\n",
    "        ax.bar(\n",
    "            x + offset,\n",
    "            merged['Average Score'],\n",
    "            width,\n",
    "            label=None,  # We'll manually handle legend\n",
    "            yerr=merged['Standard Deviation'],\n",
    "            capsize=4,\n",
    "            color=function_colors.get(func, 'gray'),  # fallback color if missing\n",
    "            hatch=sampler_hatching.get(sampler, ''),  # or '' if missing\n",
    "            edgecolor='black'                         # optional to see the hatch better\n",
    "        )\n",
    "\n",
    "# --- 4) Cosmetic adjustments ---\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "\n",
    "# X-axis label\n",
    "ax.set_xlabel('Dataset', fontsize=fontsize)\n",
    "\n",
    "# Y-axis label (change if classification vs. NRMSE)\n",
    "# e.g., if your CSV is for classification, you might put \"Classification Rate (%)\"\n",
    "ax.set_ylabel('Average Score', fontsize=fontsize)\n",
    "\n",
    "# Position x-ticks in the center of each dataset group\n",
    "total_functions = len(functions)\n",
    "total_samplers = len(samplers)  # 2\n",
    "group_width = total_functions * total_samplers * width\n",
    "ax.set_xticks(x + group_width/2 - (width/2))\n",
    "ax.set_xticklabels(datasets, rotation=0)\n",
    "\n",
    "# --- 5) Build a custom legend ---\n",
    "# 5a) Legend for the Functions (colors)\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "function_legend = [\n",
    "    Patch(facecolor=function_colors[f], edgecolor='black', label=f) for f in functions\n",
    "]\n",
    "\n",
    "# 5b) Legend for the Samplers (hatching)\n",
    "sampler_legend = [\n",
    "    Patch(facecolor='white', edgecolor='black', hatch=sampler_hatching[s], \n",
    "          label=s.upper())  # or s.title()\n",
    "    for s in samplers\n",
    "]\n",
    "\n",
    "# Combine them in one line, or do them separately\n",
    "first_legend = ax.legend(handles=function_legend, title='Function', loc='upper left', fontsize=fontsize)\n",
    "ax.add_artist(first_legend)  # explicitly add the first legend, then a second one\n",
    "ax.legend(handles=sampler_legend, title='Sampler', loc='lower left', fontsize=fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a2af988df0f5e",
   "metadata": {},
   "source": [
    "# Test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e899e0651f36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from performances.utility import retrieve_best_model\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "variate_type = \"multi\"  # \"multi\" or \"uni\"\n",
    "\n",
    "for dataset_name in [\"SpokenArabicDigits\"]: \n",
    "    new_results = pd.DataFrame(columns=columns)\n",
    "    # Can be \"MackeyGlass\", \"Lorenz\", \"Sunspot\", \"Henon\", \"NARMA\", \"CatsDogs\", \"FSDD\", \"JapaneseVowels\", \"SPEECHCOMMANDS\", \"SpokenArabicDigits\", \"Henon\"\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset_name, data_type, noise_std, visualize=True)\n",
    "    if is_instances_classification:\n",
    "        file_name = \"outputs/test_results_classification.csv\"\n",
    "    else: \n",
    "        file_name = \"outputs/test_results_prediction.csv\"\n",
    "    print(dataset_name)\n",
    "    # Simulate your data and loop for evaluation\n",
    "\n",
    "    for function_name in [\"ip_correct\", \"random_ei\", \"anti-oja\"]: # \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja\",  \"ip-anti-oja\", \"hadsp\", \"desp\"\n",
    "        print(function_name)\n",
    "        study = retrieve_best_model(function_name, dataset_name, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "    \n",
    "        scores = evaluate_dataset_on_test(\n",
    "            study, \n",
    "            dataset_name,\n",
    "            function_name, \n",
    "            pretrain_data, \n",
    "            train_data, \n",
    "            test_data,\n",
    "            Y_train, \n",
    "            Y_test,\n",
    "            is_instances_classification,\n",
    "            record_metrics=False\n",
    "        )\n",
    "        # Compute the average and standard deviation of the scores\n",
    "        average_score = np.mean(scores)\n",
    "        std_deviation = np.std(scores)\n",
    "    \n",
    "        if is_instances_classification:\n",
    "            formatted_average = f\"{round(average_score * 100, 5)} %\"\n",
    "            formatted_std = f\"± {round(std_deviation * 100, 5)} %\"\n",
    "        else:\n",
    "            formatted_average = f\"{round(average_score, 5)}\"\n",
    "            formatted_std = f\"± {round(std_deviation, 5)}\"\n",
    "        \n",
    "        # Capture the current date\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Create a new DataFrame row with the Date column\n",
    "        new_row = pd.DataFrame({\n",
    "            'Dataset': [dataset_name],\n",
    "            'Function': [function_name],\n",
    "            'Average Score': [formatted_average],\n",
    "            'Standard Deviation': [formatted_std],\n",
    "            'Date': [current_date]\n",
    "        })\n",
    "        \n",
    "        # Concatenate the new row to the results DataFrame\n",
    "        new_results = pd.concat([new_results, new_row], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(new_results)\n",
    "    \n",
    "    # Load the existing CSV\n",
    "    if os.path.exists(file_name):\n",
    "        previous_results = pd.read_csv(file_name)\n",
    "    else:\n",
    "        columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "        previous_results = pd.DataFrame(columns=columns)\n",
    "        previous_results.to_csv(file_name, index=False)\n",
    "        print(f\"{file_name} created successfully.\")\n",
    "        \n",
    "    tots_results = pd.concat([new_results, previous_results], axis=0)\n",
    "    \n",
    "    tots_results.to_csv(file_name, index=False)\n",
    "    print(f\"Results saved to {file_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e50beaff27b87",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc86944f1a29e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "file_name = \"outputs/test_results_classification.csv\"\n",
    "\n",
    "if 'file_name' not in locals() and 'file_name' not in globals():\n",
    "    file_name = \"outputs/test_results_prediction.csv\"  #  test_results_classification.csv or test_results_prediction.csv\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    previous_results = pd.read_csv(file_name)\n",
    "else:\n",
    "    # File does not exist, create it with the necessary columns\n",
    "    columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "    previous_results = pd.DataFrame(columns=columns)\n",
    "    # Save the empty DataFrame as a CSV\n",
    "    previous_results.to_csv(file_name, index=False)\n",
    "    print(f\"{file_name} created successfully.\")\n",
    "\n",
    "print(f\"Results saved to {file_name}.\")\n",
    "previous_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cffa1d012ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from seaborn import color_palette\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "all_results = pd.read_csv(file_name)\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Clean data as before\n",
    "df['Average Score'] = df['Average Score'].astype(str).str.replace('%', '').astype(float)\n",
    "df['Standard Deviation'] = df['Standard Deviation'].str.replace('±', '').str.replace('%', '').astype(float)\n",
    "\n",
    "df = df[df['Function'] != 'ip']\n",
    "\n",
    "df['Function'] = df['Function'].str.replace('ip-anti-oja', 'IP +\\nAnti-Oja')\n",
    "df['Function'] = df['Function'].str.replace('ip_correct', 'IP')\n",
    "df['Function'] = df['Function'].str.replace('anti-oja', 'Anti-Oja')\n",
    "\n",
    "df['Function'] = df['Function'].str.replace('desp', 'variance HAG')\n",
    "df['Function'] = df['Function'].str.replace('hadsp', 'mean HAG')\n",
    "df['Function'] = df['Function'].str.replace('random_ei', 'ESN')\n",
    "df['Function'] = df['Function'].str.replace('random_ee', 'E-ESN')\n",
    "\n",
    "# Optional replacements for dataset names\n",
    "#df = df[df['Dataset'].isin([\"Lorenz\", \"MackeyGlass\", \"Sunspot\"])]\n",
    "\n",
    "\n",
    "if file_name == \"outputs/test_results_classification.csv\":\n",
    "    df['Dataset'] = df['Dataset'].str.replace('SpokenArabicDigits', 'Spoken\\nArabic\\nDigits')\n",
    "    df['Dataset'] = df['Dataset'].str.replace('SPEECHCOMMANDS', 'SPEECH\\nCOMMANDS')\n",
    "    df['Dataset'] = df['Dataset'].str.replace('JapaneseVowels', 'Japanese\\nVowels')\n",
    "\n",
    "# Filter out (or keep) only the functions actually present in df\n",
    "functions = [f for f in functions_order if f in df['Function'].unique()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "datasets = df['Dataset'].unique()\n",
    "x = np.arange(len(datasets))  # The label locations\n",
    "width = 0.1                   # Width of each bar\n",
    "\n",
    "for i, func in enumerate(functions):\n",
    "    # Grab only rows for this function\n",
    "    values = df[df['Function'] == func]\n",
    "    \n",
    "    # We create a Series in the same order as 'datasets'\n",
    "    merged = pd.DataFrame({'Dataset': datasets}).merge(values, on='Dataset', how='left')\n",
    "    \n",
    "    ax.bar(\n",
    "        x + i * width,\n",
    "        merged['Average Score'],\n",
    "        width,\n",
    "        label=func,\n",
    "        yerr=merged['Standard Deviation'],\n",
    "        capsize=5,\n",
    "        color=function_colors[func]\n",
    "    )\n",
    "\n",
    "fontsize = 14\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Dataset', size=fontsize)\n",
    "if file_name == \"outputs/test_results_prediction.csv\":\n",
    "    plt.ylabel('NRMSE', size=fontsize)\n",
    "else:\n",
    "    plt.ylabel('Classification Rate', size=fontsize)\n",
    "plt.legend(title='Algorithm', fontsize=fontsize)\n",
    "\n",
    "# Position x-ticks in the center of all the bars for each dataset\n",
    "ax.set_xticks(x + width * (len(functions)-1)/2)\n",
    "ax.set_xticklabels(datasets)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae919b11422d98a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fca6f143a11566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from performances.utility import camel_to_snake, retrieve_best_model\n",
    "\n",
    "\n",
    "# List of datasets (extract from filenames)\n",
    "datasets = [\n",
    "    \"SpokenArabicDigits\",\n",
    "    \"JapaneseVowels\",\n",
    "    \"FSDD\",\n",
    "    \"SPEECHCOMMANDS\",\n",
    "    \"CatsDogs\",\n",
    "    \"MackeyGlass\",\n",
    "    \"Lorenz\",\n",
    "    \"Sunspot_daily\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "function_name = \"ip-anti-oja\"\n",
    "for dataset in datasets:\n",
    "    study = retrieve_best_model(function_name, dataset, is_multivariate=True, variate_type = \"multi\", data_type = \"normal\")\n",
    "    best_trial = study.best_trial\n",
    "    results.append({\n",
    "        \"dataset\": dataset,\n",
    "        \"function_name\": function_name,\n",
    "        **best_trial.params,\n",
    "    })\n",
    "# Convert results to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f\"outputs/best_hyperparameters_{function_name}.csv\", index=False)\n",
    "\n",
    "print(f\"Results saved to best_hyperparameters_{function_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d89caef9e9bbd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb297949e789e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from performances.utility import camel_to_snake, retrieve_best_model\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "columns = [\n",
    "    \"dataset\", \n",
    "    \"function_name\", \n",
    "    \"spectral_radius_mean\", \n",
    "    \"spectral_radius_std\", \n",
    "    \"pearson_mean\", \n",
    "    \"pearson_std\",\n",
    "    \"CEV_mean\",\n",
    "    \"CEV_std\",\n",
    "    \"scores_mean\",\n",
    "    \"scores_std\",\n",
    "]\n",
    "\n",
    "new_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "# List of datasets (extract from filenames)\n",
    "datasets = [\n",
    "#    \"CatsDogs\",\n",
    "#    \"JapaneseVowels\",\n",
    "#    \"SpokenArabicDigits\",\n",
    "#    \"FSDD\",\n",
    "    \"SPEECHCOMMANDS\",\n",
    "#    \"MackeyGlass\",\n",
    "#    \"Lorenz\",\n",
    "#    \"Sunspot_daily\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset, data_type, noise_std, visualize=False)\n",
    "    for function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja\", \"ip-anti-oja\",  \"hadsp\", \"desp\"]:\n",
    "        # Get the best trial from the study\n",
    "        print(function_name)\n",
    "        study = retrieve_best_model(function_name, dataset, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "        \n",
    "        scores, SRs, pearsons, CEVs = evaluate_dataset_on_test(\n",
    "            study, \n",
    "            dataset,\n",
    "            function_name, \n",
    "            pretrain_data, \n",
    "            train_data, \n",
    "            test_data,\n",
    "            Y_train, \n",
    "            Y_test,\n",
    "            is_instances_classification,\n",
    "            nb_trials = 4,\n",
    "            record_metrics=True\n",
    "        )\n",
    "        print(scores)\n",
    "        # Create a new DataFrame row\n",
    "        new_row = pd.DataFrame({\n",
    "            \"dataset\": [dataset],\n",
    "            \"function_name\": [function_name],\n",
    "            \"spectral_radius_mean\": [np.mean(SRs)],\n",
    "            \"spectral_radius_std\": [np.std(SRs)],\n",
    "            \"pearson_mean\": [np.mean(pearsons)],\n",
    "            \"pearson_std\": [np.std(pearsons)],\n",
    "            \"CEV_mean\": [np.mean(CEVs)],\n",
    "            \"CEV_std\": [np.std(CEVs)],\n",
    "            \"scores_mean\": [np.mean(scores)],\n",
    "            \"scores_std\": [np.std(scores)],\n",
    "        })\n",
    "    \n",
    "        # Concatenate the new row to the results DataFrame\n",
    "        new_results = pd.concat([new_results, new_row], ignore_index=True)\n",
    "        \n",
    "\n",
    "# Display the DataFrame\n",
    "print(new_results)\n",
    "file_name = \"metrics.csv\"\n",
    "\n",
    "# Load the existing CSV\n",
    "if os.path.exists(file_name) and os.path.getsize(file_name) > 0:\n",
    "    try:\n",
    "        previous_results = pd.read_csv(file_name)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"{file_name} is empty. Initializing with default columns.\")\n",
    "        previous_results = pd.DataFrame(columns=columns)\n",
    "        previous_results.to_csv(file_name, index=False)\n",
    "else:\n",
    "    print(f\"{file_name} does not exist or is empty. Creating a new file.\")\n",
    "    previous_results = pd.DataFrame(columns=columns)\n",
    "    previous_results.to_csv(file_name, index=False)\n",
    "    \n",
    "tots_results = pd.concat([new_results, previous_results], axis=0)\n",
    "\n",
    "tots_results.to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"Results saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e4e55-cdcf-4dab-ab18-f9cf8f66d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import color_palette\n",
    "from matplotlib.ticker import MaxNLocator  # Importing here for completeness\n",
    "\n",
    "# Load your data\n",
    "file_name = 'metrics.csv'\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "# Define the mapping from function_name to descriptive labels\n",
    "function_mapping = {\n",
    "    'random': 'E-ESN',\n",
    "    'random_ei': 'ESN',\n",
    "    'ip_correct': 'IP',\n",
    "    'anti-oja': 'Anti-Oja',\n",
    "    'ip-anti-oja': 'IP +\\nAnti-Oja',\n",
    "    'hadsp': 'mean HAG',      \n",
    "    'desp': 'variance HAG'  \n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    \"CatsDogs\",\n",
    "    \"JapaneseVowels\",\n",
    "    \"SpokenArabicDigits\",\n",
    "    \"FSDD\",\n",
    "    \"SPEECHCOMMANDS\",\n",
    "    \"MackeyGlass\",\n",
    "    \"Lorenz\",\n",
    "    \"Sunspot_daily\",\n",
    "]\n",
    "\n",
    "\n",
    "# Optional replacements for dataset names\n",
    "data = data[data['dataset'].isin(datasets)]\n",
    "\n",
    "# Apply the mapping to create a new column with descriptive labels\n",
    "data['Algorithm'] = data['function_name'].map(function_mapping)\n",
    "\n",
    "# Extract unique functions and datasets for plotting\n",
    "functions = ['E-ESN', 'ESN', 'IP', 'Anti-Oja', 'IP +\\nAnti-Oja', 'mean HAG', 'variance HAG'] #data['Algorithm'].unique()\n",
    "datasets = data['dataset'].unique()\n",
    "\n",
    "# Prepare color palette for the bars\n",
    "colors = color_palette(\"tab20\", n_colors=len(functions))\n",
    "fontsize = 22\n",
    "\n",
    "# Metrics to plot\n",
    "metrics = ['spectral_radius_mean', 'pearson_mean', 'CEV_mean', 'scores_mean']\n",
    "error_metrics = ['spectral_radius_std', 'pearson_std', 'CEV_std', 'scores_std']\n",
    "\n",
    "# Iterate over each metric to create separate plots\n",
    "for metric_index, metric in enumerate(metrics):\n",
    "    plt.figure(figsize=(14, 8))  # New figure for each metric\n",
    "    ax = plt.subplot(111)\n",
    "    x = np.arange(len(datasets))  # The label locations\n",
    "    width = 0.1\n",
    "    \n",
    "    \n",
    "    for i, func in enumerate(functions):\n",
    "        subset = data[data['Algorithm'] == func]\n",
    "        means = subset[metric].values\n",
    "        errors = subset[error_metrics[metric_index]].values\n",
    "        ax.bar(\n",
    "            np.arange(len(datasets)) + i * width,\n",
    "            means,\n",
    "            width,\n",
    "            label=func,\n",
    "            yerr=errors,\n",
    "            capsize=5,\n",
    "            color=function_colors[func],\n",
    "            error_kw={'elinewidth': 2, 'capthick': 2}\n",
    "        )\n",
    "\n",
    "\n",
    "    # Set x-axis labels and ticks\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=fontsize)\n",
    "    \n",
    "    # Increase y-axis tick label size\n",
    "    ax.tick_params(axis='y', labelsize=fontsize)\n",
    "    \n",
    "    # Increase the number of y-axis ticks\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=10))  # Adjust 'nbins' as needed\n",
    "    \n",
    "    # Optionally, set y-axis limits based on your data\n",
    "    # ax.set_ylim(0, Z100)  # Uncomment and adjust if necessary\n",
    "    \n",
    "\n",
    "    ax.set_xticks(x + width * (len(functions)-1)/2)\n",
    "\n",
    "    ax.set_xticklabels(datasets, rotation=45, fontsize=fontsize)\n",
    "\n",
    "    # Set legend with descriptive labels\n",
    "    ax.legend(title='Algorithm', fontsize=fontsize, title_fontsize=fontsize)\n",
    "\n",
    "    # Improve aesthetics by removing top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Adjust layout for better fit\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a468b-42a9-4c8e-ae77-cc0ebfaba1ae",
   "metadata": {},
   "source": [
    "# Final matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ce7dd-4826-4a68-ab44-66ef90f031de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from performances.utility import camel_to_snake, retrieve_best_model\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.hag import run_algorithm\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# List of datasets\n",
    "classification = [\n",
    "    \"CatsDogs\",\n",
    "    \"JapaneseVowels\",\n",
    "#    \"SpokenArabicDigits\",\n",
    "    \"FSDD\",\n",
    "#    \"SPEECHCOMMANDS\",\n",
    "]\n",
    "\n",
    "prediction = [\n",
    "    \"MackeyGlass\",\n",
    "    \"Lorenz\",\n",
    "    \"Sunspot_daily\",\n",
    "]\n",
    "datasets=prediction\n",
    "\n",
    "\n",
    "# Initialize lists to store results and max values\n",
    "Ws = []\n",
    "titles = []\n",
    "max_values = []\n",
    "\n",
    "leaky_rate = 1\n",
    "input_connectivity = 1\n",
    "\n",
    "# Loop through datasets and function names to compute W matrices and find global vmax\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    pretrain_data, train_data, test_data, Y_train, Y_test, is_multivariate, is_instances_classification = load_data(dataset, data_type, noise_std)\n",
    "\n",
    "    for function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja\", \"ip-anti-oja\",  \"hadsp\", \"desp\"]:\n",
    "        print(function_name)\n",
    "        # Get the best trial from the study\n",
    "        study = retrieve_best_model(function_name, dataset, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "        hyperparams = {param_name: param_value for param_name, param_value in study.best_trial.params.items()}\n",
    "        print(hyperparams)\n",
    "        if function_name in [\"hadsp\", \"desp\"]:\n",
    "            MAX_TIME_INCREMENT = hyperparams['time_increment'] + hyperparams['time_increment_span'] #int(max_window_size) or None or TIME_INCREMENT\n",
    "    \n",
    "        if 'variance_target' not in hyperparams and 'min_variance' in hyperparams:\n",
    "            hyperparams['variance_target'] = hyperparams['min_variance']\n",
    "        if not is_instances_classification:\n",
    "            hyperparams['use_full_instance'] = False\n",
    "\n",
    "        common_index = 1\n",
    "        if is_instances_classification:\n",
    "            common_size = pretrain_data[0].shape[common_index]\n",
    "        else:\n",
    "            common_size = pretrain_data.shape[common_index]\n",
    "\n",
    "        # We want the size of the reservoir to be at least network_size\n",
    "        K = math.ceil(hyperparams[\"network_size\"] / common_size)\n",
    "        n = common_size * K\n",
    "        \n",
    "        # UNSUPERVISED PRETRAINING \n",
    "        if function_name == \"random_ee\":\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'], K, w_distribution=stats.uniform(loc=0, scale=1), seed=random.randint(0, 1000))\n",
    "        else:\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, hyperparams['connectivity'], K, w_distribution=stats.uniform(loc=-1, scale=2), seed=random.randint(0, 1000))\n",
    "            \n",
    "        bias *= hyperparams['bias_scaling']\n",
    "        Win *= hyperparams['input_scaling']\n",
    "\n",
    "        if function_name == \"hadsp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, hyperparams['time_increment'], hyperparams['weight_increment'],\n",
    "                                     hyperparams['target_rate'], hyperparams['rate_spread'], function_name, is_instance=is_instances_classification, use_full_instance=hyperparams['use_full_instance'],\n",
    "                                     max_increment=MAX_TIME_INCREMENT, max_partners=hyperparams['max_partners'], method=\"pearson\", n_jobs=nb_jobs)\n",
    "        elif function_name == \"desp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, hyperparams['time_increment'], hyperparams['weight_increment'],\n",
    "                                    hyperparams['variance_target'], hyperparams['variance_spread'], function_name, is_instance=is_instances_classification, \n",
    "                                    use_full_instance = hyperparams['use_full_instance'], max_increment=MAX_TIME_INCREMENT, max_partners=hyperparams['max_partners'], method = \"pearson\", \n",
    "                                    intrinsic_saturation=hyperparams['intrinsic_saturation'], intrinsic_coef=hyperparams['intrinsic_coef'], n_jobs = nb_jobs)\n",
    "        elif function_name in [\"random_ee\", \"random_ei\"]:\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= hyperparams['spectral_radius'] / max(abs(eigen))\n",
    "            \n",
    "        elif function_name in [\"ip_correct\", \"anti-oja\", \"ip-anti-oja\"]:\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= hyperparams['spectral_radius'] / max(abs(eigen))\n",
    "\n",
    "            # unsupervised local rules\n",
    "            if is_instances_classification:\n",
    "                unsupervised_pretrain = np.concatenate(pretrain_data).astype(float)\n",
    "            else:\n",
    "                unsupervised_pretrain = pretrain_data.astype(float)\n",
    "            if function_name == \"ip_correct\":\n",
    "                reservoir = init_ip_reservoir(W, Win, bias, mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                              leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function\n",
    "                                              )\n",
    "                _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "            elif function_name == \"anti-oja\":\n",
    "                reservoir = init_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                       synapse_normalization=True, bcm_theta=None,\n",
    "                                                       leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                       )\n",
    "                _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "            elif function_name == \"ip-anti-oja\":\n",
    "                reservoir = init_ip_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=hyperparams['oja_eta'],\n",
    "                                                          synapse_normalization=True, bcm_theta=None,\n",
    "                                                          mu=hyperparams['mu'], sigma=hyperparams['sigma'], learning_rate=hyperparams['learning_rate'],\n",
    "                                                          leaking_rate=hyperparams['leaky_rate'], activation_function=activation_function,\n",
    "                                                          )\n",
    "                _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "            else:\n",
    "                reservoir = init_reservoir(W, Win, bias, leaky_rate, activation_function)\n",
    "\n",
    "            W = reservoir.W\n",
    "            \n",
    "\n",
    "        # Store W matrix and corresponding title\n",
    "        Ws.append(W)\n",
    "        titles.append(f\"{dataset} - {[function_name]}\")\n",
    "        max_values.append(np.max(W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ccacef-d0d2-4633-9c30-a226789aa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "\n",
    "function_names = [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja\", \"ip-anti-oja\", \"hadsp\", \"desp\"]\n",
    "n_datasets = len(datasets)\n",
    "n_functions = len(function_names)\n",
    "\n",
    "# 1) Determine global min/max to center color scale around 0\n",
    "# Flatten all Ws into one array to find overall min & max\n",
    "all_values = []\n",
    "for W in Ws:\n",
    "    if hasattr(W, \"toarray\"):\n",
    "        W = W.toarray()\n",
    "    all_values.append(W.ravel())\n",
    "all_values = np.concatenate(all_values)\n",
    "\n",
    "global_abs_max = np.max(np.abs(all_values))  # largest absolute value\n",
    "g_vmin, g_vmax = -global_abs_max, global_abs_max\n",
    "\n",
    "# 2) Create a figure with one row per dataset and one column per function\n",
    "fig, axes = plt.subplots(\n",
    "    n_datasets, \n",
    "    n_functions, \n",
    "    figsize=(n_functions * 3, n_datasets * 3),\n",
    "    sharex=True, \n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "# Ensure axes is 2D even if n_datasets=1 or n_functions=1\n",
    "if n_datasets == 1:\n",
    "    axes = np.array([axes])\n",
    "if n_functions == 1:\n",
    "    axes = np.array([axes]).T\n",
    "\n",
    "idx = 0\n",
    "im = None  # to store the last image for the colorbar\n",
    "for i in range(n_datasets):\n",
    "    for j in range(n_functions):\n",
    "        W = Ws[idx]\n",
    "        idx += 1\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(W, \"toarray\"):\n",
    "            W = W.toarray()\n",
    "        \n",
    "        # 3) Plot base heatmap, using a diverging colormap like 'bwr'\n",
    "        #    and a symmetric vmin/vmax around 0\n",
    "        local_abs_max = np.max(np.abs(W))\n",
    "        vmin, vmax = -local_abs_max, local_abs_max\n",
    "        \n",
    "        im = axes[i, j].imshow(\n",
    "            W, \n",
    "            cmap='seismic', \n",
    "            interpolation='nearest', \n",
    "            vmin=vmin, \n",
    "            vmax=vmax\n",
    "        )\n",
    "        \n",
    "        # 4) Overlay zeros in white\n",
    "        zero_mask = (W == 0)\n",
    "        axes[i, j].imshow(\n",
    "            np.ma.masked_where(~zero_mask, W),\n",
    "            cmap=mcolors.ListedColormap(['white']),\n",
    "            interpolation='nearest'\n",
    "        )\n",
    "        \n",
    "        # Remove per-subplot x/y labels\n",
    "        axes[i, j].set_xlabel('')\n",
    "        axes[i, j].set_ylabel('')\n",
    "        \n",
    "        # Row label: dataset name on the left edge\n",
    "        if j == 0:\n",
    "            axes[i, j].text(\n",
    "                -0.3, 0.5, datasets[i],\n",
    "                rotation=90,\n",
    "                transform=axes[i, j].transAxes,\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=12\n",
    "            )\n",
    "        \n",
    "        # Column label: function name on top row\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(function_names[j], fontsize=12)\n",
    "            \n",
    "        # 5) Add a colorbar for each heatmap\n",
    "        #cbar = fig.colorbar(im, ax=axes[i, j], fraction=0.046, pad=0.04)\n",
    "        #cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "# 5) Add shared axis labels\n",
    "fig.text(0.5, 0.04, 'Neurons', ha='center', va='center', fontsize=14)\n",
    "fig.text(0.04, 0.5, 'Neurons', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "# Tight layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Single colorbar on the right\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), fraction=0.03, pad=0.04)\n",
    "cbar.set_label('Value', fontsize=12)\n",
    "plt.savefig(f'connectivity_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc41df3-9a4d-4103-9f37-747b1688f2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d505632-3b59-41d6-9aa5-18dd42c8a5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hag_env",
   "language": "python",
   "name": "hag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
