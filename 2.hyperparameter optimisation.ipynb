{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T21:28:16.832787Z",
     "start_time": "2025-03-24T21:28:16.817819Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "SEED = 923984"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets loading\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Review: \n",
    "https://arxiv.org/abs/2012.02974\n",
    "\n",
    "Regression : http://tseregression.org/ + https://arxiv.org/pdf/2012.02974"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6272-708d-4709-8e35-5a84268bed64",
   "metadata": {},
   "source": [
    "Forecasting datasets available :\n",
    "\n",
    "* MackeyGlass\n",
    "* Lorenz\n",
    "* Sunspot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b2d29d-7fff-4032-8b36-559c722818b8",
   "metadata": {},
   "source": [
    "Classification Datasets available :\n",
    "\n",
    "* Custom :  FSDD, HAART, JapaneseVowels\n",
    "* Aeon : SpokenArabicDigits, CatsDogs, LSST\n",
    "* Torchaudio: SPEECHCOMMANDS\n",
    "\n",
    "More on https://www.timeseriesclassification.com/dataset.php or https://pytorch.org/audio/stable/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb188a0-a9a2-4b94-af1e-5b50dd6b1cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T21:29:11.112767Z",
     "start_time": "2025-03-24T21:29:07.809433Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.load_data import load_data as load_dataset\n",
    "step_ahead=5\n",
    "dataset_name = \"FSDD\"\n",
    "\n",
    "(is_instances_classification, is_multivariate, sampling_rate,\n",
    "     X_train_raw, X_test_raw, Y_train_raw, Y_test,\n",
    "     use_spectral_representation, spectral_representation,\n",
    "     groups) = load_dataset(dataset_name, step_ahead, visualize=False)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "ax.plot(range(len(X_train_raw[0])), X_train_raw[0])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fontsize=30\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Time', size=fontsize)\n",
    "plt.ylabel('Value', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a38d2df41a727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T21:29:11.180679Z",
     "start_time": "2025-03-24T21:29:11.178737Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh\n",
    "activation_function = lambda x : tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f892fb1cccd7511",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca76d0e06f936c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math \n",
    " \n",
    "# Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, StratifiedGroupKFold\n",
    "from datasets.preprocessing import flexible_indexing\n",
    "\n",
    "#Preprocessing\n",
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data, add_noise\n",
    "\n",
    "# Define noise parameter\n",
    "noise_std = 0.001\n",
    "\n",
    "nb_splits=3\n",
    "if is_instances_classification:\n",
    "    if groups is None:\n",
    "        splits = StratifiedKFold(n_splits=nb_splits, shuffle=True, random_state=SEED).split(X_train_raw, np.argmax(Y_train_raw, axis=1))\n",
    "    else:\n",
    "        splits = StratifiedGroupKFold(n_splits=nb_splits, shuffle=True, random_state=SEED).split(X_train_raw, np.argmax(Y_train_raw, axis=1), groups)\n",
    "else: #prediction\n",
    "    splits = TimeSeriesSplit(n_splits=nb_splits).split(X_train_raw)\n",
    "\n",
    "data_type = \"normal\" # \"normal\" ou \"noisy\"\n",
    "\n",
    "X_pretrain = []\n",
    "X_pretrain_noisy  = []\n",
    "X_train = []\n",
    "X_train_noisy = []\n",
    "X_val = []\n",
    "X_val_noisy = []\n",
    "X_pretrain_band = []\n",
    "X_pretrain_band_noisy = []\n",
    "X_train_band = []\n",
    "X_train_band_noisy = []\n",
    "X_val_band = []\n",
    "X_val_band_noisy = []\n",
    "\n",
    "Y_train = []\n",
    "Y_val = []\n",
    "\n",
    "WINDOW_LENGTH = 10\n",
    "print(\"Not smoothed frequencies\")\n",
    "freq_train_data = X_train_raw\n",
    "flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "extract_peak_frequencies(flat_train_data, sampling_rate, smooth=False, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "\n",
    "print(\"Preprocessing\")\n",
    "for i, (train_index, val_index) in enumerate(splits):\n",
    "    x_train = flexible_indexing(X_train_raw, train_index)\n",
    "    x_val = flexible_indexing(X_train_raw, val_index)\n",
    "    Y_train.append(flexible_indexing(Y_train_raw, train_index))\n",
    "    Y_val.append(flexible_indexing(Y_train_raw, val_index))\n",
    "\n",
    "\n",
    "    if is_multivariate:\n",
    "        x_train_band, x_val_band = x_train, x_val\n",
    "        del x_train, x_val\n",
    "        \n",
    "    # PREPROCESSING\n",
    "    freq_train_data = x_train_band if is_multivariate else x_train\n",
    "    flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "    peak_freqs = extract_peak_frequencies(flat_train_data, sampling_rate, smooth=True, window_length=WINDOW_LENGTH, threshold=1e-5, nperseg=1024, visualize=False)\n",
    "\n",
    "    # if it has use_spectral_representation, then it is multivariate\n",
    "    if use_spectral_representation == True: \n",
    "        if is_multivariate==False:\n",
    "            raise ValueError(\"Cannot use spectral representation if it's not multivariate !\")\n",
    "\n",
    "    if not is_multivariate:\n",
    "        x_train_band = generate_multivariate_dataset(\n",
    "            x_train, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "        x_val_band = generate_multivariate_dataset(\n",
    "            x_val, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "    elif is_multivariate and not use_spectral_representation:\n",
    "        x_train_band = generate_multivariate_dataset(\n",
    "            x_train_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "        x_val_band = generate_multivariate_dataset(\n",
    "            x_val_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "    else:\n",
    "        print(\"Data is already spectral, nothing to do\")\n",
    "\n",
    "    if not is_multivariate:\n",
    "        scaler_x_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "        x_train, x_val, _ = scale_data(x_train, x_val, None, scaler_x_uni, is_instances_classification)       \n",
    "        X_train.append(x_train)\n",
    "        X_val.append(x_val)\n",
    "\n",
    "    scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_train_band, x_val_band, _ = scale_data(x_train_band, x_val_band, None, scaler_multi, is_instances_classification)\n",
    "    X_train_band.append(x_train_band)\n",
    "    X_val_band.append(x_val_band)\n",
    "             \n",
    "    # NOISE\n",
    "    if data_type == \"noisy\":\n",
    "        if is_instances_classification:\n",
    "            # UNI\n",
    "            if not is_multivariate:\n",
    "                x_train_noisy=[add_noise(instance, noise_std) for instance in x_train]\n",
    "                X_train_noisy.append([add_noise(instance, noise_std) for instance in x_train])\n",
    "                X_val_noisy.append([add_noise(instance, noise_std) for instance in x_val])\n",
    "                \n",
    "            # MULTI\n",
    "            x_train_band_noisy=[add_noise(instance, noise_std) for instance in x_train_band]\n",
    "            X_train_band_noisy.append(x_train_band_noisy)\n",
    "            X_val_band_noisy.append([add_noise(instance, noise_std) for instance in x_val_band])\n",
    "        \n",
    "        else:  #if prediction\n",
    "            # UNI\n",
    "            if not is_multivariate:\n",
    "                x_train_noisy=add_noise(x_train, noise_std)\n",
    "                X_train_noisy.append(x_train_noisy)\n",
    "                X_val_noisy.append(add_noise(x_val, noise_std))\n",
    "        \n",
    "            # MULTI\n",
    "            x_train_band_noisy=add_noise(x_train_band, noise_std)\n",
    "            X_train_band_noisy.append(x_train_band_noisy)\n",
    "            X_val_band_noisy.append(add_noise(x_val_band, noise_std))\n",
    "\n",
    "    # Define the number of instances you want to select\n",
    "    x_size = len(x_train_band) if is_multivariate else len(x_train)\n",
    "    num_samples_for_pretrain = 500 if x_size >= 500 else x_size\n",
    "    if is_instances_classification:\n",
    "        indices = np.random.choice(x_size, num_samples_for_pretrain, replace=False)\n",
    "    else:\n",
    "        indices = range(x_size)\n",
    "    \n",
    "    if data_type == \"noisy\":\n",
    "        # Defining pretrain   \n",
    "        if not is_multivariate:\n",
    "            X_pretrain_noisy.append(np.array(x_train_noisy, dtype=object)[indices].flatten())\n",
    "        X_pretrain_band_noisy.append(np.array(x_train_band_noisy, dtype=object)[indices])\n",
    "\n",
    "    # Defining pretrain   \n",
    "    if not is_multivariate:\n",
    "        X_pretrain.append(np.array(x_train, dtype=object)[indices].flatten())\n",
    "    X_pretrain_band.append(np.array(x_train_band, dtype=object)[indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c5610-8dad-4d05-8bbb-fa34031d9c9c",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786a3c5-4928-4952-8bdb-69067753e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "\n",
    "def find_optimal_order(dim, delay, s, max_network_size, max_order_size=5):\n",
    "    \"\"\"\n",
    "    Finds the optimal order N and the total number of variables such that\n",
    "    total_variables is less than or equal to max_network_size.\n",
    "    \"\"\"\n",
    "    delay_eff = (delay - 1) // (s + 1)\n",
    "    N = 0\n",
    "    total_variables = 0\n",
    "\n",
    "    while True:\n",
    "        N += 1\n",
    "        # Ensure N does not exceed max_order_size\n",
    "        if N > max_order_size:\n",
    "            N -= 1\n",
    "            break\n",
    "\n",
    "        # Calculate the new combination incrementally\n",
    "        new_comb = comb(dim * delay_eff + N - 1, N, exact=True)\n",
    "        total_variables += new_comb\n",
    "\n",
    "        # Check if the total exceeds the maximum network size\n",
    "        if total_variables > max_network_size:\n",
    "            # Subtract the last addition and decrement N\n",
    "            total_variables -= new_comb\n",
    "            N -= 1\n",
    "            break\n",
    "\n",
    "    return N, total_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808838b6a03ba5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretraining\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.hag import run_algorithm\n",
    "from scipy import sparse, stats\n",
    "from numpy import random\n",
    "\n",
    "# Evaluating\n",
    "from performances.esn_model_evaluation import train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from performances.esn_model_evaluation import train_model_for_prediction, init_reservoir, init_ip_reservoir, init_local_rule_reservoir, init_ip_local_rule_reservoir, init_readout\n",
    "\n",
    "# score for prediction\n",
    "start_step = 30\n",
    "end_step = 500\n",
    "SLICE_RANGE = slice(start_step, end_step)\n",
    "RESERVOIR_SIZE = 500\n",
    "\n",
    "nb_jobs_per_trial = 1\n",
    "function_name = \"anti_oja_fast\" # \"desp\" ou \"hadsp\", \"random_ee\", \"random_ei\", \"ip\", \"ip_correct\" or \"anti_oja_fast\"\n",
    "variate_type = \"multi\" # \"multi\" ou \"uni\"\n",
    "if variate_type == \"uni\" and is_multivariate:\n",
    "    raise ValueError(f\"Invalid variable type: {variate_type}\")\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for the parameters you want to optimize\n",
    "    # COMMON\n",
    "    ridge = trial.suggest_int('ridge', -13, 1)\n",
    "    RIDGE_COEF = 10 ** ridge\n",
    "\n",
    "    network_size = trial.suggest_int('network_size', RESERVOIR_SIZE, RESERVOIR_SIZE)\n",
    "    input_scaling = trial.suggest_float('input_scaling', 0.01, 0.2, step=0.005)\n",
    "    bias_scaling = trial.suggest_float('bias_scaling', 0, 0.2, step=0.005)\n",
    "    leaky_rate = trial.suggest_float('leaky_rate', 1, 1)\n",
    "    input_connectivity = trial.suggest_float('input_connectivity', 1, 1)\n",
    "\n",
    "    min_window_size = sampling_rate / np.max(np.hstack(peak_freqs))\n",
    "    max_window_size = sampling_rate / np.min(np.hstack(peak_freqs))\n",
    "\n",
    "    # HADSP\n",
    "    if function_name == \"hadsp\":\n",
    "        target_rate = trial.suggest_float('target_rate', 0.5, 1, step=0.01)\n",
    "        rate_spread = trial.suggest_float('rate_spread', 0.01, 0.4, step=0.005)\n",
    "        method = trial.suggest_categorical(\"method\", [\"random\", \"pearson\"])\n",
    "    # DESP\n",
    "    elif function_name == \"desp\":\n",
    "        variance_target = trial.suggest_float('variance_target', 0.001, 0.02, step=0.001)\n",
    "        variance_spread = trial.suggest_float('variance_spread', 0.001, 0.05, step=0.002)\n",
    "        intrinsic_saturation = trial.suggest_float('intrinsic_saturation', 0.8, 0.98, step=0.02)\n",
    "        intrinsic_coef = trial.suggest_float('intrinsic_coef', 0.8, 0.98, step=0.02)\n",
    "        method = trial.suggest_categorical(\"method\", [\"pearson\"])\n",
    "    elif function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja\", \"anti-oja_fast\", \"ip-anti-oja\", \"ip-anti-oja_fast\"]:\n",
    "        connectivity = trial.suggest_float('connectivity', 0, 1)\n",
    "        sr = trial.suggest_float('spectral_radius', 0.4, 1.6, step=0.01)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid function name: {function_name}\")\n",
    "\n",
    "    if function_name in [\"ip_correct\", \"ip-anti-oja\", \"ip-anti-oja_fast\"]:\n",
    "        mu = trial.suggest_float('mu', 0, 1)\n",
    "        sigma = trial.suggest_float('sigma', 0, 1)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "    if function_name in [\"anti-oja\", \"anti-oja_fast\", \"ip-anti-oja\", \"ip-anti-oja_fast\"]:\n",
    "        # We often use a log-uniform distribution for learning rates:\n",
    "        oja_eta = trial.suggest_float('oja_eta', 1e-8, 1e-3, log=True)\n",
    "\n",
    "    if function_name in [\"hadsp\", \"desp\"]:\n",
    "        connectivity = trial.suggest_float('connectivity', 0, 0)\n",
    "        weight_increment = trial.suggest_float('weight_increment', 0.001, 0.1, step=0.001)\n",
    "        max_partners = trial.suggest_int('max_partners', 10, 20)\n",
    "        if is_instances_classification:\n",
    "            use_full_instance = trial.suggest_categorical('use_full_instance', [True, False])\n",
    "        else:\n",
    "            use_full_instance = False\n",
    "        TIME_INCREMENT = trial.suggest_int('time_increment', int(min_window_size + 1),\n",
    "                                           100)  # int(min_window_size+1) or int(max_window_size)\n",
    "        max_increment_span = int(max_window_size) if int(max_window_size) - 100 < 0 else int(max_window_size) - 100\n",
    "        time_increment_span = trial.suggest_int('time_increment_span', 0, max_increment_span)\n",
    "        MAX_TIME_INCREMENT = TIME_INCREMENT + time_increment_span  # int(max_window_size) or None or TIME_INCREMENT\n",
    "\n",
    "    # CROSS-VALIDATION METHODS\n",
    "    total_score = 0\n",
    "    for i in range(nb_splits):\n",
    "        common_index = 1\n",
    "        if is_instances_classification:\n",
    "            common_size = X_train_band[i][0].shape[common_index]\n",
    "        else:\n",
    "            common_size = X_train_band[i].shape[common_index]\n",
    "\n",
    "        # We want the size of the reservoir to be at least network_size\n",
    "        K = math.ceil(network_size / common_size)\n",
    "        n = common_size * K\n",
    "\n",
    "        pretrain_data = X_pretrain_band[i]\n",
    "        train_data = X_train_band[i]  # X_train_band_noisy_duplicated or X_train_band_duplicated\n",
    "        val_data = X_val_band_noisy[i] if data_type == \"noisy\" else X_val_band[i]\n",
    "\n",
    "        # INITIALISATION AND UNSUPERVISED PRETRAINING\n",
    "        if function_name == \"random_ee\":\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, connectivity, K, w_distribution=stats.uniform(loc=0, scale=1), seed=random.randint(0, 1000))\n",
    "        else:\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, connectivity, K, w_distribution=stats.uniform(loc=-1, scale=2), seed=random.randint(0, 1000))\n",
    "        bias *= bias_scaling\n",
    "        Win *= input_scaling\n",
    "\n",
    "        if function_name == \"hadsp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, leaky_rate, activation_function, pretrain_data, TIME_INCREMENT,\n",
    "                                         weight_increment,\n",
    "                                         target_rate, rate_spread, function_name,\n",
    "                                         is_instance=is_instances_classification, use_full_instance=use_full_instance,\n",
    "                                         max_increment=MAX_TIME_INCREMENT, max_partners=max_partners, method=method,\n",
    "                                         n_jobs=nb_jobs_per_trial)\n",
    "        elif function_name == \"desp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, leaky_rate, activation_function, pretrain_data, TIME_INCREMENT,\n",
    "                                         weight_increment,\n",
    "                                         variance_target, variance_spread, function_name,\n",
    "                                         is_instance=is_instances_classification, use_full_instance=use_full_instance,\n",
    "                                         max_increment=MAX_TIME_INCREMENT, max_partners=max_partners, method=method,\n",
    "                                         intrinsic_saturation=intrinsic_saturation, intrinsic_coef=intrinsic_coef,\n",
    "                                         n_jobs=nb_jobs_per_trial)\n",
    "        elif function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja\", \"ip-anti-oja\", \"anti-oja_fast\", \"ip-anti-oja_fast\"]:\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= sr / max(abs(eigen))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid function: {function_name}\")\n",
    "\n",
    "        # unsupervised local rules\n",
    "        if is_instances_classification:\n",
    "            unsupervised_pretrain = np.concatenate(pretrain_data).astype(float)\n",
    "        else:\n",
    "            unsupervised_pretrain = pretrain_data.astype(float)\n",
    "        if function_name == \"ip_correct\":\n",
    "            reservoir = init_ip_reservoir(W, Win, bias, mu=mu, sigma=sigma, learning_rate=learning_rate,\n",
    "                                          leaking_rate=leaky_rate, activation_function=activation_function\n",
    "                                          )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"anti_oja\":\n",
    "            reservoir = init_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=oja_eta,\n",
    "                                                  synapse_normalization=True, bcm_theta=None,\n",
    "                                                  leaking_rate=leaky_rate, activation_function=activation_function,\n",
    "                                                  )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"anti-oja_fast\":\n",
    "            reservoir = init_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=oja_eta,\n",
    "                                                  synapse_normalization=False, bcm_theta=None,\n",
    "                                                  leaking_rate=leaky_rate, activation_function=activation_function,\n",
    "                                                  )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"ip-anti-oja\":\n",
    "            reservoir = init_ip_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=oja_eta,\n",
    "                                                     synapse_normalization=True, bcm_theta=None,\n",
    "                                                     mu=mu, sigma=sigma, learning_rate=learning_rate,\n",
    "                                                     leaking_rate=leaky_rate, activation_function=activation_function,\n",
    "                                                     )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        elif function_name == \"ip-anti-oja_fast\":\n",
    "            reservoir = init_ip_local_rule_reservoir(W, Win, bias, local_rule=\"anti-oja\", eta=oja_eta,\n",
    "                                                     synapse_normalization=False, bcm_theta=None,\n",
    "                                                     mu=mu, sigma=sigma, learning_rate=learning_rate,\n",
    "                                                     leaking_rate=leaky_rate, activation_function=activation_function,\n",
    "                                                     )\n",
    "            _ = reservoir.fit(unsupervised_pretrain, warmup=100)\n",
    "        else:\n",
    "            reservoir = init_reservoir(W, Win, bias, leaky_rate, activation_function)\n",
    "        readout = init_readout(ridge_coef=RIDGE_COEF)\n",
    "\n",
    "        # TRAINING and EVALUATION\n",
    "        if is_instances_classification:\n",
    "            mode = \"sequence-to-vector\"\n",
    "            train_model_for_classification(reservoir, readout, train_data, Y_train[i], n_jobs=nb_jobs_per_trial, mode=mode)\n",
    "\n",
    "            Y_pred = predict_model_for_classification(reservoir, readout, val_data, n_jobs=nb_jobs_per_trial, mode=mode)\n",
    "            score = compute_score(Y_pred, Y_val[i], is_instances_classification)\n",
    "        else:\n",
    "            esn = train_model_for_prediction(reservoir, readout, train_data, Y_train[i])\n",
    "\n",
    "            Y_pred = esn.run(val_data, reset=False)\n",
    "            score = compute_score(Y_pred, Y_val[i], is_instances_classification)\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    average_score = total_score / nb_splits  # Average the score\n",
    "\n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b55dc47e084ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from performances.utility import camel_to_snake\n",
    "\n",
    "url= \"sqlite:///tpe_\" + camel_to_snake(dataset_name) + \"_db.sqlite3\"\n",
    "print(url)\n",
    "\n",
    "study_name = function_name + \"_\" + dataset_name + \"_\" + data_type + \"_\" + variate_type\n",
    "print(study_name)\n",
    "\n",
    "direction = \"maximize\" if is_instances_classification else \"minimize\"\n",
    "\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=url,\n",
    "    engine_kwargs={\"pool_size\": 20, \"connect_args\": {\"timeout\": 10}},\n",
    ")\n",
    "sampler = TPESampler()\n",
    "\n",
    "def optimize_study(n_trials):\n",
    "    study = optuna.create_study(storage=storage, sampler=sampler, study_name=study_name, direction=direction, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "N_TRIALS = 150\n",
    "n_parallel_studies = 10\n",
    "trials_per_process = N_TRIALS // n_parallel_studies\n",
    "\n",
    "# Use joblib to parallelize the optimization\n",
    "Parallel(n_jobs=n_parallel_studies)(\n",
    "    delayed(optimize_study)(trials_per_process) for _ in range(n_parallel_studies)\n",
    ")\n",
    "\n",
    "#study = optuna.create_study(storage=storage, sampler=sampler, study_name=study_name, direction=direction, load_if_exists=True)\n",
    "#study.optimize(objective, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97544f28-0c1a-431a-aa3c-fada8bd1f6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5da38d03c3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebcb014a0bbec3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T22:43:44.927013Z",
     "start_time": "2025-03-24T22:43:43.868034Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from performances.utility import retrieve_best_model\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "variate_type = \"multi\"  # \"multi\" or \"uni\"\n",
    "\n",
    "for dataset_name in [\"CatsDogs\", \"JapaneseVowels\", \"SpokenArabicDigits\", \"FSDD\", \"SPEECHCOMMANDS\", \"MackeyGlass\", \"Lorenz\", \"Sunspot_daily\"]:\n",
    "    for function_name in [\"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\"]: # \"random_ee\", \"random_ei\", \"ip_correct\", \"anti-oja_fast\",  \"ip-anti-oja_fast\", \"hadsp\", \"desp\"\n",
    "        is_multivariate = True\n",
    "        study = retrieve_best_model(function_name, dataset_name, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "        # get the best trial score and print it\n",
    "        best_trial = study.best_trial\n",
    "        best_score = best_trial.value\n",
    "        print(f\"{dataset_name} {function_name}: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b272afbda899e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hag_env",
   "language": "python",
   "name": "hag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
