{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1d64",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T10:40:49.539850Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e162795-bf9d-4bcf-8c16-7411242419fa",
   "metadata": {},
   "source": [
    "# Idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b54979-e907-4333-b442-f5878a053250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"outputs/images\", exist_ok=True)\n",
    "\n",
    "# Configure PDF backend\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "# Data definitions\n",
    "x = np.linspace(0, 3, 400)\n",
    "delta_w = 0.2\n",
    "t = np.linspace(0, 10, 500)\n",
    "u = 1.0 + 0.5 * np.sin(2 * np.pi * 0.3 * t)\n",
    "t_event = 5.0\n",
    "\n",
    "# Activations\n",
    "y_tanh = np.tanh(x)\n",
    "y_lin = x\n",
    "before_span = (0.7, 1.0)\n",
    "after_span = (before_span[0] + delta_w, before_span[1] + delta_w)\n",
    "\n",
    "y_A_time = np.tanh(u)\n",
    "u_B = u.copy()\n",
    "u_B[t >= t_event] += delta_w\n",
    "y_B_time = np.tanh(u_B)\n",
    "\n",
    "# Colors\n",
    "orange = \"#E66100\"\n",
    "yellow = \"#FFC100\"\n",
    "gray = \"#888888\"\n",
    "blue = \"#3251a8\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4), dpi=150)\n",
    "\n",
    "# Panel (c): Tanh effect illustration with curved arrow\n",
    "ax = axes[0]\n",
    "ax.plot(x, y_tanh, color=gray, label='tanh(x)')\n",
    "ax.plot(x, y_lin, color=blue, label='linear')\n",
    "\n",
    "# Plot before and after segments on tanh curve\n",
    "x_before = np.linspace(before_span[0], before_span[1], 100)\n",
    "x_after = np.linspace(after_span[0], after_span[1], 100)\n",
    "ax.plot(x_before, np.tanh(x_before), color=orange, lw=3, label='Before shift')\n",
    "ax.plot(x_after, np.tanh(x_after), color=yellow, lw=3, label='After shift')\n",
    "\n",
    "# Add curved arrow to indicate shift Î”w\n",
    "start_x, end_x = (before_span[1] + before_span[0]) / 2, (after_span[1] + after_span[0]) / 2\n",
    "start_y, end_y = np.tanh(start_x), np.tanh(end_x)\n",
    "ax.annotate('', xy=(end_x, end_y), xytext=(start_x, start_y),\n",
    "            arrowprops=dict(arrowstyle='->', color=gray, ls='--', lw=2,\n",
    "                            connectionstyle='arc3,rad=0.3'))\n",
    "ax.text((start_x+end_x)/2, max(start_y, end_y)+0.05, r'$\\Delta w$ shift',\n",
    "        color=gray, fontsize='small', ha='center')\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Input')\n",
    "ax.set_ylabel('Activation')\n",
    "ax.set_xlim(0.25, 2)\n",
    "ax.set_ylim(-0.05, 1.1)\n",
    "ax.grid(color='lightgray', linestyle='--', linewidth=0.5)\n",
    "ax.legend(frameon=False, fontsize='small', loc='lower right')\n",
    "ax.text(0.02, 0.95, '(c)', transform=ax.transAxes, weight='bold')\n",
    "\n",
    "# Panel (d): Time evolution illustration\n",
    "ax = axes[1]\n",
    "pre = t < t_event\n",
    "post = ~pre\n",
    "dash = 5\n",
    "n_pre = pre.sum()\n",
    "\n",
    "# Plot A before event in dashed alternating pattern\n",
    "for i in range(0, n_pre-dash, dash):\n",
    "    idx = np.arange(i, i+dash)\n",
    "    ax.plot(t[pre][idx], y_A_time[pre][idx],\n",
    "            color=orange if (i//dash) % 2 == 0 else yellow, lw=2)\n",
    "# Plot after event\n",
    "ax.plot(t[post], y_A_time[post], color=orange, lw=2, label='Neuron A')\n",
    "ax.plot(t[post], y_B_time[post], color=yellow, lw=2, label='Neuron B')\n",
    "ax.axvline(t_event, ls='--', color=gray, lw=1.5, label='Event')\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(color='lightgray', linestyle='--', linewidth=0.5)\n",
    "ax.legend(frameon=False, fontsize='small', loc='lower right')\n",
    "ax.text(0.02, 0.95, '(d)', transform=ax.transAxes, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "path = \"outputs/images/hag_effect.pdf\"\n",
    "fig.savefig(path, bbox_inches='tight')\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets loading\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Review: \n",
    "https://arxiv.org/abs/2012.02974\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00291787-4082-48ea-9002-54d777cac5b6",
   "metadata": {},
   "source": [
    "Forecasting datasets available :\n",
    "\n",
    "* ReservoirPy: MackeyGlass, Lorenz\n",
    "* Custom: Sunspot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741331a-93ba-4132-8449-0e9521c4ddcc",
   "metadata": {},
   "source": [
    "Classification Datasets available :\n",
    "\n",
    "* Custom :  FSDD, HAART, JapaneseVowels\n",
    "* Aeon : SpokenArabicDigits, CatsDogs, LSST\n",
    "* Torchaudio: SPEECHCOMMANDS\n",
    "\n",
    "More on https://www.timeseriesclassification.com/dataset.php or https://pytorch.org/audio/stable/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032bec81-1517-4dae-9a6f-9a49ea4b2d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T21:31:33.123019Z",
     "start_time": "2025-03-24T21:31:32.563557Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.load_data import load_data as load_dataset\n",
    "from datasets.preprocessing import plot_classes_distribution\n",
    "\n",
    "step_ahead=5\n",
    "dataset_name = \"FSDD\"\n",
    "\n",
    "(is_instances_classification, is_multivariate, sampling_rate,\n",
    "     X_train_raw, X_test_raw, Y_train_raw, Y_test,\n",
    "     use_spectral_representation, spectral_representation,\n",
    "     groups) = load_dataset(dataset_name, step_ahead, visualize=False)\n",
    "\n",
    "# Plot data distribution\n",
    "if is_instances_classification:\n",
    "    plot_classes_distribution(Y_train_raw, Y_test, val = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55bde5-5bb3-41ea-ae0d-10d7bc7401f4",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90237f4e-2418-48a9-a067-aece8c922912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, StratifiedGroupKFold\n",
    "from datasets.preprocessing import flexible_indexing, plot_classes_distribution\n",
    "\n",
    "# CROSS-VALIDATION METHODS\n",
    "# SEED\n",
    "SEED = 49387\n",
    "\n",
    "use_cross_validation = False\n",
    "\n",
    "n_splits=2\n",
    "\n",
    "if use_cross_validation: # we split train between train and val\n",
    "    if is_instances_classification:\n",
    "        if groups is None:\n",
    "            splits = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(X_train_raw, np.argmax(Y_train_raw, axis=1))\n",
    "        else:\n",
    "            splits = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(X_train_raw, np.argmax(Y_train_raw, axis=1), groups)\n",
    "    else: #prediction\n",
    "        splits = TimeSeriesSplit(n_splits=n_splits).split(X_train_raw)\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(splits):\n",
    "        X_train = flexible_indexing(X_train_raw, train_index)\n",
    "        X_val = flexible_indexing(X_train_raw, val_index)\n",
    "        Y_train = flexible_indexing(Y_train_raw, train_index)\n",
    "        Y_val = flexible_indexing(Y_train_raw, val_index)\n",
    "        # SPLITS\n",
    "        if is_multivariate:\n",
    "            X_train_band, X_val_band, X_test_band = X_train, X_val, X_test_raw\n",
    "            del X_train, X_val\n",
    "        else:\n",
    "            X_test = X_test_raw\n",
    "\n",
    "    if is_instances_classification:\n",
    "        plot_classes_distribution(Y_train, Y_val, val = True)\n",
    "else: # then we use the test dataset\n",
    "    if is_multivariate:\n",
    "        X_train_band, X_test_band = X_train_raw, X_test_raw\n",
    "        X_val_band = None\n",
    "    else:\n",
    "        X_test, X_train = X_test_raw, X_train_raw\n",
    "        X_val, X_val_band = None, None\n",
    "    Y_train = Y_train_raw\n",
    "#del Y_train_raw, X_train_raw, X_test_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468ca5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766304bc-3c47-4813-9828-91d8fed5845b",
   "metadata": {},
   "source": [
    "## Multivariate generation (if not multivariate) and train_validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8ea82-6c39-45d7-a3b9-04b11f7bfea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.multivariate_generation import generate_multivariate_dataset\n",
    "\n",
    "# if it has use_spectral_representation, then it is multivariate\n",
    "if use_spectral_representation == True: \n",
    "    if is_multivariate==False:\n",
    "        raise ValueError(\"Cannot use spectral representation if it's not multivariate !\")\n",
    "\n",
    "#  hop used to be  hop=100 and win_length = 50, which was wrong\n",
    "hop = 50 if is_instances_classification else 1\n",
    "win_length = edge_cut = 100\n",
    "if not is_multivariate:\n",
    "    print(f\"Converting single variate to {spectral_representation}\")\n",
    "    X_train_band = generate_multivariate_dataset(\n",
    "        X_train, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "    )\n",
    "    if use_cross_validation:\n",
    "        X_val_band = generate_multivariate_dataset(\n",
    "            X_val, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "        )\n",
    "    X_test_band = generate_multivariate_dataset(\n",
    "            X_test, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "    )\n",
    "        \n",
    "elif is_multivariate and not use_spectral_representation:\n",
    "    print(f\"Converting multivariate to {spectral_representation}\") # if None we convert to temporal\n",
    "    X_train_band = generate_multivariate_dataset(\n",
    "        X_train_band, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "    )\n",
    "    if use_cross_validation:\n",
    "        X_val_band = generate_multivariate_dataset(\n",
    "            X_val_band, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "        )\n",
    "    X_test_band = generate_multivariate_dataset(\n",
    "        X_test_band, is_instances_classification, spectral_representation, hop=hop, win_length = win_length\n",
    "    )\n",
    "else:\n",
    "    print(\"Data is already spectral, nothing to do\")\n",
    "\n",
    "if not is_instances_classification:\n",
    "    X_train_band = X_train_band[edge_cut:-edge_cut] \n",
    "    X_test_band = X_test_band[edge_cut:-edge_cut] \n",
    "    \n",
    "    Y_train = Y_train[edge_cut:-edge_cut] \n",
    "    Y_test  = Y_test[edge_cut:-edge_cut]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b859e5-c6dd-4086-ac56-42a7de3ed25e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Standardizing the amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45e001-4920-47e1-8dbb-6681cf9b32e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.856649Z",
     "start_time": "2023-10-20T11:11:39.230512Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data\n",
    "\n",
    "X_train_band_not_scale = X_train_band\n",
    "scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_band, X_val_band, X_test_band = scale_data(X_train_band, X_val_band, X_test_band, scaler_multi, is_instances_classification)\n",
    "            \n",
    "if not is_multivariate:\n",
    "    X_train_not_scale = X_train\n",
    "    scaler_x_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train, X_val, X_test = scale_data(X_train, X_val, X_test, scaler_multi, is_instances_classification)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ddcde6-215a-464a-8634-624f11da4380",
   "metadata": {},
   "source": [
    "## Visualisation checks\n",
    "\n",
    "test if the filtering happened correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d88cf-a9a8-4e17-bffb-262aaffcf605",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cross_validation: \n",
    "    # Concatenate train and test arrays for plotting\n",
    "    combined_data = np.concatenate((X_train_band, X_val_band), axis=0)\n",
    "    \n",
    "    # Calculate the merge point index\n",
    "    merge_point_index = X_train_band.shape[0]\n",
    "    \n",
    "    # Define the range around the merge point to plot\n",
    "    start_index = merge_point_index - 1000\n",
    "    end_index = merge_point_index + 2000\n",
    "    \n",
    "    # Plot for a subset N features within a range arround transition from train to test\n",
    "    N = 3\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i in [1, 2, 17]: \n",
    "        plt.plot(range(start_index, end_index), combined_data[start_index:end_index, i], label=f'Feature {i}')\n",
    "    plt.title('Feature Values Around Merge Point')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87795848-f154-419a-b7d4-35400767f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import color_palette\n",
    "import librosa.display\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "# Define different color palettes for each step\n",
    "colors_step1 = color_palette(\"tab20\")    # For the first plot\n",
    "colors_step2 = color_palette(\"Set2\")     # For the second plot\n",
    "colors_step3 = color_palette(\"Dark2\")     # For the third plot\n",
    "fontsize = 14\n",
    "\n",
    "# Create a figure with three subplots side by side\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\n",
    "\n",
    "# First Plot: Original Time Series\n",
    "if is_instances_classification:\n",
    "    train_data = np.concatenate(X_train, axis=0).T\n",
    "else:\n",
    "    train_data = X_train_raw.T\n",
    "\n",
    "START = 0\n",
    "END = 500\n",
    "DIFF = END - START\n",
    "\n",
    "ax = axes[0]\n",
    "for idx, i in enumerate(range(train_data.shape[0])):\n",
    "    ax.plot(range(DIFF), train_data[i, START:END], color=colors_step1[idx % len(colors_step1)])\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "ax.set_xlabel('Time', fontsize=fontsize)\n",
    "#ax.set_ylabel('Value', fontsize=fontsize)\n",
    "ax.set_title('Original Time Series Data', fontsize=fontsize)\n",
    "\n",
    "# Second Plot: Transformed Data\n",
    "if is_instances_classification:\n",
    "    train_band_data = np.concatenate(X_train_band_not_scale, axis=0).T\n",
    "else:\n",
    "    train_band_data = X_train_band_not_scale.T\n",
    "    \n",
    "ax = axes[1]\n",
    "win_length = 50\n",
    "freqs = librosa.fft_frequencies(sr=sampling_rate, n_fft=win_length)\n",
    "\n",
    "if use_spectral_representation:\n",
    "    END = 200\n",
    "    DIFF = END - START\n",
    "    S_db = librosa.amplitude_to_db(train_band_data, ref=np.max)[:,START:END]\n",
    "\n",
    "    # Plot the spectrogram using imshow for manual control\n",
    "    img = ax.imshow(\n",
    "        S_db,\n",
    "        aspect='auto',\n",
    "        origin='lower',\n",
    "        interpolation='none',\n",
    "        cmap='plasma'\n",
    "    )\n",
    "    \n",
    "    # Adjust the yticks to show one out of every two labels\n",
    "    num_freq_bins = S_db.shape[0]\n",
    "    ytick_labels = [f\"{int(freqs[i])}\" if i % 2 == 0 else \"\" for i in range(num_freq_bins)]\n",
    "    ax.set_yticks(np.arange(num_freq_bins))  # Keep all the ticks\n",
    "    ax.set_yticklabels(ytick_labels, fontsize=fontsize)  # Set custom labels\n",
    "    ax.tick_params(axis='both', labelsize=fontsize)\n",
    "\n",
    "    ax.set_xlabel('Steps', fontsize=fontsize)\n",
    "    ax.set_title('Transformed spectral Data', fontsize=fontsize)\n",
    "    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "else:\n",
    "    random_indexes = np.random.randint(0, train_band_data.shape[0], size=3)\n",
    "    for idx, i in enumerate(random_indexes):\n",
    "        ax.plot(range(DIFF), train_band_data[i, START:END],\n",
    "                color=colors_step2[idx % len(colors_step2)], label=f'Feature {i}')\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=fontsize)\n",
    "    ax.set_xlabel('Time', fontsize=fontsize)\n",
    "    ax.set_title('Transformed spectral Data', fontsize=fontsize)\n",
    "    ax.legend(fontsize=fontsize, loc='upper right')\n",
    "\n",
    "# Third Plot: Normalized Data\n",
    "if is_instances_classification:\n",
    "    train_band_data = np.concatenate(X_train_band, axis=0).T\n",
    "else:\n",
    "    train_band_data = X_train_band.T\n",
    "ax = axes[2]\n",
    "if 'random_indexes' not in locals():\n",
    "    random_indexes = np.random.randint(0, train_band_data.shape[0], size=3)\n",
    "    \n",
    "for idx, i in enumerate(random_indexes):\n",
    "    ax.plot(range(DIFF), train_band_data[i, START:END], color=colors_step3[idx % len(colors_step3)],\n",
    "            label=f'Feature {i}')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "ax.set_xlabel('Steps', fontsize=fontsize)\n",
    "#ax.set_ylabel('Normalized Value', fontsize=fontsize)\n",
    "ax.set_title('Normalized spectral Data', fontsize=fontsize)\n",
    "ax.legend(fontsize=fontsize, loc='upper right')\n",
    "\n",
    "# Add arrows between subplots to indicate transformation\n",
    "fig.tight_layout()\n",
    "\n",
    "# Draw arrows using ConnectionPatch for better control\n",
    "def add_arrow(axes_from, axes_to, x_from, x_to):\n",
    "    con = ConnectionPatch(\n",
    "        xyA=(x_from, 0.5), xyB=(x_to, 0.5),\n",
    "        coordsA='axes fraction', coordsB='axes fraction',\n",
    "        axesA=axes_from, axesB=axes_to,\n",
    "        arrowstyle=\"-|>\", color=\"k\", shrinkB=5\n",
    "    )\n",
    "    axes_from.add_artist(con)\n",
    "\n",
    "add_arrow(axes[0], axes[1], 0.94, -0.13)\n",
    "add_arrow(axes[1], axes[2], 1.25, -0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6f51b-0c6c-4cec-bbd5-1fff0223efed",
   "metadata": {},
   "source": [
    "## Pretrain\n",
    "\n",
    "We use the train dataset as pretraining (pretraining is an unsupervised task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5fc6e-c2cf-4441-817a-6ba38f14e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of instances you want to select\n",
    "x_size = len(X_train_band) if is_multivariate else len(X_train)\n",
    "num_samples_for_pretrain = 500 if x_size >= 500 else x_size\n",
    "if is_instances_classification:\n",
    "    indices = np.random.choice(x_size, num_samples_for_pretrain, replace=False)\n",
    "else:\n",
    "    indices = range(x_size)\n",
    "    \n",
    "if not is_multivariate:\n",
    "    X_pretrain_uni = np.array(X_train, dtype=object)[indices]\n",
    "X_pretrain_band = np.array(X_train_band, dtype=object)[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93f662-f197-4b0a-a140-090f3c0909d2",
   "metadata": {},
   "source": [
    "# Generating reservoirs\n",
    "\n",
    "We are interrested in two technique to genereate reservoir. \n",
    "* One is called mean-HAG (hadsp), was studied in previous paper, and recombines inputs based on their activity to reach a given activation target.\n",
    "* The other called variance-HAG (desp) recombines inputs in order to reach a given standard deviation of activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b33224-cc12-4edd-948c-235f415a4ab8",
   "metadata": {},
   "source": [
    "## Data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa63393-5c4e-4cce-9ed0-7caf47af8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    common_index = 1\n",
    "    print(\"Common index for multivariate classification should be 1\")\n",
    "    print(\"\\nCheck it ! \\nFirst array \", X_train_band[1].shape, \" and second array\", X_train_band[2].shape)\n",
    "    print(\"\\nCheck it ! \\nFirst array \", X_test_band[1].shape, \" and second array\", X_test_band[2].shape)\n",
    "    common_size = X_train_band[0].shape[common_index]\n",
    "else:\n",
    "    common_index = 1\n",
    "    print(\"Common index for multivariate prediction should be 1\")\n",
    "    print(\"\\nCheck it ! \\nFirst array \", X_train_band.shape, \" and second array\", X_train_band.shape)\n",
    "    common_size = X_train_band.shape[common_index]\n",
    "\n",
    "print(\"Common size:\", common_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6435e2-6f19-4987-9309-ab74694678fd",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Shared parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8001c-ba74-457f-9518-c0afa04f3606",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Reservoir parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a38d2df41a727",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from models.activation_functions import tanh\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))tanh(x)\n",
    "activation_function = lambda x : tanh(x)\n",
    "\n",
    "x=np.linspace(0, 2, 100)\n",
    "plt.plot(x, activation_function(x))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4dc28-34d5-4a98-a047-7fa6e4e9da47",
   "metadata": {},
   "source": [
    "**common_size** : the number of different dimensions in the input data\n",
    "\n",
    "**K** : the number of neurons that will receive a particular time serie as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433d6d3-acc2-4bbc-8373-12d692eda119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "RESERVOIR_SIZE = 500\n",
    "\n",
    "# We want the size of the models to be at least RESERVOIR_SIZE\n",
    "K = math.ceil(RESERVOIR_SIZE / common_size)\n",
    "n = common_size * K\n",
    "print(\"Dimension of our models :\", n)\n",
    "print(\"Copy of each time serie :\", K)\n",
    "print(\"Number of multivariate inputs :\", common_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f3072-3e8d-49f1-a890-5c3a28df3d9e",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc2c01-de54-4bf6-8fa0-1dbde5fe21c2",
   "metadata": {},
   "source": [
    "#### Customs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6c7bf77-e19f-4790-b401-a40d219d8e7c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "time_increment = 82 # int(max_window_size) or int(min_window_size+1)\n",
    "time_increment_span = 14\n",
    "MAX_TIME_INCREMENT = TIME_INCREMENT + time_increment_span #int(max_window_size) or None  or TIME_INCREMENT\n",
    "weight_increment = 0.030000000000000002\n",
    "\n",
    "target_rate = 0.58\n",
    "RATE_SPREAD = 0.34\n",
    "\n",
    "min_variance = 0.014000000000000002\n",
    "variance_spread = 0.003\n",
    "\n",
    "bias_scaling = 0.0\n",
    "input_scaling = 0.060000000000000005\n",
    "\n",
    "intrinsic_saturation = 0.92\n",
    "intrinsic_coef = 0.8\n",
    "\n",
    "max_partners= 20\n",
    "use_full_instance = False\n",
    "\n",
    "# Doesn't move :\n",
    "input_connectivity = 1\n",
    "connectivity = 0\n",
    "leaky_rate = 1\n",
    "if int(max_window_size) < TIME_INCREMENT or TIME_INCREMENT < min_window_size:\n",
    "    raise ValueError(f\"INCREMENT must be greater than {min_window_size} and smaller than {max_window_size}. Current INCREMENT is {TIME_INCREMENT}.\")\n",
    "\n",
    "TIME_INCREMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ba684-539a-4f94-99f0-9bf242717c8b",
   "metadata": {},
   "source": [
    "#### Optuna's bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae9536-b10e-43b5-9c28-3e66dc8d10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from performances.utility import retrieve_best_model\n",
    "\n",
    "function_name = \"hadsp\"  # \"desp\" ou \"hadsp\" or \"random\"\n",
    "study = retrieve_best_model(function_name, dataset_name, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \")\n",
    "for param_name, param_value in study.best_trial.params.items():\n",
    "    globals()[param_name] = param_value\n",
    "    print(param_name, param_value)\n",
    "\n",
    "if not is_instances_classification:\n",
    "    use_full_instance = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fd1fc57ed9157",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Function to initialise and generate reservoir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39af15",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from models.reservoir import init_matrices\n",
    "from connexion_generation.hag import run_algorithm\n",
    "from scipy import sparse\n",
    "\n",
    "target_rate = 0.7\n",
    "rate_spread = 0.1\n",
    "\n",
    "def initialise_and_run(input_scaling, n, input_connectivity, connectivity, K, bias_scaling, seed, training_set):\n",
    "    \n",
    "    Win, W, bias = init_matrices(n, input_connectivity, connectivity,  K, seed=seed)\n",
    "    bias *= bias_scaling\n",
    "    Win *= input_scaling\n",
    "\n",
    "    if function_name == \"hadsp\":\n",
    "        W, (state_history, delta_z_history, W_history) = run_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, \n",
    "                                weight_increment, target_rate, rate_spread, function_name, \n",
    "                                multiple_instances=is_instances_classification, min_increment = min_increment, max_increment=max_increment, use_full_instance = use_full_instance, \n",
    "                                max_partners=np.inf, method = \"pearson\", \n",
    "                                n_jobs = 12, visualize=False, record_history=True)\n",
    "\n",
    "    elif function_name == \"desp\":\n",
    "        W, (state_history, delta_z_history, W_history) = run_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, \n",
    "                            weight_increment, min_variance, variance_spread, function_name, \n",
    "                            multiple_instances=is_instances_classification, min_increment = min_increment, max_increment=max_increment, use_full_instance = use_full_instance, \n",
    "                            max_partners=np.inf, method = \"pearson\", \n",
    "                            intrinsic_saturation=intrinsic_saturation, intrinsic_coef=intrinsic_coef, n_jobs = 12, visualize=False, record_history=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid function_name: {function_name}, should be 'hadsp' or 'desp'\")\n",
    "    connectivity =  np.count_nonzero(W) / (W.shape[0] * W.shape[1])\n",
    "    eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr = np.max(np.abs(eigen))\n",
    "\n",
    "    \n",
    "    \n",
    "    return Win, W, bias, connectivity, sr, state_history, delta_z_history, W_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4de89f833989d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## HAG algorithm generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54472b03-ca39-4f13-92cf-e71e6d63c135",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(Win_hag_multi, \n",
    " W_hag_multi, \n",
    " bias_hag_multi, \n",
    " connectivity_band, \n",
    " sr_hag_multi, \n",
    " state_history_hag,\n",
    " delta_z_history,\n",
    " W_history) = initialise_and_run(input_scaling, n, input_connectivity, connectivity, K, bias_scaling, SEED, X_pretrain_band)\n",
    "\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "custom_colormap = ListedColormap(np.vstack((plt.cm.cividis(0.0), plt.cm.cividis(np.linspace(0.5, 1, 128)))))\n",
    "vmin = 0\n",
    "vmax = max(np.max(Win_hag_multi), np.max(W_hag_multi))\n",
    "fig, axs = plt.subplots(ncols=3, gridspec_kw=dict(width_ratios=[0.5,6,0.2]))\n",
    "heatmap(Win_hag_multi, cmap=custom_colormap, cbar=False, square=True, ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "heatmap(W_hag_multi, cmap=custom_colormap, yticklabels=False, cbar=False, ax=axs[1], vmin=vmin, vmax=vmax)\n",
    "fig.colorbar(axs[1].collections[0], cax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c886405-da86-4b5e-858b-fabeff6f5473",
   "metadata": {},
   "source": [
    "## Random matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d44149-8fd2-4528-9cdf-1e384eff50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reservoir import init_matrices\n",
    "from scipy import sparse\n",
    "from scipy import stats\n",
    "\n",
    "# To set particular values\n",
    "function_name = \"random_ee\"  # \"desp\" ou \"hadsp\" or \"random_ee\"\n",
    "study = retrieve_best_model(function_name, dataset_name, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \")\n",
    "random_esn = {}\n",
    "for param_name, param_value in study.best_trial.params.items():\n",
    "    random_esn[param_name] = param_value\n",
    "    print(param_name, param_value)\n",
    "\n",
    "if not is_instances_classification:\n",
    "    use_full_instance = None\n",
    "\n",
    "if function_name == \"random_ee\":\n",
    "    Win_random_multi, W_random_multi, bias_random_multi = init_matrices(n, input_connectivity, random_esn['connectivity'],  K, w_distribution=stats.uniform(loc=0, scale=1), seed=57489)\n",
    "else:\n",
    "    Win_random_multi, W_random_multi, bias_random_multi = init_matrices(n, input_connectivity, random_esn['connectivity'],  K, w_distribution=stats.uniform(loc=-1, scale=2), seed=57489)\n",
    "      \n",
    "bias_random_multi= bias_random_multi*random_esn['bias_scaling']\n",
    "Win_random_multi= Win_random_multi*random_esn['input_scaling']\n",
    "\n",
    "\n",
    "eigen_random_multi = sparse.linalg.eigs(W_random_multi, k=1, which=\"LM\", maxiter=W_random_multi.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "sr_random_multi = np.max(np.abs(eigen_random_multi))\n",
    "W_random_multi = W_random_multi * sr_random_multi / eigen_random_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9a1da-cc49-4b49-aa7c-935d8c12cb2c",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206b894-d2b6-43a2-8f23-8b62678c44d7",
   "metadata": {},
   "source": [
    "## Algorithm Dynamics\n",
    "\n",
    "### âˆ†z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be4078-9172-48f0-b39f-9b9c634c136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if delta_z_history is not None:\n",
    "    fig, ax = plt.subplots(figsize=(16, 5))\n",
    "    print(f'Dynamics')\n",
    "    \n",
    "    # NEURON ACTIVITY PLOT\n",
    "    random_neurons_indices =  np.sort(np.random.randint(RESERVOIR_SIZE, size=7)) #Size max is 19 because there is not enough colors\n",
    "    random_neurons_indices = np.append(random_neurons_indices, 51)\n",
    "    colors = color_palette(\"tab20\")\n",
    "    # NUMBER_OF_STEP_TO_WATCH\n",
    "    WATCH_FROM = 0\n",
    "    WATCH_TO = 3000\n",
    "    neurons_evolution = np.array(delta_z_history)[WATCH_FROM:WATCH_TO]\n",
    "    x =range(len(delta_z_history))[WATCH_FROM:WATCH_TO]\n",
    "    j = 0\n",
    "    for i in random_neurons_indices:\n",
    "        ax.plot(x, neurons_evolution[: ,i], label=str(i), color=colors[j])\n",
    "        j += 1\n",
    "    fontsize=18\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=fontsize)\n",
    "    plt.xlabel('Time', size=fontsize)\n",
    "    plt.ylabel('Value', size=fontsize)\n",
    "    plt.legend(fontsize=fontsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466f83e-ce7c-4625-93a2-a8630dcbd57c",
   "metadata": {},
   "source": [
    "### Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef5f04-6a58-46eb-a958-3937fa36947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "function_name = \"hadsp\"\n",
    "# NEURON ACTIVITY PLOT \n",
    "random_neurons_indices =  np.sort(np.random.randint(RESERVOIR_SIZE, size=4)) #Size max is 19 because there is not enough colors\n",
    "colors = color_palette(\"tab20\")\n",
    "# NUMBER_OF_STEP_TO_WATCH \n",
    "WATCH_FROM = 0\n",
    "WATCH_TO = 100000\n",
    "neurons_evolution = np.array(state_history_hag)[WATCH_FROM:WATCH_TO]\n",
    "x =range(len(state_history_hag))[WATCH_FROM:WATCH_TO]\n",
    "j = 0\n",
    "for i in random_neurons_indices:\n",
    "    ax.plot(x, neurons_evolution[: ,i], label=str(i), color=colors[j])\n",
    "    j += 1\n",
    "if function_name == \"hadsp\":\n",
    "    ax.plot(x, np.ones(len(x))*(target_rate-rate_spread), color=\"red\", linestyle = \"--\")\n",
    "    ax.plot(x, np.ones(len(x))*(target_rate+rate_spread), color=\"red\", linestyle = \"--\")\n",
    "    ax.text(WATCH_TO, target_rate - rate_spread, r'$\\rho_r - \\beta_r$', color='red', fontsize=fontsize, va='center', ha='left')\n",
    "    ax.text(WATCH_TO, target_rate + rate_spread, r'$\\rho_r + \\beta_r$', color='red', fontsize=fontsize, va='center', ha='left')\n",
    "\n",
    "# target_rate, rate_spread\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Time', size=fontsize)\n",
    "plt.ylabel('Neuron Activity', size=fontsize)\n",
    "plt.legend(title=\"Neurons :\",fontsize=fontsize, title_fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0eae33-4a27-49ce-b7b5-1be0dadd9c7e",
   "metadata": {},
   "source": [
    "## Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24170c46-7cc0-4b20-85bf-e29b22589da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.richness import pearson\n",
    "\n",
    "num_windows = 1500\n",
    "size_window = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e041190-1f47-4252-bfe4-6c55e894ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_correlations_hag, std_correlations_hag = pearson(state_history_hag, size_window=size_window, num_windows=num_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696a2a0-a60f-4704-b947-605e58b7879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reservoir import update_reservoir\n",
    "\n",
    "states_history_multi = []\n",
    "neurons_state = np.random.uniform(0, 1, bias_hag_multi.size)\n",
    "inputs = np.concatenate(X_pretrain_band, axis=0) if is_instances_classification else X_pretrain_band\n",
    "for input_value in inputs:\n",
    "    neurons_state = update_reservoir(W_hag_multi, Win_hag_multi, input_value, neurons_state, leaky_rate, bias_hag_multi, activation_function)\n",
    "    states_history_multi.append(neurons_state)\n",
    "\n",
    "mean_correlations_multi, std_correlations_multi = pearson(states_history_multi, num_windows=num_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386591fb-1869-4f7a-8a0b-fd12acee7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(num_windows)\n",
    "plt.plot(time_windows, mean_correlations_multi, marker='.', linestyle='-', color='b',  label=\"with same input after training\")\n",
    "plt.fill_between(time_windows, np.array(mean_correlations_multi) - np.array(std_correlations_multi),\n",
    "                 np.array(mean_correlations_multi) + np.array(std_correlations_multi), color='blue', alpha=0.2)\n",
    "\n",
    "plt.plot(time_windows, mean_correlations_hag, marker='.', linestyle='-', color='red', label=\"during training\")\n",
    "plt.fill_between(time_windows, np.array(mean_correlations_hag) - np.array(std_correlations_hag),\n",
    "                 np.array(mean_correlations_hag) + np.array(std_correlations_hag), color='red', alpha=0.2)\n",
    "plt.ylim(0, 1)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Correlation', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42acc5c-4ab9-42a2-bf63-194210f62198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reservoir import update_reservoir\n",
    "\n",
    "states_history_random = []\n",
    "neurons_state = np.random.uniform(0, 1, bias_random_multi.size)\n",
    "inputs = np.concatenate(X_pretrain_band, axis=0) if is_instances_classification else X_pretrain_band\n",
    "for input_value in inputs:\n",
    "    neurons_state = update_reservoir(W_random_multi, Win_random_multi, input_value, neurons_state, leaky_rate, bias_random_multi, activation_function)\n",
    "    states_history_random.append(neurons_state)\n",
    "\n",
    "mean_correlations_random, std_correlations_random = pearson(states_history_random, num_windows=num_windows)\n",
    "\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(num_windows)\n",
    "plt.plot(time_windows, mean_correlations_random, marker='.', linestyle='-', color='g',  label=\"with same input for random matrix\")\n",
    "plt.fill_between(time_windows, np.array(mean_correlations_random) - np.array(std_correlations_random),\n",
    "                 np.array(mean_correlations_random) + np.array(std_correlations_random), color='g', alpha=0.2)\n",
    "plt.ylim(0, 1)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Correlation', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03592584-13e0-4abb-b06e-b68cc6b3a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_correlations_random, std_correlations_random = pearson(inputs, num_windows=num_windows)\n",
    "\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(num_windows)\n",
    "plt.plot(time_windows, mean_correlations_random, marker='.', linestyle='-', color='y',  label=\"inputs correlation\")\n",
    "plt.fill_between(time_windows, np.array(mean_correlations_random) - np.array(std_correlations_random),\n",
    "                 np.array(mean_correlations_random) + np.array(std_correlations_random), color='y', alpha=0.2)\n",
    "plt.ylim(0, 1)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Correlation', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272473cc-1309-4a35-978e-d6197c3a8f31",
   "metadata": {},
   "source": [
    "## Cumulative explained variance\n",
    "Or squared_uncoupled_dynamics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b6cd7-d060-4ba4-add7-13e0a492ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import squared_uncoupled_dynamics_alternative\n",
    "\n",
    "uds_hag = squared_uncoupled_dynamics_alternative(np.array(state_history_hag), size_window=500, num_windows=num_windows, A=0.99)\n",
    "uds_multi = squared_uncoupled_dynamics_alternative(np.array(states_history_multi), size_window=500, num_windows=num_windows, A=0.99)\n",
    "\n",
    "max_value = np.max([np.max(uds_hag), np.max(uds_hag)])\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(uds_hag))\n",
    "plt.plot(range(len(uds_hag)), uds_hag, marker='.', linestyle='-', color='r',  label=\"during training\")\n",
    "plt.plot(range(len(uds_multi)), uds_multi, marker='.', linestyle='-', color='b',  label=\"with same input after training\")\n",
    "plt.ylim(0, max_value)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('CEV', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8580e57-d01b-4f38-b535-6b47e2850277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "\n",
    "uds_random = squared_uncoupled_dynamics_alternative(states_history_random, size_window=500, num_windows=num_windows, A=0.99)\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "plt.plot(range(len(uds_random)), uds_random, marker='.', linestyle='-', color='g',  label=\"with same input for random matrix\")\n",
    "\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('CEV', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc60097-34c7-4c5e-9ff9-a4e50bd8cee0",
   "metadata": {},
   "source": [
    "## Linear uncoupled dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6a8db-77c0-461e-b65a-319a9fa362b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import linear_uncoupled_dynamics\n",
    "num_windows = 150\n",
    "\n",
    "LUDs_hag = linear_uncoupled_dynamics(np.array(state_history_hag), size_window=500, num_windows=num_windows, theta=0.9)\n",
    "LUDs_multi = linear_uncoupled_dynamics(np.array(states_history_multi), size_window=500, num_windows=num_windows, theta=0.9)\n",
    "\n",
    "max_value = np.max([np.max(LUDs_hag), np.max(LUDs_multi)])\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(LUDs_hag))\n",
    "plt.plot(range(len(uds_hag)), LUDs_hag, marker='.', linestyle='-', color='r',  label=\"during training\")\n",
    "plt.plot(range(len(LUDs_multi)), LUDs_multi, marker='.', linestyle='-', color='b',  label=\"with same input after training\")\n",
    "plt.ylim(0, max_value)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Linear Uncoupled Dynamics', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086ba68-ffa6-404b-81c4-3c553e92c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import linear_uncoupled_dynamics\n",
    "\n",
    "LUDs_random = linear_uncoupled_dynamics(np.array(states_history_random), size_window=500, num_windows=num_windows, theta=0.9)\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(LUDs_random))\n",
    "plt.plot(time_windows, LUDs_random, marker='.', linestyle='-', color='g',  label=\"with same input for random matrice\")\n",
    "fontsize=18\n",
    "max_value = np.max([np.max(LUDs_random)])\n",
    "plt.ylim(0, max_value)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Linear Uncoupled Dynamics', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32b731-f24b-4da3-9273-aff069b3b9e5",
   "metadata": {},
   "source": [
    "## Condition number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74141fa4-b0cf-427e-b534-a0fed799ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import condition_number\n",
    "num_windows = 150\n",
    "\n",
    "CNs_hag = condition_number(np.array(state_history_hag), size_window=500, num_windows=num_windows)\n",
    "CNs_multi = condition_number(np.array(states_history_multi), size_window=500, num_windows=num_windows)\n",
    "\n",
    "max_value = np.max([np.max(CNs_hag), np.max(CNs_hag)])\n",
    "min_value = np.min([np.min(CNs_hag), np.min(CNs_hag)])\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(CNs_hag))\n",
    "plt.plot(range(len(uds_hag)), CNs_hag, marker='.', linestyle='-', color='r',  label=\"during training\")\n",
    "plt.plot(range(len(uds_multi)), CNs_multi, marker='.', linestyle='-', color='b',  label=\"with same input after training\")\n",
    "plt.ylim(min_value, max_value)\n",
    "fontsize=18\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Condition Numbers', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956ac90-411a-4b4e-8a8a-ef80ffb3f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import condition_number\n",
    "\n",
    "CNs_random = condition_number(np.array(states_history_random), size_window=500, num_windows=num_windows)\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(CNs_random))\n",
    "plt.plot(time_windows, CNs_random, marker='.', linestyle='-', color='g',  label=\"with same input for random matrice\")\n",
    "fontsize=18\n",
    "max_value = np.max([np.max(CNs_random)])\n",
    "min_value = np.min([np.min(CNs_random)])\n",
    "\n",
    "plt.ylim(min_value, max_value)\n",
    "ax.set_yscale('log')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Linear Uncoupled Dynamics', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b3d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:36:57.885053Z",
     "start_time": "2023-10-09T13:36:57.882050Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Univariate case"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70e02ca4-fea0-464f-b21b-29d141aa79b8",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-22T12:57:10.271291Z",
     "iopub.status.busy": "2024-07-22T12:57:10.271110Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "if not is_multivariate:\n",
    "    # hag + uni\n",
    "    (Win_hag_uni, \n",
    "     W_hag_uni, \n",
    "     bias_hag_uni, \n",
    "     connectivity_hag_uni, \n",
    "     sr_hag_uni,\n",
    "     state_history_hag_uni,\n",
    "     _,\n",
    "     _) = initialise_and_train(input_scaling, n, input_connectivity, connectivity, n, bias_scaling, SEED, X_pretrain_uni)\n",
    "\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    custom_colormap = ListedColormap(np.vstack((plt.cm.cividis(0.0), plt.cm.cividis(np.linspace(0.5, 1, 128)))))\n",
    "    vmin = 0\n",
    "    vmax = max(np.max(Win_hag_uni), np.max(W_hag_uni))\n",
    "    fig, axs = plt.subplots(ncols=3, gridspec_kw=dict(width_ratios=[0.5,6,0.2]))\n",
    "    heatmap(Win_hag_uni, cmap=custom_colormap, cbar=False, square=True, ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "    heatmap(W_hag_uni, cmap=custom_colormap, yticklabels=False, cbar=False, ax=axs[1], vmin=vmin, vmax=vmax)\n",
    "    fig.colorbar(axs[1].collections[0], cax=axs[2])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73fe1a2c-c54e-4ae8-80fe-f57447c8dbe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T12:41:26.032884Z",
     "iopub.status.busy": "2024-07-22T12:41:26.032101Z",
     "iopub.status.idle": "2024-07-22T12:43:14.065679Z",
     "shell.execute_reply": "2024-07-22T12:43:14.065383Z",
     "shell.execute_reply.started": "2024-07-22T12:41:26.032821Z"
    }
   },
   "source": [
    "from analysis.richness import pearson\n",
    "\n",
    "pearson(state_history_hag_uni)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d205ad0-0c4e-4e76-9a21-d69d1b963c54",
   "metadata": {},
   "source": [
    "if not is_multivariate:    \n",
    "    # random + uni\n",
    "    Win_normal, W_normal, bias_normal = init_matrices(n, 1, connectivity_hag_uni, n, sr_hag_uni)\n",
    "    bias_normal= bias_normal*bias_scaling\n",
    "    Win_normal= Win_normal*input_scaling\n",
    "    \n",
    "    eigen_normal = sparse.linalg.eigs(W_normal, k=1, which=\"LM\", maxiter=W_normal.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr_normal = np.max(np.abs(eigen_normal))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "084e5dfe-8d4d-4e7e-89af-01fbe9e436d1",
   "metadata": {},
   "source": [
    "print(connectivity_band)\n",
    "if not is_multivariate:\n",
    "    print(connectivity_hag_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b447f6-e964-4f6c-9116-597cd29c1755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:13:26.845564Z",
     "start_time": "2023-10-09T13:13:26.821527Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ffd9c-ffc5-47ea-885d-f2f2a3e549fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = -1\n",
    "RIDGE_COEF= 1e-9 if ridge is None else 10**ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a95dbc-2f36-485e-b1c4-85f6ac6f30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import init_reservoir, init_readout\n",
    "\n",
    "reservoir_hag_multi = init_reservoir(W_hag_multi, Win_hag_multi, bias_hag_multi, leaky_rate, activation_function)\n",
    "readout_hag_multi = init_readout(ridge_coef=RIDGE_COEF)\n",
    "reservoir_random_multi = init_reservoir(W_random_multi, Win_random_multi, bias_random_multi, leaky_rate, activation_function)\n",
    "readout_random_multi = init_readout(ridge_coef=RIDGE_COEF)\n",
    "\n",
    "if False: # TODO: change condition for when we want to plot univariate data\n",
    "    reservoir_hag_uni  = init_reservoir_model(W_hag_uni, Win_hag_uni, bias_hag_uni, leaky_rate, activation_function)\n",
    "    readout_hag_uni = init_readout(ridge_coef=RIDGE_COEF)\n",
    "    reservoir_random_uni = init_reservoir_model(W_random_uni, Win_random_uni, bias_random_uni, leaky_rate, activation_function)\n",
    "    readout_hag_uni = init_readout(ridge_coef=RIDGE_COEF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27380a6-3976-442d-8bb0-34bfc142985a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08537a67-9097-4930-b5af-29659c8cfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_instances_classification:\n",
    "    raise ValueError(\"This is not the right Classification section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b3a4e-b06e-450a-b7c5-68802be8013d",
   "metadata": {},
   "source": [
    "### Classification for multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3534f99-25b5-4653-8dcf-7073183e0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_band_duplicated example shape :\", X_train_band[1].shape)     \n",
    "print(\"We should have :\", X_train_band[0].shape[1], \"==\", common_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9130e-a834-4256-8f29-e42add2eee19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import train_model_for_classification\n",
    "\n",
    "# To remember :\n",
    "#  For reservoirpy   pre_s = W @ r + Win @ (u + noise_gen(dist=dist, shape=u.shape, gain=g_in)) + bias\n",
    "\n",
    "train_data_multi = X_train_band # X_train_band_noisy or X_train_band\n",
    "Y_data = Y_val if use_cross_validation else Y_test\n",
    "\n",
    "mode=\"sequence-to-vector\"\n",
    "train_model_for_classification(reservoir_hag_multi, readout_hag_multi, train_data_multi, Y_train, n_jobs = N_JOBS, mode=mode)\n",
    "train_model_for_classification(reservoir_random_multi, readout_random_multi, train_data_multi, Y_train, N_JOBS, mode=mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e344e-de91-4709-bfce-467b1d049546",
   "metadata": {},
   "source": [
    "#### noisy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3dc743-36c8-4821-948d-f14f43af8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import predict_model_for_classification, compute_score\n",
    "\n",
    "test_data_multi_noisy = X_test_band_noisy # X_test_band_noisy or X_test_band\n",
    "\n",
    "Y_pred_hag_multi = predict_model_for_classification(reservoir_hag_multi, readout_hag_multi, test_data_multi_noisy, N_JOBS)\n",
    "score = compute_score(Y_pred_hag_multi, Y_data, is_instances_classification, function_name + \" multi\", verbosity=1)\n",
    "\n",
    "Y_pred_random_multi = predict_model_for_classification(reservoir_random_multi, readout_random_multi, test_data_multi_noisy, N_JOBS)\n",
    "score = compute_score(Y_pred_random_multi, Y_data, is_instances_classification, \"random multi\", verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6287d16-2f61-4d2a-97e3-0a0844918879",
   "metadata": {},
   "source": [
    "#### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49735fb0-8701-4a4f-8d0c-a319c142f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_multi = X_test_band # X_test_band_noisy or X_test_band\n",
    "\n",
    "Y_pred_hag_multi = predict_model_for_classification(reservoir_hag_multi, readout_hag_multi, test_data_multi, -1)\n",
    "score = compute_score(Y_pred_hag_multi, Y_data, is_instances_classification, function_name + \" multi\", verbosity=1)\n",
    "\n",
    "Y_pred_random_multi = predict_model_for_classification(reservoir_random_multi, readout_random_multi, test_data_multi, N_JOBS)\n",
    "score = compute_score(Y_pred_random_multi, Y_data, is_instances_classification, \"random multi\", verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f3553-1d28-4340-b4b9-f3dc673bdcac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:59:05.097521Z",
     "start_time": "2023-10-09T13:59:04.992489Z"
    }
   },
   "source": [
    "### Classification for univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50e2ca-93d7-48bb-bcff-4f885e8d5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # TODO: change condition for when we want to plot univariate data\n",
    "    # Create a list to store the arrays with the same shape as the expected input of the models\n",
    "\n",
    "    train_data_uni = [ts.reshape(-1, 1) for ts in X_train]\n",
    "    test_data_uni = [ts.reshape(-1, 1) for ts in X_test]\n",
    "\n",
    "    print(\"number of instances in train_data_uni :\", len(train_data_uni), \"should be equal to\", len(X_train))     \n",
    "    print(\"example of train_data_uni train shape :\", train_data_uni[0].shape)     \n",
    "    print(\"We should have :\", train_data_uni[0].shape[1], \"==\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d01e8-8763-4bf3-ae0a-4c4569231dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # TODO: change condition for when we want to plot univariate data\n",
    "    reservoir_hag_uni, readout_hag_uni = train_model_for_classification(W_hag_uni, Win_hag_uni, bias_hag_uni, leaky_rate, activation_function, train_data_uni, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")\n",
    "\n",
    "    reservoir_random_uni, readout_random_uni = train_model_for_classification(W_normal, Win_normal, bias_normal, leaky_rate, activation_function, train_data_uni, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f4028-f457-46b4-a219-7f5fed575af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # TODO: change condition for when we want to plot univariate data\n",
    "    Y_pred_hag_uni = predict_model_for_classification(reservoir_hag_uni, readout_hag_uni, test_data_uni, N_JOBS)\n",
    "    score = compute_score(Y_pred_hag_uni, Y_test, is_instances_classification, function_name + \" uni\", verbosity=1)\n",
    "    \n",
    "    Y_pred_normal = predict_model_for_classification(reservoir_random_uni, readout_random_uni, test_data_uni, N_JOBS)\n",
    "    score = compute_score(Y_pred_normal, Y_test, is_instances_classification, \"random uni\", verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7ab1a-e020-489a-8ff9-9bbd689de15e",
   "metadata": {},
   "source": [
    "## Prediction ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c06e5-71c1-4f72-ac0f-7e4f1f15afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    raise ValueError(\"This is not the right Classification section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11c5fc-45b0-4180-9953-bb13800da95b",
   "metadata": {},
   "source": [
    "### Plot datasets\n",
    "Noisy or normal dataset can be ploted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d45a7-49c9-4e1e-a473-652bc90d64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test arrays for plotting\n",
    "combined_data = np.concatenate((X_train_band, X_val_band), axis=0)\n",
    "\n",
    "# noisy version\n",
    "combined_data_noisy = np.concatenate((X_train_band, X_val_band_noisy), axis=0)\n",
    "combined_Y =np.concatenate((Y_train, Y_val), axis=0)\n",
    "\n",
    "# Calculate the merge point index\n",
    "merge_point_index = X_train_band.shape[0]\n",
    "\n",
    "# Define the range around the merge point to plot\n",
    "start_index = merge_point_index - 100\n",
    "end_index = merge_point_index + 100\n",
    "\n",
    "# Plot for a subset N features within a range arround transition from train to test\n",
    "N = 3\n",
    "plt.figure(figsize=(16, 5))\n",
    "for i in [1, 13, 17]: \n",
    "    plt.plot(range(start_index, end_index), combined_data_noisy[start_index:end_index, i], label=f'Feature {i}')\n",
    "plt.plot(range(start_index, end_index), combined_Y[start_index:end_index], label=\"Prediction\")\n",
    "plt.title('Feature Values Around Merge Point')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Feature Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bcdbf",
   "metadata": {},
   "source": [
    "### Training\n",
    "Noisy or normal dataset can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d012581-4559-4924-8ba1-9d1b6dea2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import train_model_for_prediction\n",
    "\n",
    "if not is_multivariate:\n",
    "    train_data_uni = X_train # X_train_noisy or X_train\n",
    "    \n",
    "    # Training random + MG\n",
    "    esn_random_uni = train_model_for_prediction(reservoir_random_uni, readout_random_uni, train_data_uni, Y_train)\n",
    "    \n",
    "    # Training for hag + MG\n",
    "    esn_hag_uni = train_model_for_prediction(reservoir_hag_uni, readout_hag_uni, train_data_uni, Y_train)\n",
    "\n",
    "    \n",
    "train_data_multi = X_train_band # X_train_band_noisy or train_band_inputs\n",
    "\n",
    "# Training random + bandfilter\n",
    "esn_random_multi = train_model_for_prediction(reservoir_random_multi, readout_random_multi, train_data_multi, Y_train)\n",
    "\n",
    "# Training output HASDP + bandfilter\n",
    "esn_hag_multi = train_model_for_prediction(reservoir_hag_multi, readout_hag_multi,train_data_multi, Y_train)\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcfe07-1ad0-4d8f-827f-bb9776e8086e",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Noisy or normal dataset can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dac84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    test_data_uni = X_val # X_val_noisy or X_val\n",
    "\n",
    "    # Prediction for random + MG\n",
    "    y_pred_random_uni = esn_random_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "    # Prediction for hag + MG\n",
    "    y_pred_hag_uni = esn_hag_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "\n",
    "test_data_multi = X_val_band # X_test_band_noisy_duplicated or X_test_band_duplicated\n",
    "\n",
    "# Prediction for random + bandfilter\n",
    "y_pred_random_multi = esn_random_multi.run(test_data_multi, reset=False)\n",
    "\n",
    "# Prediction for hag + bandfilter\n",
    "y_pred_hag_multi = esn_hag_multi.run(test_data_multi, reset=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61165568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from performances.plots import plot_prediction_vs_actual\n",
    "from performances.esn_model_evaluation import compute_score\n",
    "\n",
    "START_STEP = 30\n",
    "END_STEP = 500\n",
    "slice_range = slice(START_STEP, END_STEP)\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(\"nrmse normal         :\", compute_score(Y_val[slice_range], y_pred_random_uni[slice_range], is_instances_classification))\n",
    "    print(\"nrmse hag          :\", compute_score(Y_val[slice_range], y_pred_hag_uni[slice_range], is_instances_classification))\n",
    "\n",
    "print(\"nrmse random + band  :\", compute_score(Y_val[slice_range], y_pred_random_multi[slice_range], is_instances_classification))\n",
    "print(\"nrmse hag + band   :\", compute_score(Y_val[slice_range], y_pred_hag_multi[slice_range], is_instances_classification))\n",
    "\n",
    "plot_prediction_vs_actual(y_pred_hag_multi, Y_test, 0, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645bd84-e26a-497c-9053-6f6dd4e69845",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    test_data_uni = X_val_noisy # X_val_noisy or X_val\n",
    "\n",
    "    # Prediction for random + MG\n",
    "    y_pred_random_uni = esn_random_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "    # Prediction for hag + MG\n",
    "    y_pred_hag_uni = esn_hag_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "test_data_multi = X_val_band_noisy # X_test_band_noisy_duplicated or X_test_band_duplicated\n",
    "\n",
    "# Prediction for random + bandfilter\n",
    "y_pred_random_multi = esn_random_multi.run(test_data_multi, reset=False)\n",
    "\n",
    "# Prediction for hag + bandfilter\n",
    "y_pred_hag_multi = esn_hag_multi.run(test_data_multi, reset=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d132a1-0766-497f-bd98-635048b14b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.plots import plot_prediction_vs_actual\n",
    "\n",
    "START_STEP = 50\n",
    "END_STEP = 500\n",
    "slice_range = slice(START_STEP, END_STEP)\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(\"nrmse normal         :\", compute_score(y_pred_random_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "    print(\"nrmse hag          :\", compute_score(y_pred_hag_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse random + band  :\", compute_score(y_pred_random_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse hag + band   :\", compute_score(y_pred_hag_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "\n",
    "plot_prediction_vs_actual(y_pred_hag_multi, Y_test, 0, 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb0591-4213-425e-92ff-a5da80e718dc",
   "metadata": {},
   "source": [
    "#### Moving average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5bf2d-368f-4e55-b068-c42c36efe2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import compute_score\n",
    "\n",
    "# moving average of the y\n",
    "span=7\n",
    "pad_width = span // 2\n",
    "\n",
    "if not is_multivariate:\n",
    "    ave_y_random_uni = np.convolve(np.pad(y_pred_random_uni.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "    ave_y_hag_uni = np.convolve(np.pad(y_pred_hag_uni.flatten(), pad_width, mode='edge') , np.ones(span), 'valid') / span\n",
    "ave_y_random_multi = np.convolve(np.pad(y_pred_random_multi.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "ave_y_hag_multi = np.convolve(np.pad(y_pred_hag_multi.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(\"nrmse normal         :\", compute_score(ave_y_random_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "    print(\"nrmse hag          :\", compute_score(ave_y_hag_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse random + band  :\", compute_score(ave_y_random_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse hag + band   :\", compute_score(ave_y_hag_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    " \n",
    "plot_prediction_vs_actual(ave_y_hag_multi.reshape(-1, 1), Y_test, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc19b97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nrmse_array_random_uni = []\n",
    "nrmse_array_hag_uni = []\n",
    "nrmse_array_random_multi = []\n",
    "nrmse_array_hag_multi = []\n",
    "\n",
    "for i in range(len(Y_val)-100- step_ahead):\n",
    "    Y_val_i = Y_val[i:100+i]\n",
    "    nrmse_array_random_uni.append(compute_score(Y_val_i, y_pred_random_uni[i:100+i], is_instances_classification))\n",
    "    nrmse_array_hag_uni.append(compute_score(Y_val_i, y_pred_hag_uni[i:100+i], is_instances_classification))\n",
    "    nrmse_array_random_multi.append(compute_score(Y_val_i, y_pred_random_multi[i:100+i], is_instances_classification))\n",
    "    nrmse_array_hag_multi.append(compute_score(Y_val_i, y_pred_hag_multi[i:100+i], is_instances_classification))\n",
    "    \n",
    "log10_nrmse_random_uni= np.log10(nrmse_array_random_uni)\n",
    "log10_nrmse_hag_uni = np.log10(nrmse_array_hag_uni)\n",
    "log10_nrmse_random_multi = np.log10(nrmse_array_random_multi)\n",
    "log10_nrmse_hag_multi = np.log10(nrmse_array_hag_multi)\n",
    "plt.figure()\n",
    "plt.plot(log10_nrmse_random_uni[:1000])\n",
    "plt.plot(log10_nrmse_hag_uni[:1000])\n",
    "plt.plot(log10_nrmse_random_multi[:1000])\n",
    "plt.plot(log10_nrmse_hag_multi[:1000])\n",
    "\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Log10 NRMSE')\n",
    "plt.legend([\"hag+band\", \"random\", \" random + bandfilter\", \"hag\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27da37",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2478c-bb79-4bc0-855f-b028109c8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# pretrain\n",
    "# Be really carefull of the column order here !\n",
    "df_data = scaler.fit_transform(X_pretrain_band.T)\n",
    "df_data = df_data.T\n",
    "df = pd.DataFrame(df_data.T)\n",
    "# Initialize a progress bar for total number of series\n",
    "progress_bar = tqdm(total=df.shape[1]**2, position=0, leave=True)\n",
    "\n",
    "# Initialize an empty correlation matrix\n",
    "correlation_matrix = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "\n",
    "# Calculate the correlation for each pair of series\n",
    "for col1 in df.columns:\n",
    "    progress_bar.set_description(f\"Processing {col1}\")\n",
    "    for col2 in df.columns:\n",
    "        correlation_matrix.loc[col1, col2] = df[col1].corr(df[col2], method='pearson', min_periods=5)\n",
    "\n",
    "        progress_bar.update(1)  # Update the progress bar after processing each series\n",
    "    \n",
    "progress_bar.close()\n",
    "\n",
    "# Convert correlation_matrix to numeric as it is stored as objects due to tqdm\n",
    "correlation_matrix = correlation_matrix.apply(pd.to_numeric)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linked = linkage(correlation_matrix, 'single')\n",
    "\n",
    "# Get the order of rows/columns after hierarchical clustering\n",
    "row_order = leaves_list(linked)\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "sorted_corr_matrix = correlation_matrix.iloc[row_order, :].iloc[:, row_order]\n",
    "\n",
    "# Visualize the sorted correlation matrix with a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(sorted_corr_matrix, annot=False, cmap='vlag', vmin=-1, vmax=1)\n",
    "plt.title('Clustered Pairwise Correlation of Time Series')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec25bb-f3e8-4cdb-87ee-9772460a8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "row_order_r = np.array([i + k for i in row_order*K for k in range(K)])\n",
    "\n",
    "# Convert the sparse matrix to a dense format (if memory allows)\n",
    "dense_matrix = W_hag_multi.toarray()\n",
    "\n",
    "# Reorder the dense matrix using the repeated ordering\n",
    "reordered_matrix = dense_matrix[np.ix_(row_order_r, row_order_r)]\n",
    "\n",
    "# Convert the reordered dense matrix back to a sparse format if needed\n",
    "sparse_reordered_matrix = coo_matrix(reordered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901730be-d407-4d6d-973d-9fc24951bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(sparse_reordered_matrix.todense(), cmap=color_palette(\"vlag\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e46145-b737-4e62-a8ed-9860be34eb2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T12:52:00.989091Z",
     "start_time": "2023-10-10T12:52:00.984122Z"
    }
   },
   "source": [
    "## Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78de13-7b2d-4ea0-9854-c7ef05c9780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.topology\n",
    "reload(analysis.topology)\n",
    "from analysis.topology import motif_distribution, draw_motifs_distribution\n",
    "\n",
    "motifs_count = motif_distribution(W_hag_multi.A)\n",
    "draw_motifs_distribution(motifs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7d48e-892f-44a0-9f33-a0b561e3a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4480d-5e4f-4603-90c0-e2f0576e0288",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson, binom\n",
    "\n",
    "def analyze_connectivity_matrix(matrix):\n",
    "    # Extract weights from the matrix (ignoring the diagonal and zeros)\n",
    "    weights = matrix.flatten()\n",
    "    weights = weights[weights != 0]\n",
    "    bin_centers, counts = np.unique(weights, return_counts=True)\n",
    "    \n",
    "    # Calculate the difference for all centers\n",
    "    diffs = np.diff(bin_centers)\n",
    "    # Add the last difference for the last bin\n",
    "    diffs = np.append(diffs, diffs[-1])\n",
    "    \n",
    "    # Calculate the bin edges based on bin centers and differences\n",
    "    bin_edges = bin_centers - diffs/2\n",
    "    # Add the last bin edge\n",
    "    bin_edges = np.append(bin_edges, bin_centers[-1] + diffs[-1]/2)\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.bar(bin_centers, counts, align='center', alpha=0.6, width=np.diff(bin_centers).min())\n",
    "    \n",
    "    # Fit to Poisson distribution\n",
    "    lambda_est = np.mean(weights)\n",
    "    plt.plot(bin_centers, poisson.pmf(range(len(bin_centers)), lambda_est)*counts[0], 'r-', label='Poisson fit')\n",
    "    \n",
    "    # Fit to Binomial distribution using derived relations\n",
    "    mean = np.mean(weights)\n",
    "    variance = np.var(weights)\n",
    "    \n",
    "    # Calculate p and n estimates\n",
    "    p_est = mean ** 2 / (n * mean - variance) if (n * mean - variance) != 0 else 0\n",
    "    n_est = int(round(mean / p_est)) if p_est != 0 else 0  # n should be integer\n",
    "\n",
    "    # Check parameter validity\n",
    "    if not(0 < p_est < 1):\n",
    "        print(\"Estimated parameters are not valid for the Binomial distribution.\")\n",
    "    else:\n",
    "        x_vals = range(len(bin_centers))\n",
    "        plt.plot(bin_centers, binom.pmf(x_vals, n_est, p_est) * counts[0], 'g-', label='Binomial fit')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"Poisson\": lambda_est, \"Binomial\": (n_est, p_est)}\n",
    "\n",
    "\n",
    "# Assuming W_hag_multi.A is your connectivity matrix\n",
    "analyze_connectivity_matrix(W_hag_multi.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995e386-8847-4da0-8197-c73ff68179bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca5733-cd7f-44df-b321-5146b6df7a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6298ca7-92fe-4f3b-a3fc-6a94d33ab0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ae5c6-b0d8-431b-aef0-a7d5606eb4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54442bc3-7602-4ab1-b008-deb139afde1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccced642-5be0-459c-a8b9-d0c1f2de5e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57ae5f-70d7-4250-9044-0e217865a4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hag_env",
   "language": "python",
   "name": "hag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
